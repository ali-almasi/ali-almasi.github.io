<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-10-05">
<meta name="description" content="The study of Uniformly Minimum Variance Unbiased Estimators (UMVUE) is an old-fashioned but interesting bit of statistical theory. In this post, I consider a very minimal toy-model with no UMVUE and show a family of estimators which are efficient at a single point in space and suboptimal everywhere else.">

<title>Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator) – Guillaume Dehaene</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-40a97741d360803f1a66e8f017dcaab6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-9ade4d119e22f7bacdbf24199394c96c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>
@import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')
@import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')
</style>
<script>
MathJax = {
    tex: {
        tags: 'all'  // should be 'ams', 'none', or 'all'
    },
    output: {
        font: 'mathjax-fira'
    }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Guillaume Dehaene</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../publications.html"> 
<span class="menu-text">My research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About me</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#some-background-in-statistical-theory" id="toc-some-background-in-statistical-theory" class="nav-link active" data-scroll-target="#some-background-in-statistical-theory"><span class="header-section-number">1</span> Some background in statistical theory</a>
  <ul class="collapse">
  <li><a href="#comparing-estimators" id="toc-comparing-estimators" class="nav-link" data-scroll-target="#comparing-estimators"><span class="header-section-number">1.1</span> Comparing estimators</a></li>
  <li><a href="#unbiased-estimators" id="toc-unbiased-estimators" class="nav-link" data-scroll-target="#unbiased-estimators"><span class="header-section-number">1.2</span> Unbiased estimators</a></li>
  <li><a href="#umvue-theory" id="toc-umvue-theory" class="nav-link" data-scroll-target="#umvue-theory"><span class="header-section-number">1.3</span> UMVUE theory</a></li>
  </ul></li>
  <li><a href="#a-simple-exemple-with-no-umvue" id="toc-a-simple-exemple-with-no-umvue" class="nav-link" data-scroll-target="#a-simple-exemple-with-no-umvue"><span class="header-section-number">2</span> A simple exemple with no UMVUE</a></li>
  <li><a href="#getting-value-out-of-an-example" id="toc-getting-value-out-of-an-example" class="nav-link" data-scroll-target="#getting-value-out-of-an-example"><span class="header-section-number">3</span> Getting value out of an example</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">math</div>
    <div class="quarto-category">stackoverflow</div>
  </div>
  </div>

<div>
  <div class="description">
    <p>The study of Uniformly Minimum Variance Unbiased Estimators (UMVUE) is an old-fashioned but interesting bit of statistical theory. In this post, I consider a very minimal toy-model with no UMVUE and show a family of estimators which are efficient at a single point in space and suboptimal everywhere else.</p>
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>

<nav id="TOC-body" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#some-background-in-statistical-theory" id="toc-some-background-in-statistical-theory"><span class="header-section-number">1</span> Some background in statistical theory</a>
  <ul>
  <li><a href="#comparing-estimators" id="toc-comparing-estimators"><span class="header-section-number">1.1</span> Comparing estimators</a></li>
  <li><a href="#unbiased-estimators" id="toc-unbiased-estimators"><span class="header-section-number">1.2</span> Unbiased estimators</a></li>
  <li><a href="#umvue-theory" id="toc-umvue-theory"><span class="header-section-number">1.3</span> UMVUE theory</a></li>
  </ul></li>
  <li><a href="#a-simple-exemple-with-no-umvue" id="toc-a-simple-exemple-with-no-umvue"><span class="header-section-number">2</span> A simple exemple with no UMVUE</a></li>
  <li><a href="#getting-value-out-of-an-example" id="toc-getting-value-out-of-an-example"><span class="header-section-number">3</span> Getting value out of an example</a></li>
  </ul>
</nav>
<p>It is hard to build consensus in statistical theory because it is very hard to formulate a universal set of clear universal rules that one should follow when analysing data. For example, if we want to estimate some parameter <span class="math inline">\(\theta\)</span> given some data <span class="math inline">\(x_1 \dots x_n\)</span> and a parametric probabilistic model <span class="math inline">\(\theta \rightarrow X_i\)</span>, we could:</p>
<ul>
<li>compute the maximum likelihood estimator,</li>
<li>add a prior on <span class="math inline">\(\theta\)</span> and compute a Bayesian estimator,</li>
<li>use a moment-matching technique,</li>
<li>use a robust method,</li>
<li>etc.</li>
</ul>
<p>An early direction for work in statistical theory was to focus instead on comparisons. If finding the best estimator was too tricky, perhaps it would be easier to find that some estimators are <em>dominated</em> by others, ie using them is guaranteed to be worse. For example, if we have two candidate estimators but the second candidate always has smaller variance than the first one, it would be foolish to continue using the first one.</p>
<p>The study of UMVUE is a limitting case of this logic in which we are able to prove that, among the class of unbiased estimators, there is a clear best candidate. As we will discuss, this is a very rare occurence.</p>
<p>I was inspired by <a href="https://math.stackexchange.com/questions/1051346/optimal-unbiased-estimator/5015204#5015204">this old question on stack-overflow and I have written a shorter version on this post over there</a>.</p>
<p>When writing this post, I assume that you already have a background level in some key concepts of statistical theory. I hope that it can bring these concepts into a new light. If self-studying, I recommend the Casella-Berger <em>Statistical Inference</em> book which should be easy to find.</p>
<section id="some-background-in-statistical-theory" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="some-background-in-statistical-theory"><span class="header-section-number">1</span> Some background in statistical theory</h2>
<section id="comparing-estimators" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="comparing-estimators"><span class="header-section-number">1.1</span> Comparing estimators</h3>
<p>Assume that we want to compare two estimators <span class="math inline">\(\hat \theta_1\)</span> and <span class="math inline">\(\hat \theta_2\)</span>, constructed somehow on the basis of some dataset <span class="math inline">\(x_i\)</span>. Comparing the precise conditional distributions of these estimators would be:</p>
<ul>
<li>an extremely complex task, since deriving conditional distributions for complex estimators is very hard;</li>
<li>useless, because comparing distributions is extremely tricky.</li>
</ul>
<p>Instead, we can take a step back. We care about these estimators because we want to reconstruct <span class="math inline">\(\theta\)</span> precisely. By choosing an appropriate loss-function, we can encode in a mathematically-precise way what exactly it means to “reconstruct precisely” <span class="math inline">\(\theta\)</span>. Recall that a loss function combines the true value of <span class="math inline">\(\theta\)</span> and an estimated value <span class="math inline">\(\hat \theta\)</span> and returns a numeric loss associated to this pair.</p>
<p><span class="math display">\[
L: \theta, \hat \theta \rightarrow l \in \mathbb R
\]</span></p>
<p>Given a loss function, we can now compare the two estimators based on their expected losses at each value of <span class="math inline">\(\theta\)</span>. These are two functions:</p>
<p><span class="math display">\[
\theta \rightarrow \mathbb E_\theta \big[ L(\theta, \hat \theta) \big]
\]</span></p>
<p>When comparing two estimators, we can have two scenarios.</p>
<ul>
<li><p>At every value of <span class="math inline">\(\theta\)</span>, one estimator is better than the other. This means that this estimator <strong>dominates</strong> the other. It would be very weird to choose a dominated estimator since you incur a net loss doing so.</p></li>
<li><p>One estimator dominates one some regions but is dominated on other regions. This means that there is no clear hierarchy between the two estimators.</p></li>
</ul>
<p>Mathematically, we thus have constructed a <a href="https://en.wikipedia.org/wiki/Partially_ordered_set">partial order</a> on the ensemble of estimators. This is far from perfect but it is (somewhat[^Having a single loss function fails to represent the full diversity of the ensemble. We are back to the issue of comparing conditional distributions. Perhaps we should consider multiple expected losses, constructed on multiple losses representing different aspects of faithfulness of <span class="math inline">\(\hat \theta\)</span> to <span class="math inline">\(\theta\)</span>?]) faithful to the complexity of that ensemble. If we really want to have a full order, then we need to somehow collapse the function to a single value (while conserving the usual properties of an order):</p>
<ul>
<li>either by taking the <span class="math inline">\(\max\)</span>: this justifies the study of minimax estimators,</li>
<li>or by taking an average over <span class="math inline">\(\theta\)</span>: this gives us a Bayesian perspective on loss functions which justifies Bayesian inference from another angle.</li>
</ul>
</section>
<section id="unbiased-estimators" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="unbiased-estimators"><span class="header-section-number">1.2</span> Unbiased estimators</h3>
<p>A key subset of estimators is the class of <em>unbiased estimators</em>. These are such that their mean is correctly centered at <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\mathbb E_\theta (\hat \theta) = \theta
\]</span></p>
<p>Note that, while the mean has obviously a key role in probability and statistics, it is not obvious that it would be the best way to encode the center of an estimator. Perhaps it could be more relevant, in some specific scenario, to consider estimators with an unbiased median, or an unbiased trimmed mean.</p>
<p>There are two reasons why we care about unbiased estimators. The first one is very practical. Among estimators, biased estimators are very common and straightforward to understand. The extreme case is the constant estimators: <span class="math inline">\(\hat \theta = \theta_0\)</span>. Frustratingly, biased estimators are locally optimal according to the partial order we just defined on estimators. For example, the constant estimators cannot be beat at <span class="math inline">\(\theta_0\)</span>, by definition. Restricting our study to unbiased estimators enables us to exclude these “cheating” estimators.</p>
<p>The second key reason is the fact that we can prove things on estimators with an unbiased mean. The key result is the <a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cramer-Rao theorem</a> which establishes that any unbiased estimator has a minimal variance which depends on <span class="math inline">\(\theta\)</span> and the conditional model of the underlying data.</p>
</section>
<section id="umvue-theory" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="umvue-theory"><span class="header-section-number">1.3</span> UMVUE theory</h3>
<p>We now have sufficient background to discuss what a UMVUE is. A <a href="https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator">UMVUE</a> is an estimator that:</p>
<ul>
<li>is unbiased,</li>
<li>has minimal variance; i.e.&nbsp;is globally optimal for the expected <span class="math inline">\(L^2\)</span> loss: <span class="math inline">\(L^2(\theta, \hat \theta) = (\hat \theta - \theta)^2\)</span>.</li>
</ul>
<p>This is a very strong property since it requires that the partial order is somehow such that it has single global optimum.</p>
<p>Unsurprisingly, UMVUE typically do not exist. The <a href="https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem">Lehmann-Scheffé theorem</a> gives sufficient conditions for their existence (and unicity). Roughly, we can have an UMVUE for the moment-parameters of exponential families.</p>
</section>
</section>
<section id="a-simple-exemple-with-no-umvue" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="a-simple-exemple-with-no-umvue"><span class="header-section-number">2</span> A simple exemple with no UMVUE</h2>
<p>The thing that was bugging was the following. If our data is such that we do not have a complete statistic, then the argument of the Lehmann-Scheffé theorem does not work and we cannot construct a UMVUE candidate. However, that does not prove that a UMVUE does not exist, just that our proof is too limited. This made me very curious about how to construct minimal examples in which a UMVUE does not exist. Here is one very simple construction.</p>
<p>Consider the following model.</p>
<ul>
<li>We have a one-dimensional parameter <span class="math inline">\(\theta \in \mathbb R\)</span>.</li>
<li>We generate a random scale <span class="math inline">\(S\)</span>. For example[^For the curious reader, we will need that the distribution of <span class="math inline">\(S\)</span> does not put too much weight near <span class="math inline">\(0\)</span>.], uniformly distributed on <span class="math inline">\([1,2]\)</span>: <span class="math inline">\(S \sim U[1,2]\)</span>.</li>
<li>We generate a single datapoint by adding scaled Gaussian noise to <span class="math inline">\(\theta\)</span>.</li>
</ul>
<p>In a single equation:</p>
<p><span class="math display">\[
\begin{align}
\theta &amp;\in \mathbb R \\
S &amp;\sim U[1, 2] \\
Z &amp;\sim \mathcal N(0, 1) \\
X &amp;= \theta + S Z \\
\end{align}
\]</span></p>
<p><span class="math inline">\(X\)</span> is then a very natural unbiased estimator of <span class="math inline">\(\theta\)</span> with constant variance:</p>
<p><span class="math display">\[
\operatorname{var}(X) = E(S^2)
\]</span></p>
<p>However, we cannot apply the Lehmann-Scheffé theorem since <span class="math inline">\(s\)</span> provides ancillary information and the pair <span class="math inline">\(S,X\)</span> is sufficient but not complete.</p>
<p>Can we now construct other unbiased estimators which are somehow better than <span class="math inline">\(X\)</span>? My insistence on recalling the Cramer-Rao bound is a strong hint that yes. Indeed, applying the Cramer-Rao bound to my example yields:</p>
<p><span class="math display">\[
\operatorname{var}(\hat \theta) \geq \big[ \mathbb E (1 / S^2) \big]^{-1} = I^{-1}
\]</span></p>
<p>which is smaller than the variance of <span class="math inline">\(X\)</span> due to the Jensen inequality applied to the convex function: <span class="math inline">\(s \rightarrow 1/s^2\)</span>.</p>
<p>We can thus try to construct an estimator which reaches the Cramer-Rao bound. Consider estimators of the form:</p>
<p><span class="math display">\[
\hat \theta = \theta_0 + w(s) (X - \theta_0)
\]</span></p>
<p>where <span class="math inline">\(w(s)\)</span> is some weight function. These estimators bias <span class="math inline">\(X\)</span> towards <span class="math inline">\(\theta_0\)</span> by weighting the evidence according to the <span class="math inline">\(w(s)\)</span> function.</p>
<p>In order for such estimators to be unbiased, they need to respect <span class="math inline">\(\mathbb E[w(S)] = 1\)</span>. Plugging in <span class="math inline">\(w(S) = 1 / I / S^2\)</span> (with <span class="math inline">\(I\)</span> the Fisher information $I = E (1 / S^2) $), we find the <span class="math inline">\(L^2\)</span> error of this estimator:</p>
<p><span class="math display">\[
\mathbb E (\hat \theta - \theta)^2 = \mathbb E (w(S) - 1)^2 (\theta - \theta_0)^2 + I^-1
\]</span></p>
<p>These weighted estimators are thus unbiased and locally optimal at <span class="math inline">\(\theta_0\)</span> where they exactly saturate the Cramer-Rao bound.</p>
<p>The existence of this family of estimators demonstrates the impossibility of the existence of a UMVUE in this example. Indeed, in order to be a UMVUE, an estimator would need to saturate the Cramer-Rao bound everywhere so that it improves on each single estimator in the family. This is impossible because, in order to saturate the Cramer-Rao bound at <span class="math inline">\(\theta_0\)</span>, a unbiased estimator needs to be proportional to the score function at <span class="math inline">\(\theta_0\)</span> (the partial derivative with respect to <span class="math inline">\(\theta_0\)</span> of the log-likelihood) which is exactly how I constructed the local estimators. Since you cannot be proportional to multiple local estimators at once, there is no UMVUE.</p>
<p>Instead, in this example, we need to choose between several reasonable estimators:</p>
<ul>
<li>a minimax estimator <span class="math inline">\(X\)</span> which is decent everywhere,</li>
<li>locally optimal estimators which are maximally efficient at <span class="math inline">\(\theta_0\)</span>, which improve on <span class="math inline">\(X\)</span> in a neighborhood of <span class="math inline">\(\theta_0\)</span> but are increasingly worse as we move away from <span class="math inline">\(\theta_0\)</span>.</li>
</ul>
</section>
<section id="getting-value-out-of-an-example" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="getting-value-out-of-an-example"><span class="header-section-number">3</span> Getting value out of an example</h2>
<p>I have just presented a single example of a model in which the UMVUE does not exist and this might seem irrelevant. Indeed, the field of mathematics does not place a lot of value on examples. However, I strongly disagree with this point of view: in my opinion, examples such as this one are a great platforms on which to build mathematical intuition which can then be strenghtened by mathematical rigor.</p>
<p>Here, this example is very valuable because it is a <em>toy-model</em>, a simplified situtation in which we can carry out rigorous analysis but which remains representative of a large class of practical situations. This model is simple in the following ways:</p>
<ul>
<li>We have a Gaussian model with unknown mean, i.e.&nbsp;the simplest statistical model with great mathematical properties.</li>
<li>The the scale of the noise is randomized but observed.</li>
</ul>
<p>As a consequence, we have a clear unbiased estimator, we were able to derive the Fisher information, the locally optimal estimators (and they are unbiased!) and prove that the UMVUE does not exist. However, the overall structure of this model is universal. Many models have this structure in which we actually care about a single parameter, or a subset of parameters, and the other parameters are mostly a <em>nuisance</em> which impacts the quality of the information about the parameters of interest. In such situations, this analysis of the toy model shows that we will likely have a similar situation as here: we will again have a choice between globally efficient estimators and locally efficient ones.</p>



</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("www\.guillaumedehaene\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">    The study of Uniformly Minimum Variance Unbiased Estimators (UMVUE) is</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    an old-fashioned but interesting bit of statistical theory.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    In this post, I consider a very minimal toy-model with no UMVUE and show</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    a family of estimators which are efficient at a single point in space</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    and suboptimal everywhere else.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "10/05/2024"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - math</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">  - stackoverflow</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>It is hard to build consensus in statistical theory</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>because it is very hard to formulate a universal set of clear universal rules</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>that one should follow when analysing data.</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>For example, if we want to estimate some parameter $\theta$ given some data $x_1 \dots x_n$ and a parametric probabilistic model $\theta \rightarrow X_i$, we could:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>compute the maximum likelihood estimator,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>add a prior on $\theta$ and compute a Bayesian estimator,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use a moment-matching technique,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use a robust method,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>etc.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>An early direction for work in statistical theory was to focus instead on comparisons.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>If finding the best estimator was too tricky, perhaps it would be easier to find</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>that some estimators are *dominated* by others, ie using them is guaranteed to be worse.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>For example, if we have two candidate estimators but the second candidate always has smaller variance than the first one, it would be foolish to continue using the first one.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>The study of UMVUE is a limitting case of this logic in which we are able to prove that, among the class of unbiased estimators, there is a clear best candidate.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>As we will discuss, this is a very rare occurence.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>I was inspired by <span class="co">[</span><span class="ot">this old question on stack-overflow and I have written a shorter version on this post over there</span><span class="co">](https://math.stackexchange.com/questions/1051346/optimal-unbiased-estimator/5015204#5015204)</span>.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>When writing this post, I assume that you already have a background level in some key concepts of statistical theory. I hope that it can bring these concepts into a new light. If self-studying, I recommend the Casella-Berger *Statistical Inference* book which should be easy to find.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="fu"># Some background in statistical theory</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing estimators</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>Assume that we want to compare two estimators $\hat \theta_1$ and $\hat \theta_2$,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>constructed somehow on the basis of some dataset $x_i$.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>Comparing the precise conditional distributions of these estimators would be:</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>an extremely complex task, since deriving conditional distributions for complex estimators is very hard;</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>useless, because comparing distributions is extremely tricky.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Instead, we can take a step back.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>We care about these estimators because we want to reconstruct $\theta$ precisely.</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>By choosing an appropriate loss-function, we can encode in a mathematically-precise way</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>what exactly it means to "reconstruct precisely" $\theta$.</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>Recall that a loss function combines the true value of $\theta$ and an estimated value $\hat \theta$ and returns a numeric loss associated to this pair.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>L: \theta, \hat \theta \rightarrow l \in \mathbb R</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>Given a loss function, we can now compare the two estimators based on their expected losses at each value of $\theta$.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>These are two functions:</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>\theta \rightarrow \mathbb E_\theta \big<span class="co">[</span><span class="ot"> L(\theta, \hat \theta) \big</span><span class="co">]</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>When comparing two estimators, we can have two scenarios.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>At every value of $\theta$, one estimator is better than the other.</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    This means that this estimator **dominates** the other.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    It would be very weird to choose a dominated estimator since you incur a net loss doing so.</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One estimator dominates one some regions but is dominated on other regions.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    This means that there is no clear hierarchy between the two estimators.</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>Mathematically, we thus have constructed a <span class="co">[</span><span class="ot">partial order</span><span class="co">](https://en.wikipedia.org/wiki/Partially_ordered_set)</span> on the ensemble of estimators.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>This is far from perfect but it is (somewhat<span class="ot">[^Having a single loss function fails to represent the full diversity of the ensemble. We are back to the issue of comparing conditional distributions. Perhaps we should consider multiple expected losses, constructed on multiple losses representing different aspects of faithfulness of $\hat \theta$ to $\theta$?]</span>) faithful to the complexity of that ensemble.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>If we really want to have a full order, then we need to somehow collapse the function to a single value (while conserving the usual properties of an order):</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>either by taking the $\max$: this justifies the study of minimax estimators,</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>or by taking an average over $\theta$: this gives us a Bayesian perspective on loss functions which justifies Bayesian inference from another angle.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="fu">## Unbiased estimators</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>A key subset of estimators is the class of *unbiased estimators*.</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>These are such that their mean is correctly centered at $\theta$:</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\mathbb E_\theta (\hat \theta) = \theta</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>Note that, while the mean has obviously a key role in probability and statistics, it is not obvious that it would be the best way to encode the center of an estimator.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Perhaps it could be more relevant, in some specific scenario, to consider estimators with an unbiased median, or an unbiased trimmed mean.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>There are two reasons why we care about unbiased estimators. The first one is very practical. Among estimators, biased estimators are very common and straightforward to understand. The extreme case is the constant estimators: $\hat \theta = \theta_0$. Frustratingly, biased estimators are locally optimal according to the partial order we just defined on estimators. For example, the constant estimators cannot be beat at $\theta_0$, by definition. Restricting our study to unbiased estimators enables us to exclude these "cheating" estimators.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>The second key reason is the fact that we can prove things on estimators with an unbiased mean. The key result is the <span class="co">[</span><span class="ot">Cramer-Rao theorem</span><span class="co">](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound)</span> which establishes that any unbiased estimator has a minimal variance which depends on $\theta$ and the conditional model of the underlying data.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">## UMVUE theory</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>We now have sufficient background to discuss what a UMVUE is.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>A <span class="co">[</span><span class="ot">UMVUE</span><span class="co">](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator)</span> is an estimator that:</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>is unbiased,</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>has minimal variance; i.e. is globally optimal for the expected $L^2$ loss: $L^2(\theta, \hat \theta) = (\hat \theta - \theta)^2$.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>This is a very strong property since it requires that the partial order is somehow such that it has single global optimum.</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>Unsurprisingly, UMVUE typically do not exist. The <span class="co">[</span><span class="ot">Lehmann-Scheffé theorem</span><span class="co">](https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem)</span> gives sufficient conditions for their existence (and unicity). Roughly, we can have an UMVUE for the moment-parameters of exponential families.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="fu"># A simple exemple with no UMVUE</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>The thing that was bugging was the following.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>If our data is such that we do not have a complete statistic, then the argument of the Lehmann-Scheffé theorem does not work and we cannot construct a UMVUE candidate.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>However, that does not prove that a UMVUE does not exist, just that our proof is too limited.</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>This made me very curious about how to construct minimal examples in which a UMVUE does not exist.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>Here is one very simple construction.</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>Consider the following model.</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We have a one-dimensional parameter $\theta \in \mathbb R$.</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We generate a random scale $S$. For example<span class="ot">[^For the curious reader, we will need that the distribution of $S$ does not put too much weight near $0$.]</span>, uniformly distributed on $<span class="co">[</span><span class="ot">1,2</span><span class="co">]</span>$: $S \sim U<span class="co">[</span><span class="ot">1,2</span><span class="co">]</span>$.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We generate a single datapoint by adding scaled Gaussian noise to $\theta$.</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>In a single equation:</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>\theta &amp;\in \mathbb R <span class="sc">\\</span></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>S &amp;\sim U<span class="co">[</span><span class="ot">1, 2</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>Z &amp;\sim \mathcal N(0, 1) <span class="sc">\\</span></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>X &amp;= \theta + S Z <span class="sc">\\</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$X$ is then a very natural unbiased estimator of $\theta$ with constant variance:</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>\operatorname{var}(X) = E(S^2)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>However, we cannot apply the Lehmann-Scheffé theorem since $s$ provides ancillary information and the pair $S,X$ is sufficient but not complete.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>Can we now construct other unbiased estimators which are somehow better than $X$?</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>My insistence on recalling the Cramer-Rao bound is a strong hint that yes.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>Indeed, applying the Cramer-Rao bound to my example yields:</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>\operatorname{var}(\hat \theta) \geq \big<span class="co">[</span><span class="ot"> \mathbb E (1 / S^2) \big</span><span class="co">]</span>^{-1} = I^{-1}</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>which is smaller than the variance of $X$ due to the Jensen inequality applied to the convex function: $s \rightarrow 1/s^2$.</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>We can thus try to construct an estimator which reaches the Cramer-Rao bound. Consider estimators of the form:</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>\hat \theta = \theta_0 + w(s) (X - \theta_0)</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>where $w(s)$ is some weight function. These estimators bias $X$ towards $\theta_0$ by weighting the evidence according to the $w(s)$ function.</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>In order for such estimators to be unbiased, they need to respect $\mathbb E<span class="co">[</span><span class="ot">w(S)</span><span class="co">]</span> = 1$. Plugging in $w(S) = 1 / I / S^2$ (with $I$ the Fisher information $I = \mathbb E (1 / S^2) $), we find the $L^2$ error of this estimator:</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>\mathbb E (\hat \theta - \theta)^2 = \mathbb E (w(S) - 1)^2 (\theta - \theta_0)^2 + I^-1</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>These weighted estimators are thus unbiased and locally optimal at $\theta_0$ where they exactly saturate the Cramer-Rao bound.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>The existence of this family of estimators demonstrates the impossibility of the existence of a UMVUE in this example.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>Indeed, in order to be a UMVUE, an estimator would need to saturate the Cramer-Rao bound everywhere so that it improves on each single estimator in the family.</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>This is impossible because, in order to saturate the Cramer-Rao bound at $\theta_0$, a unbiased estimator needs to be proportional to the score function at $\theta_0$ (the partial derivative with respect to $\theta_0$ of the log-likelihood) which is exactly how I constructed the local estimators.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>Since you cannot be proportional to multiple local estimators at once, there is no UMVUE.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>Instead, in this example, we need to choose between several reasonable estimators:</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a minimax estimator $X$ which is decent everywhere,</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>locally optimal estimators which are maximally efficient at $\theta_0$, which improve on $X$ in a neighborhood of $\theta_0$ but are increasingly worse as we move away from $\theta_0$.</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="fu"># Getting value out of an example</span></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>I have just presented a single example of a model in which the UMVUE does not exist and this might seem irrelevant.</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>Indeed, the field of mathematics does not place a lot of value on examples.</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>However, I strongly disagree with this point of view:</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>in my opinion, examples such as this one are a great platforms on which to build mathematical intuition which can then be strenghtened by mathematical rigor.</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>Here, this example is very valuable because it is a *toy-model*, a simplified situtation in which we can carry out rigorous analysis but which remains representative of a large class of practical situations.</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>This model is simple in the following ways:</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We have a Gaussian model with unknown mean, i.e. the simplest statistical model with great mathematical properties.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The the scale of the noise is randomized but observed.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>As a consequence, we have a clear unbiased estimator, we were able to derive the Fisher information, the locally optimal estimators (and they are unbiased!) and prove that the UMVUE does not exist.</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>However, the overall structure of this model is universal.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>Many models have this structure in which we actually care about a single parameter, or a subset of parameters, and the other parameters are mostly a *nuisance* which impacts the quality of the information about the parameters of interest.</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>In such situations, this analysis of the toy model shows that we will likely have a similar situation as here:</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>we will again have a choice between globally efficient estimators and locally efficient ones.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Copyright Guillaume Dehaene. All rights reserved.
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>