[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My research",
    "section": "",
    "text": "1 Publications\n  \n  1.1 A deterministic and computable Bernstein-von Mises theorem\n  1.2 Computing the quality of the Laplace approximation\n  1.3 Expectation Propagation in the large-data limit\n  1.4 Expectation Propagation performs a smoothed gradient descent\n  1.5 Bounding errors of expectation-propagation\n  \n  2 Research interests\n  \n  2.1 The problem of the uncomputable posteriors\n  2.2 Approximate inference schemes\n  2.3 Bayesian statistics for the frequentist\nI currently work full-time as an AI software engineer. This page recaps my work in statistics as a PhD. student at university of Geneva and as an instructor at Ecole Polytechnique Fédérale de Lausanne (EPFL) (from 2012 to 2020):"
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "My research",
    "section": "1 Publications",
    "text": "1 Publications\n\n1.1 A deterministic and computable Bernstein-von Mises theorem\nGuillaume Dehaene, 2019.\nArticle link.\nIn order to make Bayesian inference possible on large datasets, approximations are required. For example, computing the Laplace approximation is straightforward since it only requires finding the maximum of the posterior. However, while the Bernstein-von Mises theorem guarantees that the error of the Laplace approximation goes to in the limit of infinitely large datasets, it is hard to measure precisely the size of the error in a given example.\nThis article derives a tight and computable elegant approximation of the size of this error. I show that the Kullback-Leibler divergence between a given probability distribution and its Laplace approximation can be approximated using the “Kullback-Leibler variance”\n\n\n1.2 Computing the quality of the Laplace approximation\nGuillaume Dehaene, 2017, AABI NIPS 2017 Workshop.\nArticle link.\nBayesian inference requires approximations because the posterior distribution is generally uncomputable. The Laplace approximation is a fairly basic one which gives us a Gaussian approximation of the posterior distribution. This begs the question: how good is the approximation? The classical answer to this question is the Bernstein-von Mises theorem, which asserts that in the large-data limit, the Laplace approximation becomes exact. However, this theorem is mostly useless in practice, mostly because its assumptions are hard to check.\nThis article presents a computationally-relevant extension of the classical result: we give an explicit upper-bound for the distance between a given posterior and its Laplace approximation. The approach we follow can be extended to more advanced Gaussian approximation methods which we will do in further work.\n\n\n1.3 Expectation Propagation in the large-data limit\nGuillaume Dehaene and Simon Barthelmé, 2017, Journal of the Royal Statistical Society, series B.\nArticle link.\nExpectation Propagation is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it. Our two main contributions consist in showing that EP is closely related to Newton’s method for finding the maximum of the posterior, and showing that EP is asymptotically exact, meaning that when the number of datapoints goes to infinity the method recovers the posterior exactly.\nWe also introduce some new theoretical tools that help analysing EP formally, including a simpler variant, called Average-EP (or Stochastic-EP), that is asymptotically equivalent to EP.\n\n\n1.4 Expectation Propagation performs a smoothed gradient descent\nGuillaume Dehaene, 2016, AABI NIPS 2016 Workshop.\nArticle link.\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nIf one wants to compute a Gaussian approximation of a probability distribution, there are three popular alternatives: the Laplace approximation, the Gaussian Variational Approximation, and Expectation Propagation.\nI show in this work that the approximations found by these three methods are actually very closely related, as they all correspond to variants from the same algorithm. This shines a bright light on the deep connections between these three algorithms.\n\n\n1.5 Bounding errors of expectation-propagation\nGuillaume Dehaene and Simon Barthelmé, 2015, NIPS 2015.\nArticle link.\nExpectation Propagation (EP) is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it.\nOur contribution in this work consists in showing that, in the large-data limit, EP is asymptotically exact and, furthermore, more precise than the alternative Laplace approximation. However, our results only hold for strongly log-concave distributions, which very rarely exist."
  },
  {
    "objectID": "publications.html#research-interests",
    "href": "publications.html#research-interests",
    "title": "My research",
    "section": "2 Research interests",
    "text": "2 Research interests\nMy research interests are briefly summarized in this section.\n\n2.1 The problem of the uncomputable posteriors\nBayesian inference is a very interesting method for someone who is interested in an axiomatic approach to inference. Indeed, the statistician Cox set out a simple set of rules for a robot to represent, using real numbers, the strength of his beliefs in various propositions and proved that the only system which obeys these axioms is probability theory. Furthermore, the rule that the robot should use to update his beliefs when faced new information is Bayes’s rule. The only axiomatization of rational thinking about the world is thus Bayesian inference.\nHowever, there is a huge problem with Bayesian inference: in most cases, the computations required for exact implementation of the method are too expensive to be used in practice. Most of the practical research work on Bayesian methods actually revolves about how to deal with this thorny issue with various approximation schemes. These approximation schemes can be decomposed into two large families: sampling methods (dominated by Markov Chain Monte Carlo methods; acronym MCMC) which aim at producing samples from the posterior distribution and on which I don’t have much to say, and what I call approximate inference methods: methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution.\n\n\n2.2 Approximate inference schemes\nMy work centers on methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution. These are often called “variational” methods but I’d rather call them approximate inference methods instead. This is because:\n\nthe word “variational” is already used for the Variational Bayes algorithm which, even though it is the most popular approximate inference method, is far from being the only one\n“variational” implies an optimisation, which means that the term variational excludes the Expectation Propagation algorithm\nthe only critic of the “approximate inference” vs “sampling” separation I have gotten is that sampling methods also aim at producing an approximation of the posterior. I still feel like this is fine, since sampling methods require quite a bit of further processing in order to answer questions about the posterior whereas approximate inference methods directy output an approximation.\n\nThere is currently a large number of open questions on such methods.\n\nThe most important one concerns the speed at which these algorithms perform their task. Most often, these algorithms perform an iteration until they reach a fixed-point. Estimating the number of loops needed for convergence is very important for being able to guarantee that our algorithms will run quickly.\nA second critical question concerns the quality of the approximation we obtain. We need to understand in which cases these algorithms are good enough, and in which cases we should use the more expensive but more accurate sampling methods. However, current theoretical results are inapplicable for a number of reasons: the hypotheses are untestable, they apply to approximations that are not in use, etc.\nThe final important question is of a more practical nature. It is simply whether the current versions of the algorithm we have are the best we can do, or whether there are better variants that are yet to be found. This can only be solved once we are able to compute the speed and the approximation-quality of current methods. We will then be able to see whether introducing slight changes to the current algorithms will improve them.\n\n\n\n2.3 Bayesian statistics for the frequentist\nAnother aspect of my work concerns trying to convince my frequentist colleagues that Bayesian inference is the best system of statistics.\nIn practice, the best way to do this is not to be a dogmatic Bayesian which goes on shouting about the Cox axioms and subjective probability, but instead to adopt the frequentist point of view on problems, and show that Bayesian methods are the best at solving these. Adopting Bayesian methods is then simply a matter of choosing the most efficient tool for solving problems.\nIn practice, following this idea means studying the posterior distribution as a function-valued random variable, and then studying the behavior of this random variable. My work expands on earlier work by, among other, Lecam on what is called the “Bernstein-von Mises theorem”. My objective is to expand current forms of this theorem so that they are:\nAs general as they can be. As efficient as they can be. Relevant in practical applications of Bayesian inference."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__2.html",
    "href": "posts/2024/12/stack_overflow_1_k__2.html",
    "title": "Investigating a conjecture from Stack Overflow 2/2",
    "section": "",
    "text": "1 The problem\n  2 Proof of point 1\n  3 Proof of point 2\n  \n  3.1 The easy case \\(m = k\\) and \\(n=a m\\)\n  3.2 Recursion: reducing \\(k\\)\n  3.3 Recursion: reducing \\(n\\) and \\(m\\)\n  3.4 Examples"
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__2.html#the-problem",
    "href": "posts/2024/12/stack_overflow_1_k__2.html#the-problem",
    "title": "Investigating a conjecture from Stack Overflow 2/2",
    "section": "1 The problem",
    "text": "1 The problem\nInitial question on SO.\n\nThere are \\(n\\) cards which have numbers \\(1\\)~\\(n\\) on each. You pick \\(m\\) cards from it, and you don’t put it back once you pick from it. Is the probability that their sum is divisible by \\(k\\) always \\(\\dfrac 1 k\\), while \\(k|n\\)? If not, how do we generalize the probability?\n\nMore mathematically, let \\(X_i\\) be samples without replacement from \\([1, n]\\). Let \\(S\\) be their sum modulo \\(k\\):\n\\[\nS = \\sum X_i \\bmod k\n\\]\nWe want to find the probability of \\(S = 0\\).\nAfter an intitial study, I was able to conjecture that the probability is closely related to the greatest common divisor of \\(k, m\\). Let \\(p\\) be the probability of the sum being divisible.\n\nIf \\(\\operatorname{gcd}(m, k)=1\\), then \\(p=1/k\\).\nIf \\(\\operatorname{gcd}(m, k) \\neq 1\\), then \\(p \\neq 1/k\\). The deviation is small and I derive a procedure to compute it."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__2.html#proof-of-point-1",
    "href": "posts/2024/12/stack_overflow_1_k__2.html#proof-of-point-1",
    "title": "Investigating a conjecture from Stack Overflow 2/2",
    "section": "2 Proof of point 1",
    "text": "2 Proof of point 1\nThe proof of point 1 was essentially derived by user Matthew Spam nn of the answers of the original thread. Their argument is very elegant and consists in showing that the number of subsets \\(x_1 \\dots x_m\\) of \\([1, n]\\) such that \\(S=0\\) is equal to the number of subsets such that \\(S=1\\) or \\(2\\), etc. Thus, \\(S\\) is a uniform variable over an ensemble with \\(k\\) possibilities: \\([0,k-1]\\) and \\(p=1/k\\).\nTo see this, let \\(x_1 \\dots x_m\\) be one subset, and consider the subset \\(x_1 + 1 \\dots x_m + 1\\) where we add \\(1\\) to each \\(x\\). This adds \\(m\\) to the sum:\n\\[\n\\sum (x_i + 1) == m + \\sum x_i \\mod k\n\\]\nSimilarly, adding \\(i \\in [0,k-1]\\) instead of \\(1\\) adds \\(m i\\) to the total sum.\nSince \\(\\operatorname{gcd}(m, k) = 1\\), as \\(i\\) takes all values in \\([0, k-1]\\), the sum takes all values in \\([0, k-1]\\).\nThus, we find that, for any subset \\(x_1 \\dots x_m\\) summing to some value, there is also a subset summing to any other value. There is thus the same number of subsets for every value in \\([0, k-1]\\).\nThis proves that the distribution of \\(\\sum X_i \\bmod k\\) is uniform over \\([0, k-1]\\).\n\n\n\n\n\n\nIf \\(\\operatorname{gcd}(m, k) \\bmod k = 1\\), then \\(\\sum X_i \\bmod k\\) is uniform over \\([0, k-1]\\)."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__2.html#proof-of-point-2",
    "href": "posts/2024/12/stack_overflow_1_k__2.html#proof-of-point-2",
    "title": "Investigating a conjecture from Stack Overflow 2/2",
    "section": "3 Proof of point 2",
    "text": "3 Proof of point 2\nIn order to prove the second point, I do not have a direct proof. Instead, the best I have is a series of results to:\n\nestablish a result in a simple case,\ndecompose the general case as a combination of simple cases.\n\nAs a bonus, the decomposition could be used to write down a computer program to compute the probability explicitly, which I leave as an exercise to the interested reader.\nThis divide-and-conquer approach to proofs is common, but I typically find it unsatisfying: I am always worried that a more elegant proof exists. In this particular case though, the result is quite complex so I would be surprised that a simpler formula could exist.\n\n3.1 The easy case \\(m = k\\) and \\(n=a m\\)\nThe easiest case is when all three values \\(n, m, k\\) are equal. Then, the process is not random at all: we have a single possible value which is:\n\\[\n\\sum_{i=1}^n i \\bmod n = n (n-1) / 2 \\bmod n\n\\]\nWhether this is divisible by \\(n\\) depends on its parity:\n\nIf \\(n\\) is odd then \\((n-1) / 2\\) is an integer and thus the sum is divisible by \\(n\\).\nIf \\(n\\) is even, then the sum is not divisible by \\(n\\).\n\n\n\n\n\n\n\nThe general easy case is when \\(m=k\\) and \\(n=ak\\). We then have three possibilities:\n\n\\(n\\) odd: \\(p=1\\).\n\\(n\\) even, \\(a\\) even: \\(p=1\\).\n\\(n\\) even, \\(a\\) odd: \\(p=0\\).\n\n\n\n\n\n\n3.2 Recursion: reducing \\(k\\)\nWe now turn to the task of reducing an arbitrary triplet \\(n, m, k\\) to the simple case.\nLet us return to the proof of point 1. If \\(\\operatorname{gcd}(m, k) = g &gt; 1\\), then the argument doesn’t work because we cannot reach all values modulo from the starting point. When we add \\(i \\in [0, k-1]\\) to each \\(x_i\\), we add \\(m i\\) to the sum, and that only reaches values which are offset by \\(g\\).\nHowever, that is still an interesting observation: the probability distribution of the sum is such that it is invariant to these offsets of \\(g\\). Thus, in order to caracterize the distribution, we only need to know the probability distribution of the sum modulo \\(g\\).\n\\[\n\\mathbb P \\bigg(\\sum x_i \\bmod k = j \\bigg) = \\frac{g}{k} \\mathbb P \\bigg(\\sum x_i = j \\mod g \\bigg)\n\\]\nThus, we can simplify the calculation of \\(p\\):\n\n\n\n\n\n\n\\[\np(n, m, k) = \\frac{g}{k} p(n, m, g)\n\\]\n\n\n\nNote that this extends result 1 which corresponds to the case \\(g=1\\).\n\n\n3.3 Recursion: reducing \\(n\\) and \\(m\\)\nNow, we turn to reducing \\(n\\) and \\(m\\). This is more complex than reducing \\(k\\) so hang tight.\nThe key observation I have is the following. The situation is complex here because we have sampling without replacement from values in \\([1, n]\\). If we had instead sampling with replacement, we would have a straightforward symmetry and the probability distribution of the sum would be uniform.\nNow observe further that we do not actually care about the precise value of \\(x_i\\) but only about its value modulo \\(k\\). For example, if \\(n = 6, m = 2, k = 3\\), we are actually sampling from the ensemble: \\(\\{1, 2, 3, 1, 2, 3 \\}\\). In this situation, the correlations due to sampling with replacement can be analyzed in the following way:\n\neither the two observations come from the same half, in which case they cannot take the same value, i.e. they are conditionally highly anti-correlated,\nor they come from two different halves, in which case they are conditionally independent.\n\nWe can decompose the calculation of \\(p(n=6, m=2, k=3)\\) by considering these two cases separately.\nWe will generalize this approach of conditioning but we will need additional notation for this. Given some initial number of samples \\(m\\), we will denote with a list \\([c_1, c_2 \\dots]\\) the non-zero counts of the number of samples falling into the different subsets of size \\(k\\) in \\([1, n]\\). NB: we do not distinguish the subsets. In our example, we have two cases: \\([2]\\) and \\([1,1]\\).\nThe distribution of the counts depends on the number of groups \\(n / k\\), the size of the groups \\(k\\) and the number of samples \\(m\\). Deriving its distribution is a complex exercise in combinatorics which I leave to the interested reader. Sampling from it is straightforward: we simply sample from \\([1, n]\\), count the number of samples in each group, and sort them.\nNow, given a list of counts \\([c_1 \\dots]\\), let us investigate the probability distribution of \\(\\sum x_i \\bmod k\\). Again, we can bring to bear the translation argument, but inside of each group. Instead of translating in \\([1, n]\\) translate the j-th group by \\(i\\). This translates the sum by \\(c_j i\\) modulo \\(k\\).\nLet us start by the simplest case: assume that a group has a count of 1. By translating that group by \\(i \\in [0, k-1]\\), we can reach any value in \\([0, k-1]\\). Thus, for any count which has a \\(1\\), the probability distribution of the sum is uniform over \\([0, k-1]\\).\nNow, assume that we have multiple counts such that their greatest common divisor is \\(1\\). By definition of the gcd, we are able to find combinations of the \\(c_j\\) such that the sum reaches any value in \\([0, k-1]\\). Again, the probability distribution of the sum is uniform over \\([0, k-1]\\). The most general case of this argument is when \\(\\operatorname{gcd}(k, c_1 \\dots) = 1\\).\nWhen the gcd is not 1, we cannot reach all values. Instead, we have partial symmetry, exactly like in Section 3.2: we can only increase the count in increments of the gcd. Again, we can reduce the modulo in this situation:\n\\[\np(n, [c_1 \\dots], k) = \\frac{g}{k} p(k, [c_1 \\dots ], g)\n\\]\nIn this final situation, we now have again a discrepancy between the group-size \\(k\\) and the modulo \\(g\\). Again, we should split the counts into groups of size \\(g\\). Iterating this process reduces the counts and \\(k\\) until we are in the simple case of all groups of the same size equal to the group size and modulo. I.e. we are able to reduce into the simple case discussed in Section 3.1.\n\n\n3.4 Examples\nLet us do some examples.\n\n\\(n, m, k = 4, 2, 2\\)\nWe have \\(\\operatorname{gcd}(m, k) = 2\\) which is already equal to \\(k\\). We thus turn to conditionning on counts. We have two possible counts: \\([2]\\) and \\([1, 1]\\) with probabilities \\(1/3\\) and \\(2/3\\). Decomposing:\n\\[\n\\begin{align}\np(4,2,2) &= 1/3 p(2, [2], 2) + 2/3 p(2, [1, 1], 2] \\\\\n        &= 0 + 2/3 1/2 \\\\\n        &= 1/3\n\\end{align}\n\\]\n\n\n\\(n, m, k = 8, 4, 2\\)\nAgain, we turn to counts. The only “interesting” count is \\([2, 2]\\). Denote it with \\(A\\) and let \\(B\\) be the complement event. All others counts have gcd 1. Thus, for any count in \\(B\\) we have \\(p(2, [c_1 \\dots], 2) = 1/2\\).\n\\[\n\\begin{align}\np(8, 4, 2) &= \\mathbb P(A) p(2, [2, 2], 2) + \\mathbb P(B) 1/2 \\\\\n           &= \\mathbb P(A) 1 + \\mathbb P(B) 1/2\n\\end{align}\n\\]\nand we find that the probability is only slightly higher than \\(1/k = 1/2\\)\n\n\n\\(n, m, k = 6, 3, 3\\)\nThis is the example identified in the initial question. Again, we have a single interesting count: \\([3]\\) which we denote with \\(A\\). We have \\(\\mathbb P(A) = 1/10\\) and:\n\\[\n\\begin{align}\np(6, 3, 3) &= \\mathbb P(A) p(3, [3], 3) + \\mathbb P(B) 1/3 \\\\\n           &= \\mathbb P(A) 1 + \\mathbb P(B) 1/3 \\\\\n           &\\approx 0.4333\n\\end{align}\n\\]\n\n\n\\(n, m, k = 18, 6, 6\\)\nThis final example is the first one in which we need to re-split the counts. It is also interesting since it is emblematic of cases for which we could not prove that \\(p \\neq 1/k\\).\nWe have three interesting counts: \\([6], [3,3], [2,2,2]\\). The first one is a simple case with \\(p=0\\), but the two final counts need further work.\n\\[\np(6, [3, 3], 6) = \\frac{1}{2} p(6, [3, 3], 3)\n\\]\nwhere the \\(3\\) counts in a group of size \\(6\\) need to be split. The only interesting count is when both groups remain intact.\n\\[\np(6, [3, 3], 3) = \\mathbb P (\\text{Groups intact}) p(3, [3, 3], 3)\n            + [1 - \\mathbb P (\\text{Groups intact})] 1/3 &gt; 1/3\n\\]\nSimilarly, for the \\([2, 2, 2]\\) count:\n\\[\np(6, [2, 2, 2], 6) = \\frac{1}{3} p(6, [2, 2, 2], 2)\n\\]\nand again, the only interesting split is when all groups remain intact:\n\\[\np(6, [2, 2, 2], 2) = \\mathbb P (\\text{Groups intact}) p(2, [2, 2, 2], 2)\n            + [1 - \\mathbb P (\\text{Groups intact})] 1/2 &lt; 1/2\n\\]\nReturning the example, we have:\n\\[\n\\begin{align}\np(18,6,6) &= \\mathbb P(A) 0 + \\mathbb P(B) p(6, [3, 3], 6)\n            + \\mathbb P(C) p(6, [2, 2, 2], 6)\n            + \\mathbb P(D) 1/6\n\\end{align}\n\\]\nInterestingly, it is ambiguous whether the final result is greater or smaller than \\(1/6\\) since the \\(A, C\\) terms point down but the \\(B\\) term is pointing up."
  },
  {
    "objectID": "posts/2024/10/understanding_umvue.html",
    "href": "posts/2024/10/understanding_umvue.html",
    "title": "Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)",
    "section": "",
    "text": "1 Some background in statistical theory\n  \n  1.1 Comparing estimators\n  1.2 Unbiased estimators\n  1.3 UMVUE theory\n  \n  2 A simple exemple with no UMVUE\n  3 Getting value out of an example\nIt is hard to build consensus in statistical theory because it is very hard to formulate a universal set of clear universal rules that one should follow when analysing data. For example, if we want to estimate some parameter \\(\\theta\\) given some data \\(x_1 \\dots x_n\\) and a parametric probabilistic model \\(\\theta \\rightarrow X_i\\), we could:\nAn early direction for work in statistical theory was to focus instead on comparisons. If finding the best estimator was too tricky, perhaps it would be easier to find that some estimators are dominated by others, ie using them is guaranteed to be worse. For example, if we have two candidate estimators but the second candidate always has smaller variance than the first one, it would be foolish to continue using the first one.\nThe study of UMVUE is a limitting case of this logic in which we are able to prove that, among the class of unbiased estimators, there is a clear best candidate. As we will discuss, this is a very rare occurence.\nI was inspired by this old question on stack-overflow and I have written a shorter version on this post over there.\nWhen writing this post, I assume that you already have a background level in some key concepts of statistical theory. I hope that it can bring these concepts into a new light. If self-studying, I recommend the Casella-Berger Statistical Inference book which should be easy to find."
  },
  {
    "objectID": "posts/2024/10/understanding_umvue.html#some-background-in-statistical-theory",
    "href": "posts/2024/10/understanding_umvue.html#some-background-in-statistical-theory",
    "title": "Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)",
    "section": "1 Some background in statistical theory",
    "text": "1 Some background in statistical theory\n\n1.1 Comparing estimators\nAssume that we want to compare two estimators \\(\\hat \\theta_1\\) and \\(\\hat \\theta_2\\), constructed somehow on the basis of some dataset \\(x_i\\). Comparing the precise conditional distributions of these estimators would be:\n\nan extremely complex task, since deriving conditional distributions for complex estimators is very hard;\nuseless, because comparing distributions is extremely tricky.\n\nInstead, we can take a step back. We care about these estimators because we want to reconstruct \\(\\theta\\) precisely. By choosing an appropriate loss-function, we can encode in a mathematically-precise way what exactly it means to “reconstruct precisely” \\(\\theta\\). Recall that a loss function combines the true value of \\(\\theta\\) and an estimated value \\(\\hat \\theta\\) and returns a numeric loss associated to this pair.\n\\[\nL: \\theta, \\hat \\theta \\rightarrow l \\in \\mathbb R\n\\]\nGiven a loss function, we can now compare the two estimators based on their expected losses at each value of \\(\\theta\\). These are two functions:\n\\[\n\\theta \\rightarrow \\mathbb E_\\theta \\big[ L(\\theta, \\hat \\theta) \\big]\n\\]\nWhen comparing two estimators, we can have two scenarios.\n\nAt every value of \\(\\theta\\), one estimator is better than the other. This means that this estimator dominates the other. It would be very weird to choose a dominated estimator since you incur a net loss doing so.\nOne estimator dominates one some regions but is dominated on other regions. This means that there is no clear hierarchy between the two estimators.\n\nMathematically, we thus have constructed a partial order on the ensemble of estimators. This is far from perfect but it is (somewhat[^Having a single loss function fails to represent the full diversity of the ensemble. We are back to the issue of comparing conditional distributions. Perhaps we should consider multiple expected losses, constructed on multiple losses representing different aspects of faithfulness of \\(\\hat \\theta\\) to \\(\\theta\\)?]) faithful to the complexity of that ensemble. If we really want to have a full order, then we need to somehow collapse the function to a single value (while conserving the usual properties of an order):\n\neither by taking the \\(\\max\\): this justifies the study of minimax estimators,\nor by taking an average over \\(\\theta\\): this gives us a Bayesian perspective on loss functions which justifies Bayesian inference from another angle.\n\n\n\n1.2 Unbiased estimators\nA key subset of estimators is the class of unbiased estimators. These are such that their mean is correctly centered at \\(\\theta\\):\n\\[\n\\mathbb E_\\theta (\\hat \\theta) = \\theta\n\\]\nNote that, while the mean has obviously a key role in probability and statistics, it is not obvious that it would be the best way to encode the center of an estimator. Perhaps it could be more relevant, in some specific scenario, to consider estimators with an unbiased median, or an unbiased trimmed mean.\nThere are two reasons why we care about unbiased estimators. The first one is very practical. Among estimators, biased estimators are very common and straightforward to understand. The extreme case is the constant estimators: \\(\\hat \\theta = \\theta_0\\). Frustratingly, biased estimators are locally optimal according to the partial order we just defined on estimators. For example, the constant estimators cannot be beat at \\(\\theta_0\\), by definition. Restricting our study to unbiased estimators enables us to exclude these “cheating” estimators.\nThe second key reason is the fact that we can prove things on estimators with an unbiased mean. The key result is the Cramer-Rao theorem which establishes that any unbiased estimator has a minimal variance which depends on \\(\\theta\\) and the conditional model of the underlying data.\n\n\n1.3 UMVUE theory\nWe now have sufficient background to discuss what a UMVUE is. A UMVUE is an estimator that:\n\nis unbiased,\nhas minimal variance; i.e. is globally optimal for the expected \\(L^2\\) loss: \\(L^2(\\theta, \\hat \\theta) = (\\hat \\theta - \\theta)^2\\).\n\nThis is a very strong property since it requires that the partial order is somehow such that it has single global optimum.\nUnsurprisingly, UMVUE typically do not exist. The Lehmann-Scheffé theorem gives sufficient conditions for their existence (and unicity). Roughly, we can have an UMVUE for the moment-parameters of exponential families."
  },
  {
    "objectID": "posts/2024/10/understanding_umvue.html#a-simple-exemple-with-no-umvue",
    "href": "posts/2024/10/understanding_umvue.html#a-simple-exemple-with-no-umvue",
    "title": "Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)",
    "section": "2 A simple exemple with no UMVUE",
    "text": "2 A simple exemple with no UMVUE\nThe thing that was bugging was the following. If our data is such that we do not have a complete statistic, then the argument of the Lehmann-Scheffé theorem does not work and we cannot construct a UMVUE candidate. However, that does not prove that a UMVUE does not exist, just that our proof is too limited. This made me very curious about how to construct minimal examples in which a UMVUE does not exist. Here is one very simple construction.\nConsider the following model.\n\nWe have a one-dimensional parameter \\(\\theta \\in \\mathbb R\\).\nWe generate a random scale \\(S\\). For example[^For the curious reader, we will need that the distribution of \\(S\\) does not put too much weight near \\(0\\).], uniformly distributed on \\([1,2]\\): \\(S \\sim U[1,2]\\).\nWe generate a single datapoint by adding scaled Gaussian noise to \\(\\theta\\).\n\nIn a single equation:\n\\[\n\\begin{align}\n\\theta &\\in \\mathbb R \\\\\nS &\\sim U[1, 2] \\\\\nZ &\\sim \\mathcal N(0, 1) \\\\\nX &= \\theta + S Z \\\\\n\\end{align}\n\\]\n\\(X\\) is then a very natural unbiased estimator of \\(\\theta\\) with constant variance:\n\\[\n\\operatorname{var}(X) = E(S^2)\n\\]\nHowever, we cannot apply the Lehmann-Scheffé theorem since \\(s\\) provides ancillary information and the pair \\(S,X\\) is sufficient but not complete.\nCan we now construct other unbiased estimators which are somehow better than \\(X\\)? My insistence on recalling the Cramer-Rao bound is a strong hint that yes. Indeed, applying the Cramer-Rao bound to my example yields:\n\\[\n\\operatorname{var}(\\hat \\theta) \\geq \\big[ \\mathbb E (1 / S^2) \\big]^{-1} = I^{-1}\n\\]\nwhich is smaller than the variance of \\(X\\) due to the Jensen inequality applied to the convex function: \\(s \\rightarrow 1/s^2\\).\nWe can thus try to construct an estimator which reaches the Cramer-Rao bound. Consider estimators of the form:\n\\[\n\\hat \\theta = \\theta_0 + w(s) (X - \\theta_0)\n\\]\nwhere \\(w(s)\\) is some weight function. These estimators bias \\(X\\) towards \\(\\theta_0\\) by weighting the evidence according to the \\(w(s)\\) function.\nIn order for such estimators to be unbiased, they need to respect \\(\\mathbb E[w(S)] = 1\\). Plugging in \\(w(S) = 1 / I / S^2\\) (with \\(I\\) the Fisher information $I = E (1 / S^2) $), we find the \\(L^2\\) error of this estimator:\n\\[\n\\mathbb E (\\hat \\theta - \\theta)^2 = \\mathbb E (w(S) - 1)^2 (\\theta - \\theta_0)^2 + I^-1\n\\]\nThese weighted estimators are thus unbiased and locally optimal at \\(\\theta_0\\) where they exactly saturate the Cramer-Rao bound.\nThe existence of this family of estimators demonstrates the impossibility of the existence of a UMVUE in this example. Indeed, in order to be a UMVUE, an estimator would need to saturate the Cramer-Rao bound everywhere so that it improves on each single estimator in the family. This is impossible because, in order to saturate the Cramer-Rao bound at \\(\\theta_0\\), a unbiased estimator needs to be proportional to the score function at \\(\\theta_0\\) (the partial derivative with respect to \\(\\theta_0\\) of the log-likelihood) which is exactly how I constructed the local estimators. Since you cannot be proportional to multiple local estimators at once, there is no UMVUE.\nInstead, in this example, we need to choose between several reasonable estimators:\n\na minimax estimator \\(X\\) which is decent everywhere,\nlocally optimal estimators which are maximally efficient at \\(\\theta_0\\), which improve on \\(X\\) in a neighborhood of \\(\\theta_0\\) but are increasingly worse as we move away from \\(\\theta_0\\)."
  },
  {
    "objectID": "posts/2024/10/understanding_umvue.html#getting-value-out-of-an-example",
    "href": "posts/2024/10/understanding_umvue.html#getting-value-out-of-an-example",
    "title": "Some understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)",
    "section": "3 Getting value out of an example",
    "text": "3 Getting value out of an example\nI have just presented a single example of a model in which the UMVUE does not exist and this might seem irrelevant. Indeed, the field of mathematics does not place a lot of value on examples. However, I strongly disagree with this point of view: in my opinion, examples such as this one are a great platforms on which to build mathematical intuition which can then be strenghtened by mathematical rigor.\nHere, this example is very valuable because it is a toy-model, a simplified situtation in which we can carry out rigorous analysis but which remains representative of a large class of practical situations. This model is simple in the following ways:\n\nWe have a Gaussian model with unknown mean, i.e. the simplest statistical model with great mathematical properties.\nThe the scale of the noise is randomized but observed.\n\nAs a consequence, we have a clear unbiased estimator, we were able to derive the Fisher information, the locally optimal estimators (and they are unbiased!) and prove that the UMVUE does not exist. However, the overall structure of this model is universal. Many models have this structure in which we actually care about a single parameter, or a subset of parameters, and the other parameters are mostly a nuisance which impacts the quality of the information about the parameters of interest. In such situations, this analysis of the toy model shows that we will likely have a similar situation as here: we will again have a choice between globally efficient estimators and locally efficient ones."
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "",
    "text": "1 Installation errors for annoy\n  2 Installing the C compiler on Windows"
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html#installation-errors-for-annoy",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html#installation-errors-for-annoy",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "1 Installation errors for annoy",
    "text": "1 Installation errors for annoy\nWhen trying to install the annoy python library, you might encounter a cryptic error like: error: command 'gcc'. failed with exit status 1 This error is due to python not having access to the latest C compiler.\nWhen searching for a solution to this problem, I found many poor solutions which instead advised to use pre-compiled versions. Please, please, please never use random compiled code from unvetted third parties: that’s a major security risk! Instead, this error is very easy to fix: we just have to install the latest version of the C compiler."
  },
  {
    "objectID": "posts/2024/04/installing_spotify_annoy_windows.html#installing-the-c-compiler-on-windows",
    "href": "posts/2024/04/installing_spotify_annoy_windows.html#installing-the-c-compiler-on-windows",
    "title": "Installing spotify’s annoy on a Windows machine",
    "section": "2 Installing the C compiler on Windows",
    "text": "2 Installing the C compiler on Windows\nMicrosoft provides a download link for the C++ build tools. This actually downloads an installer util which will itself install the build tools:\n\nDownload the file from the Microsoft website.\nRun it.\nAmong the many installation options, select the latest version Visual Studio Build Tools (at the time of this writing, 2022).\n\n\nAnd done! You should now be able to install annoy:\npip install annoy\nWhen installing python packages, please keep in mind the good practice of creating a separate environment for each project. I recommend using the built-in venv util for this, and accessing it via the VSCode command line."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html",
    "href": "posts/2024/03/styling_a_quarto_blog.html",
    "title": "Styling a Quarto blog.",
    "section": "",
    "text": "1 Style objectives\n  2 Customization steps\n  \n  2.1 Summary\n  2.2 Overall style\n  2.3 Code blocks\n  2.4 Mermaid diagrams\n  2.5 Math blocks\n  2.6 Plotly figures\n  \n  3 Examples"
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#style-objectives",
    "href": "posts/2024/03/styling_a_quarto_blog.html#style-objectives",
    "title": "Styling a Quarto blog.",
    "section": "1 Style objectives",
    "text": "1 Style objectives\nWith any project, I like to start with a small list of objectives, instead of diving straight-away into code. This helps keep the project grounded.\nAs far as styling this website goes, I want to:\n\ncustomize the overall styling of the page.\ncustomize the font.\nmake sure the style is correctly applied to all special blocks:\n\ncode blocks.\nmermaid diagrams.\nMathJax math blocks.\nplotly figures.\n\n\nPlease refer to Section 3 for examples of how my current style is applied to various Quarto elements."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#customization-steps",
    "href": "posts/2024/03/styling_a_quarto_blog.html#customization-steps",
    "title": "Styling a Quarto blog.",
    "section": "2 Customization steps",
    "text": "2 Customization steps\n\n2.1 Summary\nI’m using:\n\nthe cosmo bootstrap theme. Once Quarto 1.5 is released, I’ll add a light-dark switch.\nEG Garamond as a text font, Fira Code for code blocks, mathjax-fira for math blocks.\nfor mermaid, I’m using the cosmo version of mermaid that Quarto introduces, but modified to have black text.\nfor code highlighting, I’m using the github style (I’m unsure if it actually matches what shows on github).\nI haven’t styled plotly plots. It is a bit complicated.\n\nMy current configuration is (or check out the latest version here):\n\n\n_quarto.yml\n\nformat:\n    html:\n        theme:\n            - cosmo\n            - style.scss\n        mainfont: \"EB Garamond, Georgia, serif\"\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n        \n        html-math-method:\n            method: mathjax\n1            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n2                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n                MathJax = {\n3                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n\n\n1\n\nImporting Mathjax version 4.0 beta.\n\n2\n\nImporting the fonts.\n\n3\n\nUnrelated code: this numbers all equations.\n\n\nI also added an external style sheet: changing the font was causing a bad alignment in annotated code blocks, and I had to turn off a peculiar styling that Garamond applies to numerals. Check out the latest version here:\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\nbody {\n    // By default, EG Garamond uses \"old-styles\" numerics which have a baseline offset\n    // for some numerals. This turns that ugly feature off.\n    font-variant-numeric: lining-nums;\n}\n\ndl.code-annotation-container-grid dt {\n    // needed because I have changed the mono font\n    line-height: 20px !important;\n}\n\n$mermaid-font-family: \"EB Garamond, Georgia, serif\";\n$mermaid-label-bg-color: #000000;\n$mermaid-label-fg-color: #000000;\n\n\n\n2.2 Overall style\nFor the overall style, I would like:\n\nsomething sleek and modern.\nsomething with a light and dark mode.\nsomething slightly personalized (but not too much! I’m really bad at graphical design).\nsomething simple.\n\nGiven that Quarto has built-in support for various free themes, I’ve decided to keep it simple and just use that.\n\nInitially, I used to combo of flatly + darkly but I dislike some of the blues they use.\nI’ve seen that, in version 1.5, Quarto will support light and dark mode styling for all themes. I’ve thus switched to using the cosmo theme. It is sleek, modern, and simple, like I wanted. Once the necessary update is pushed out, I will add light/dark mode and I will perhaps play around with the colors.\nto put a tiny bit of personalization, I’ve modified the text font to use Garamond instead (with appropriate fallbacks, just in case):\n\n\n_quarto.yml\n\nformat:\n    html:\n        mainfont: \"EB Garamond, Georgia, serif\"\n\nI’m unsure whether you need to add a call to download the font from the web in the HTML header: I’ve added one anyway.\nannoyingly, Garamond uses old-style numerals by default. This setting makes some numerals, such as 3 and 4, align the middle of the character to the baseline of the text. I’m honestly surprised that this even exists and I’ve turned it off.\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\nbody {\n    // By default, EB Garamond uses \"old-styles\" numerics which have a baseline offset\n    // for some numerals. This turns that ugly feature off.\n    font-variant-numeric: lining-nums;\n}\n\n\n\n\n2.3 Code blocks\nFor code blocks, there are several basic choices available:\n\nthe font. Again, I’m going for a bit of personalization and using Fira code.\ncode highlighting.\n\nthe default setting is very grey: let’s try to have more color. 1\nafter testing most of them, my shortlist was:\n\ngithub\nsolarized\npygments\nbreeze\ngruvbox\n\nIn the end, I’ve decided on github. It’s light but detailed.\nIt would probably be worth it to explore a little bit more if I want something with more colors, like my setting on vscode.\n\n\nThe resulting website configuration is:\n\n\n_quarto.yml\n\nformat:\n    html:\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n\nChanging the font caused a bad alignment in annotated code blocks, so I had to introduce an external style. I must be doing something slightly wrong because I needed to add an !important tag to ensure that this took priority over the Quarto styling.\n\n\nstyle.scss\n\n/*-- scss:defaults --*/\ndl.code-annotation-container-grid dt {\n    // needed because I have changed the mono font\n    line-height: 20px !important;\n}\n\n\n\n2.4 Mermaid diagrams\nMermaid diagrams can either use:\n\nreactive styling, based on the base style (ie: cosmo for me): I didn’t find that it worked out great for cosmo since it uses a very luminous blue for text.\nusing the built-in themes. I didn’t think it worked great with the cosmo main theme.\ncustom styling. ⚠️ I struggled with this since it is not compatible with mermaid built-in themes ⚠️. I’ve just used this to set the font to black and the font-family to match the text font.\n\nI’ll probably want to return to this in the future, but for the time being, the diagrams are going to be neutral and easy on the eyes: good enough for now.\n\n\nstyle.scss\n\n$mermaid-font-family: \"EB Garamond, Georgia, serif\";\n$mermaid-label-bg-color: #000000;\n$mermaid-label-fg-color: #000000;\n\n\n\n2.5 Math blocks\nCurrently, the released version of MathJax: version 3.0, does not support much in the way of customization. Apparently, version 4.0 will allow to choose the font. Well, then let’s use the 4.0 beta version!\n\n\n_quarto.yml\n\nformat:\n    html:\n        html-math-method:\n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n                MathJax = {\n                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n\nI’ve set the font to mathjax-fira, to align with code blocks.\n\n\n2.6 Plotly figures\nAs far as I can tell, plotly does not support css-based customization of its figures. It is apparently possible to use existing or create new python templates to have consistent styling accross figures. Apparently, this dash-bootstrap-components library (pip link) includes templates that match the bootstrap styles that are built-into Quarto? It should even work with light-dark switching? I’ll make a note of it, and return to this if it becomes necessary."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#sec-examples",
    "href": "posts/2024/03/styling_a_quarto_blog.html#sec-examples",
    "title": "Styling a Quarto blog.",
    "section": "3 Examples",
    "text": "3 Examples\nSince this page is on the website, I’ll write down some examples, to have a visual reference.\nText formatting:\n\nitalics, bold, bold italics\nsuperscript2 / subscript2\nstrikethrough\n\n\nA blockquote\n\nA keyboard shortcut: Shift-Ctrl-PShift-Ctrl-P. 2\nA table:\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\nInline math \\(0+0=0\\) and inline code compute_something(foo). And their display equivalents:\n\\[\\begin{equation}\n0 + 0 = 0\n\\end{equation}\\]\n\\[\\begin{align}\n(a+2)(a-2) &= a^2 + 2a - 2a - 4 \\\\\n            &= a^2 - 4\n\\end{align}\\]\nThis block doesn't have syntax coloring.\nThe next one has python syntax coloring and a file title.\n\n\nsquare.py\n\ndef square(x: int) -&gt; int:\n    \"\"\"Compute the squared value.\n\n    This computes the square of the input.\n    \"\"\"\n    # This is high-level math!\n    return x**2\n\nA mermaid diagram, with figure styling:\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n This is a caption. \n\n\n\nA callout block:\n\n\n\n\n\n\nNote\n\n\n\nNote that there are five types of callouts, including: note, tip, warning, caution, and important.\n\n\nThis is a span that has the class aside which places it in the margin without a footnote number.\nThis is a plotly figure:\n\nimport plotly.express as px\nimport plotly.io as pio\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", \n                 color=\"species\", \n                 marginal_y=\"violin\", marginal_x=\"box\", \n                 trendline=\"ols\", template=\"simple_white\")\nfig.show()\n\n                                                \n\n\nAnnotated code block:\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n1penguins |&gt;\n2  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area."
  },
  {
    "objectID": "posts/2024/03/styling_a_quarto_blog.html#footnotes",
    "href": "posts/2024/03/styling_a_quarto_blog.html#footnotes",
    "title": "Styling a Quarto blog.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNB: the fact that the syntax highlighting is so grey could be due to the cosmo theme. Maybe the colors are computed from the base colors of the theme? That would be a weird choice.↩︎\nUnless I start doing IT posts, this probably won’t be useful, but there’s no reason to be lazy and not check it.↩︎"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html",
    "href": "posts/2024/03/quarto_is_better.html",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "",
    "text": "1 The sales pitch\n  2 Latex use cases\n  3 Quarto improvements\n  4 Try Quarto"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#the-sales-pitch",
    "href": "posts/2024/03/quarto_is_better.html#the-sales-pitch",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "1 The sales pitch",
    "text": "1 The sales pitch\nLatex has a very simple sales pitch.\n\nDo you want to author high-quality technical documents? Then we have the right tool for the job! (And it’s also mandatory if you plan on publishing it in an academic journal).\n\nPretty compelling!\nBut Latex is also 40 year old software that has a number of issues. What if I told you that you can better by using Quarto?\n\npublish to a much wider variety of formats including HTML.\ndynamic figures in HTML.\nsimplify the syntax of your source document.\nintegrate with Python, Javascript, Julia, R for figure generation (or any sort of content generation).\n\nEven more compelling, right?"
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#latex-use-cases",
    "href": "posts/2024/03/quarto_is_better.html#latex-use-cases",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "2 Latex use cases",
    "text": "2 Latex use cases\nLet’s start by reviewing typical use cases for Latex. Here are some typical use cases:\n\nI want to author a technical document that I will distribute myself. E.g. exercise sheets, presentations, notes.\nI want to author a group of technical documents that I will distirbute myself. E.g. for an academic class, I would write class notes, multiple presentations, and exercise sheets.\nI want to author a research article that I will distribute myself.\nI want to author a research article to the specific requirements of the Journal of Awesome Research.\n\nAmong these requirements, point 4. is the most constrained. Many technical journals have integrated Latex as part of their publishing workflow. They thus require authors to submit their manuscripts as Latex files so that they can be processed automatically. For example, as part of the review process, the document is first compiled using review settings (increased margins, skipping lines, more space between paragraphs) whereas the final published article is compiled using more standard settings.\nEven if you aren’t forced into using Latex, it still provides a decent solution for all these points. Once you have mastered it, Latex produces beautiful and clear documents with consistent styling. It is slightly trickier to coordinate a corpus of documents but it is still doable.\nBut Latex is also more than 40 years old and like other long-running software, it has got a few limits. My main criticisms would be:\n\nheavy syntax. Obviously, with sufficient work, you reach a point where you can parse Latex (and having syntactic coloring helps!), but the ratio of code to content is never negligible.\npoor documentation. Latex is a confusing language and accessing the right level of detail is extremely hard. Most times, when you search for information about the right solution to a problem, you find an old stackoverflow post with a recipe with no explanation. Hopefully, that fixes your issue but your understanding doesn’t grow.\nvery steep learning curve. These first two points compound to make Latex very tricky to learn.\nslow compilation. Seeing the rendered result of what you are currently working on can take multiple seconds or even minutes.\nlimited output formats. Latex is built to generate pdf content, but HTML documents can provide a much better support:\n\nthey adapt to the screen size.\nthey support dynamic figures (zooming, selecting subsets of data, etc.).\nwith some degree of javascript mastery, the sky is the limit.\n\nhard to extend. It is hard to interact with the Latex core and integrate additional software with it.\n\nIf want to expand on point 5. a bit more, since it might seem like a fairly minor point. To me, a key aspect of evaluating software is seeing how it can integrate with other software: the value of software is also derived from the environment surrounding it. For example, if every math journal on earth used MSWord (shudders), then the value of Latex would be considerably diminished for mathematicians. The fact that it is not possible to easily integrate tweaks and improvements from other software into Latex is a mark against it. For example, if suddenly someone invents a great javascript visualization tool, then we lose value by not being able to use that easily in Latex.\nNow, please understand that these flaws do not mean that Latex is bad. It is a great system for typesetting documents. But we can do better, even if we want to remain compatible with the requirement of being able to produce a .tex file for journal submission. The solution is Quarto."
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#quarto-improvements",
    "href": "posts/2024/03/quarto_is_better.html#quarto-improvements",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "3 Quarto improvements",
    "text": "3 Quarto improvements\nQuarto is a modern alternative to Latex. It is built for publishing technical documents, whether academic or not. Its key features are:\n\nclear and simple markdown syntax.\nbuilt-in support for generating dynamic content in Python, Javascript, Julia, or R.\nfocus on producing crisp HTML documents.\ndynamic figures.\nexpanded syntax for code blocks, diagrams, callouts, etc.\nLatex compatibility: Quarto documents can always be exported to Latex format.\nsupport for a huge variety of output formats including:\n\nMSWord and Powerpoint (and their open office variants), when you want to do manual tweaking.\nSeveral wiki formats.\nSeveral ebook formats.\n\nsupport for coordinated ensemble of documents: websites, books, etc. This website is written in Quarto.\neasily tweakable. Quarto has built-in support for extensions and a large library of user-supplied extensions.\n\nPersonally, the shift to markdown syntax is the biggest draw for me. The key idea of markdown is that simplicity is essential:\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nCompare the following two fairly minimal documents with a title and a list, in markdown and Latex:\n# Choosing a language\n\nWhen choosing a language to write content:\n\n1. Source document should be easily human readable.\n1. Source document should be easily computer readable, except where it conflicts with rule 1.\n\\section{Choosing a language}\n\nWhen choosing a language to write content:\n\n\\begin{enumerate}\n\\item Source document should be easily human readable.\n\\item Source document should be easily computer readable, except where it conflicts with rule 1.\n\\end{enumerate}\nNot quite convinced by this simple example? Then I leave as an exercise to you, dear reader, to imagine the Latex code that would generate these two blocks of raw code with syntactic coloring."
  },
  {
    "objectID": "posts/2024/03/quarto_is_better.html#try-quarto",
    "href": "posts/2024/03/quarto_is_better.html#try-quarto",
    "title": "Don’t use Latex: Quarto is better!",
    "section": "4 Try Quarto",
    "text": "4 Try Quarto\nCurious? Then give it a try and download Quarto!\nTry the following:\n\nget familiar with the basic syntax.\ntry mermaid diagrams.\nif you are already familiar with Python, try a plotly interactive figure\nrender to html and pdf, and compare the results.\nrender to Latex.\n\nAre you curious how Quarto manages to do all that? I’ll address that in a future post. Unlike Latex, it is very possible to understand what goes on under-the-hood of Quarto and tweak it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guillaume Dehaene",
    "section": "",
    "text": "My goal is to tackle interesting AI problems by combining expert knowledge of deep learning, Bayesian inference, statistical theory and Python.\nWebsite - Blog - Email - LinkedIn"
  },
  {
    "objectID": "index.html#sec-header",
    "href": "index.html#sec-header",
    "title": "Guillaume Dehaene",
    "section": "",
    "text": "My goal is to tackle interesting AI problems by combining expert knowledge of deep learning, Bayesian inference, statistical theory and Python.\nWebsite - Blog - Email - LinkedIn"
  },
  {
    "objectID": "index.html#sec-professional-experience",
    "href": "index.html#sec-professional-experience",
    "title": "Guillaume Dehaene",
    "section": " Professional experience",
    "text": "Professional experience\n\nArtificial Intelligence Engineer\n\nGeodaisics | Sept. 2023\n\nCreation and implementation of the internal validation procedures of AI models.\nCreation of a shared workflow and tooling for the research team.\nWrote and reviewed developpement procedures for the internal Quality Management System in accordance with ISO 13485 / 14971 / 62304 / …\n\n\n\n\nSenior data Engineer\n\nMarelli - Smart Me Up | July 2022 - July 2023\n\nProject supervision: embedded gesture recognition (precision &gt;95%).\nDefinition of data needs for the Grenoble teams.\nResponsible for the internal online data annotation tool.\nSupervision of the annotation team (12 people). Creation and supervision of the data sharing procedures.\n\n\n\n\nR&D computer vision engineer\n\nMarelli Smart Me Up | April 2020 - June 2022\n\nCreation and implementation of an unsupervised stereo vision algorithm.\nCreation of an internal library to standardize R&D activity on neural networks.\nFeatures: automated code standards, unit testing, web visualization of results, reproducibility.\nTechnology watch on computer vision. Algorithms adapted: SwAV, Mean teacher, depth estimation, transformer.\n\n\n\n\nAssistant professor in Statistics\n\nEcole Polytechnique Fédérale de Lausanne | Sept. 2016 - April 2020\n\nNeurips 2016 AABI workshop Disney Research Paper Awards awarded for: Expectation Propagation performs a smoothed gradient descent, G. Dehaene.\nCreation and implementation of a method to validate the results of a Bayesian statistical analysis\nSupervision of one PhD. and three master theses."
  },
  {
    "objectID": "index.html#sec-skills",
    "href": "index.html#sec-skills",
    "title": "Guillaume Dehaene",
    "section": " Skills",
    "text": "Skills\n\nPrograming\n\n\n\n\n\n\n\nPython  (Django, Pytorch, Tensorflow)\n\n\n\nRust\n\n\n\nJavascript\n\n\n\nHTML, CSS\n\n\n\nGit\n\n\n\nDocker\n\n\n\nLinux admin\n\n\n\n\n\n\nManagement\n\n\n\n\n\n\n\nAgile project management\n\n\n\nR&D supervision\n\n\n\nCI / CD\n\n\n\n\n\n\nArtificial Intelligence\n\n\n\n\n\n\n\nNeural networks\n\n\n\nComputer vision\n\n\n\nStatistical theory\n\n\n\nBayesian statistics\n\n\n\n\n\n\nLanguages\n\n\n\n\n\n\n\nFrench\nNative\n\n\nEnglish\nBilingual"
  },
  {
    "objectID": "index.html#sec-education",
    "href": "index.html#sec-education",
    "title": "Guillaume Dehaene",
    "section": " Education",
    "text": "Education\nPh.D. in neuroscience and statistics\nUniversité de Genève, Université Paris- Descartes\n2012-2016\nEcole Polytechnique engineer diploma\nEcole Polytechnique Paris\n2008-2012\nMaster in Cognitive Science\nENS-Paris, EHESS, Université Paris-Descartes\n2011-2012"
  },
  {
    "objectID": "index.html#sec-publications",
    "href": "index.html#sec-publications",
    "title": "Guillaume Dehaene",
    "section": " Publications",
    "text": "Publications\nA deterministic and computable Bernstein-von Mises theorem\nG. Dehaene, 2019\nPresented at: Séminaire BIG (Grenoble), Séminaire de Statistique de Berne\nExpectation Propagation in the large data limit\nG. Dehaene and S. Barthelmé, 2018\nJournal of the Royal Statistical Society - Series B\nPresented at: Séminaire BIG (Grenoble), Séminaire de Statistique de Genève\nExpectation Propagation performs a smoothed gradient descent\nG. Dehaene, 2016\nAdvances in Approximate Bayesian Inference NeuRIPS workshop\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nBounding errors of Expectation-Propagation\nG. Dehaene and S. Barthelmé, 2015\nNeurIPS 2015"
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Guillaume Dehaene",
    "section": "Hobbies",
    "text": "Hobbies\n\nJudo\nBandes dessinées\nCompetitive coding: Advent of code"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog. I write mostly about:\n\nstatistics, with a focus on Bayesian statistics, variational inference and theory.\ntypesetting documents with Quarto and alternatives.\nprogramming, mostly in Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating a conjecture from Stack Overflow 2/2\n\n\n\n\n\n\nmath\n\n\nstack-overflow\n\n\n\nA user on Stack Overflow proposes an interesting problem featuring modulos (and thus arithmetic) and sampling without replacement. I write a detailed mathematical analysis of the problem and answer the initial question. \n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\n\nInvestigating a conjecture from Stack Overflow 1/2\n\n\n\n\n\n\nmath\n\n\nstack-overflow\n\n\n\nA user on Stack Overflow proposes an interesting problem featuring modulos (and thus arithmetic) and sampling without replacement. I attempt to understand it better by identifying easy cases and performing some simulations. \n\n\n\n\n\nDec 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome understanding for UMVUE (Uniformly Minimum Variance Unbiased Estimator)\n\n\n\n\n\n\nmath\n\n\nstackoverflow\n\n\n\nThe study of Uniformly Minimum Variance Unbiased Estimators (UMVUE) is an old-fashioned but interesting bit of statistical theory. In this post, I consider a very minimal toy-model with no UMVUE and show a family of estimators which are efficient at a single point in space and suboptimal everywhere else. \n\n\n\n\n\nOct 5, 2024\n\n\n\n\n\n\n\n\n\n\n\nPruning git local branches.\n\n\n\n\n\n\ngit\n\n\n\nA short guide to the commands necessary to automatically prune unneeded local branches from git. \n\n\n\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nConnecting gitlab to redmine\n\n\n\n\n\nConnecting gitlab to a redmine ticket-management server. Why? Because their ticket management is slightly better. \n\n\n\n\n\nJun 10, 2024\n\n\n\n\n\n\n\n\n\n\n\nAfter action report: running a simple statistical experiment\n\n\n\n\n\n\nstatistics\n\n\ndata science\n\n\n\nI wanted to test a simple statistical method so I ran a simulation-based experiment. This post gives an after action report of my priorities and process to validate my method. \n\n\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nAligning a DICOM to a NIFTI, pixel-by-pixel\n\n\n\n\n\n\nrandom\n\n\n\nI have struggled at work to find how to align pixels. These pixels are particuarly vexing: they came from a DICOM file or a NIFTI file, which are both formats with awful documentation. If this helps a single person deal with this issue, then this post has reached its goal. \n\n\n\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nInstalling spotify’s annoy on a Windows machine\n\n\n\n\n\n\npython\n\n\n\nSpotify has shared the annoy library for nearest-neighbor calculations. However, trying to install it on Windows can give cryptic errors if python does not have access to the latest C compiler. In this post, I tell you how to fix this issue. \n\n\n\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nConfiguring a Quarto website\n\n\n\n\n\n\nquarto\n\n\n\nSetting up a Quarto website is not trivial. In this post, I discuss how to setup the _quarto.yml configuration file optimally. \n\n\n\n\n\nMar 31, 2024\n\n\n\n\n\n\n\n\n\n\n\nStyling a Quarto blog.\n\n\n\n\n\n\nquarto\n\n\ngithub pages\n\n\n\nCustomizing a website is fun and interesting. Let’s learn how to tweak the appearance of a Quarto website to make it our own. \n\n\n\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\n\n\n\nDon’t use Latex: Quarto is better!\n\n\n\n\n\n\ntypesetting\n\n\nquarto\n\n\nlatex\n\n\n\nLatex is the defacto standard typesetting tool in large fractions of the academic world, but can we do better? Now that Quarto is around, the answer is yes! \n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\ngithub pages setup for this website\n\n\n\n\n\n\nquarto\n\n\ngithub pages\n\n\n\nSetting up a Quarto website is not trivial. Here is my setup to publish a single github repository using the www.gandi.net DNS. \n\n\n\n\n\nMar 27, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html",
    "title": "github pages setup for this website",
    "section": "",
    "text": "1 Desired setup\n  2 Steps\n  \n  2.1 Standard deploy to github pages\n  2.2 Custom domain for github page"
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html#desired-setup",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html#desired-setup",
    "title": "github pages setup for this website",
    "section": "1 Desired setup",
    "text": "1 Desired setup\nSetting up a Quarto website on github pages is fairly straightforward: you just need to follow the docs! But in case you need some help, I’m sharing here all the steps I’ve went through.\nIn my case, I thought there would be a slight complication. I want to have multiple pages from multiple repositories:\n\none for my personal website (on which you are right now).\none for my other Quarto projects. For each one, I want to have an associated github pages which can serve as a demo or readme.\n\nMy initial plan was the following:\n\nI want the personnal website to use the root domain guillaumedehaene.com and the default subodmain www.guillaumedehaene.com while I want project pages to use another subdomain demo.guillaumedehaene.com. I would like project_a to live under: demo.guillaumedehaene.com/project_a, etc.\n\nThis didn’t work out at all, so I’ve brought all repositories together for now, a summarized in Figure 1.\n\n\n\n\n\n\nflowchart LR\n    subgraph github\n        direction TB\n        A\n        C\n        E\n    end\n    subgraph urls\n        direction TB\n        B\n        BB\n        D\n        DD\n        F\n        FF\n    end\n    A[Website repository] --&gt; B[guillaumedehaene.github.io]\n    B -- use URL --- BB[www.guillaumedehaene.com]\n    C[Project A] --&gt; D[guillaumedehaene.github.io/project_a] -- use URL --- DD[www.guillaumedehaene.com/project_a]\n    E[Project B] --&gt; F[guillaumedehaene.github.io/project_b] -- use URL --- FF[www.guillaumedehaene.com/project_b]\n\n\n\n\nFigure 1\n\n\n\n\n\nIn the future, it would be straightforward to pull out any demo into its own subdomain, but I’ll first test out the current setup and see if I can make it work. I’m a bit unhappy about the current demo having a different style than the rest of the website but I can live with it."
  },
  {
    "objectID": "posts/2024/03/github_pages_setup_with_quarto.html#steps",
    "href": "posts/2024/03/github_pages_setup_with_quarto.html#steps",
    "title": "github pages setup for this website",
    "section": "2 Steps",
    "text": "2 Steps\n\n2.1 Standard deploy to github pages\n\n2.1.1 Quarto configuration\nFirst, we need a Quarto website. I have decided that I will render the website on my machine, and that github will just serves the files once they are uploaded (corresponding to this section of the docs).\nThus, we just change the global website configuration so that quarto render compiles to the docs folder:\n\n\n_quarto.yml\n\nproject:\n    type: website\n    output-dir: docs\n\n\n\n2.1.2 github configuration\nNow, we need to setup github:\n\nadd a .nojekyll file at the root of repository.\ncreate a new github repository with the specific repository name USERNAME.github.io.\n\nnormally, github associates USERNAME.github.io/REPOSITORY_NAME to a given repository.\nif we use this special repository name, github uses the root URL USERNAME.github.io.\n\npush the project to the remote repository.\ngo to settings &gt;&gt; pages and tell github pages to publish from the docs folder on the main branch.\n\nAt this point, the page website guillaumedehaene.github.io is fully functional. Now we need to host it on its the custom domain.\n\n\n\n2.2 Custom domain for github page\n\n2.2.1 Quarto configuration\nWe will make it so that the website answers to my custom url www.guillaumedehaene.com. If we omit this step, the website will act as if the custom url just acts as a redirect to the pages url.\nFirst, we need to tell Quarto about the custom url:\n\ncreate a cname file at the project root containing the custom url:\n\n\ncname\n\nwww.guillaumedehaene.com\n\nmodify the project configuration file: _quarto.yml to include the cname file and so that Quarto uses the custom url:\n\n\n_quarto.yml\n\nproject:\n    # publishing to github pages\n    # ref: https://quarto.org/docs/publishing/github-pages.html#render-to-docs\n    type: website\n    output-dir: docs\n    resources:\n        - CNAME\n\nwebsite:\n    title: \"Guillaume Dehaene\"\n    site-url: www.guillaumedehaene.com\n\nrender and push to github.\n\n\n\n2.2.2 Gandi DNS configuration\nWe will configure the DNS server so that it knows to serve the custom url to the right github pages url.\nOn gandi, I get to set the DNS configuration manually.\n@ 10800 IN SOA ns1.gandi.net. hostmaster.gandi.net. 1711531617 10800 3600 604800 10800\n@ 1800 IN A 185.199.108.153\n@ 1800 IN A 185.199.109.153\n@ 1800 IN A 185.199.110.153\n@ 1800 IN A 185.199.111.153\nwww 10800 IN CNAME guillaumedehaene.github.io.\ndemo 10800 IN CNAME guillaumedehaene.github.io.\nNB: the IP addresses are documented on this page of the github docs.\nI’m honestly a bit confused about exactly what is going on under the hood here, but as far as I can tell:\n\nthe first four lines set the binding between the root domain guillaumedehaene.com and my github page.\nthe fifth line sets the binding specifically for the www subdomain.\nthe sixth line sets the binding specifically for the demo subdomain. Initially, I wanted to use this to host the demo pages for my Quarto projects, but I’m not using it currently.\n\nThere’s an underlying magic step where github knows, because of the cname file that we need to add in each repository, which pages correspond to which subdomain:\n\nthe website cname file contains www.guillaumedehaene.com on websites that should use that domain. Currently, that is all my repositories.\nthe website cname file contains demo.guillaumedehaene.com on websites that should use that domain. I have tested that this works on a single demo repo. I’m not sure what would happen if I hooked up two. Github might be very confused if I do that 🤷.\n\n\n\n2.2.3 github configuration\nFinally, we tell github about the cname:\n\ngo to settings &gt;&gt; pages and tell github pages to use a Custom domain and specify your desired full domain.\nwait a day, then enable https.\n\nAs a security precaution, please also verify your domain with github pages."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html",
    "href": "posts/2024/03/quarto_website_configuration.html",
    "title": "Configuring a Quarto website",
    "section": "",
    "text": "1 Quarto website configuration\n  2 My choices\n  3 Page-specific options\n  4 Other interesting options"
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#quarto-website-configuration",
    "href": "posts/2024/03/quarto_website_configuration.html#quarto-website-configuration",
    "title": "Configuring a Quarto website",
    "section": "1 Quarto website configuration",
    "text": "1 Quarto website configuration\nQuarto offers a lot of configuration options for html and also for websites! In this post, I focus on the configuration options I believe to be the most important. These are, of course, a reflection of my personnal priorities, but I hope it can serve as a stepping stone towards your own mastery of Quarto.\nRemember that all of these options can be modified on a document-by-document basis by modifying the yaml header of the .qmd file."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#my-choices",
    "href": "posts/2024/03/quarto_website_configuration.html#my-choices",
    "title": "Configuring a Quarto website",
    "section": "2 My choices",
    "text": "2 My choices\nHere is my current global settings file with comments. Check out the latest version here.\n\n\n_quarto.yml\n\nproject:\n1    type: website\n    output-dir: docs\n    resources:\n        - CNAME\n\nwebsite:\n    title: \"Guillaume Dehaene\"\n    site-url: www.guillaumedehaene.com\n2    page-footer: \"This website was created with [Quarto](https://quarto.org/).\"\n3    page-navigation: true\n    back-to-top-navigation: true\n    navbar:\n        left:\n            - blog.qmd\n        right:\n            - publications.qmd\n            -   href: about.html\n                # file: about.qmd\n                text: About me\n\nformat:\n    html:\n4        theme:\n            - cosmo\n            - style.scss\n        mainfont: \"EB Garamond, Georgia, serif\"\n        monofont: Fira Code, consolas, courier, monospace\n        highlight-style: github\n\n        html-math-method:\n            method: mathjax\n            url: \"https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js\"\n        include-in-header:\n            text: |\n                &lt;style&gt;\n                @import url('https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&display=swap')\n                @import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&display=swap')\n                &lt;/style&gt;\n                &lt;script&gt;\n5                MathJax = {\n                    tex: {\n                        tags: 'ams'  // should be 'ams', 'none', or 'all'\n                    },\n6                    output: {\n                        font: 'mathjax-fira'\n                    }\n                };\n                &lt;/script&gt;\n        \n7        toc: true\n        toc-location: right-body\n        \n8        number-sections: true\n        number-depth: 3\n\n9        shift-heading-level-by: 1\n        anchor-sections: true\n\n10        code-copy: true\n\n11        code-tools: true\n\n12        freeze: auto\n\n13        link-external-icon: true\n        link-external-newwindow: true\n\n14        lang: en\n\n15        strip-comments: true\n\n\n1\n\nPublishing to github pages using this method.\n\n2\n\nA simple footer which I’ll need to improve latter.\n\n3\n\nAdd page navigation information.\n\n4\n\nStyling options. See my blog post on how I built the styling.\n\n5\n\nNumber (almost all)1 all math equations.\n\n6\n\nStyling options. See my blog post on how I built the styling.\n\n7\n\nIncluding a toc menu:\n\n8\n\nNumbering sections, more or less like a Latex document. Headers from # to ### get numbered.\n\n9\n\nAdding an anchor symbol to headers. This is purely about communicating to the user that they can link to headers. shift-heading-level-by: 1 is necessary here. It converts # titles to &lt;h2&gt; instead of &lt;h1&gt;. &lt;h1&gt; elements do not receive the anchor treatment.\n\n10\n\nAdd a “copy code” anchor to code blocks. Another nice user-facing feature.\n\n11\n\nAdd a button to view the Quarto markdown source of each document. This is not a super useful feature but it has no downside.\n\n12\n\nAvoid repeating Python calculations to decrease document rendering time.\n\n13\n\nAdd additional styling to make obvious to the user which links are external and open these in new windows / tabs.\n\n14\n\nUse english language for automated language construction.\n\n15\n\nRemove html comments from the source. Any comment I write are about the Quarto content, and are not relevant for the HTML document."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#page-specific-options",
    "href": "posts/2024/03/quarto_website_configuration.html#page-specific-options",
    "title": "Configuring a Quarto website",
    "section": "3 Page-specific options",
    "text": "3 Page-specific options\nThese settings get applied globally accross the website, but they are not appropriate for special pages. Thankfully, we can override the global parameters on a given by giving them another value in the yaml header.\nI’ve used this feature to customize the settings on the about.html page.\nanchor-sections: false\nnumber-sections: false\ncode-tools: false\nThis page has a very different type of content. Numbering sections, anchoring all sections, and providing the page source are all features which distract from that content. Interestingly, some features are automatically removed just by using the about page format."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#other-interesting-options",
    "href": "posts/2024/03/quarto_website_configuration.html#other-interesting-options",
    "title": "Configuring a Quarto website",
    "section": "4 Other interesting options",
    "text": "4 Other interesting options\nThese options are presented in the order in which they appear in the Quarto docs:\n\nsmooth-scroll: instead of jumping to the target anchor, scroll the page smoothly. I feel that this improves the wow factor of the website, but it can also be annoying. It’s definitely a matter of personnal preference.\nhtml-math-method: choose the math renderer. mathjax is the best as far as I’m concerned (especially with version 4.0 on the way) but you can try out the other options.\nlinestretch: more space between lines. Can be the right choice if you want to produce a document that a reviewer would print. That’s not really a website option though.\nlightbox: Give a gallery scroller to the figures. I feel that this setting depends on the type of documents you write. For an article, I feel this should be on. I believe that Nature and several other journals use a similar type of styling for their figures.\ncrossref: if you want to customize the behavior of cross-references.\ninclude-in-header, include-before-body, include-after-body: these commands inject html code or files in specific sections of the final document. If you need to do some advanced features, you probably need this.\ncopyright, license: the information gets added to the appendix of the document."
  },
  {
    "objectID": "posts/2024/03/quarto_website_configuration.html#footnotes",
    "href": "posts/2024/03/quarto_website_configuration.html#footnotes",
    "title": "Configuring a Quarto website",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEquations can either be delimited by $$ $$ signs or an ams-delimiter (for example: \\begin{equation} \\end{equation}). This numbers all ams equations. I’ll have more to say about math in future posts.↩︎"
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html",
    "href": "posts/2024/04/aligning_dicom_pixels.html",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "",
    "text": "1 The problem\n  2 The solution\n  \n  2.1 A bit of geometry\n  2.2 Finding the matrices\n  \n  3 Bloopers\n  4 References"
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#the-problem",
    "href": "posts/2024/04/aligning_dicom_pixels.html#the-problem",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "1 The problem",
    "text": "1 The problem\nWe encountered the following situation at work. We had two files:\n\na DICOM file with a MRI scan of the subjects brain.\na NIFTI file with the segmentation of the brain into regions.\n\nThese are essentially 3D arrays of numbers, with a little bit of meta-data.\nThe issue is that the DICOM file and the NIFTI file can have arbitrary orientations with respect to one-another:\n\nthe axes might not be in the same order. For example, maybe we have:\ndicom.shape == 400, 300, 200\nnifti.shape == 300, 400, 200\nthe pixels of each axis might not be in the same order: maybe the first pixel in one format is the last pixel of the other format.\n\nThe problem is thus quite simple to state: there are 48 possibilities1. Let’s just open the docs, read the correct header keys, figure out how the arrays are organized, code the correct re-ordering and call it a day. Right? Right?\nWell, that’s certainly a plan. However, No plan survives first contact with the enemy2, and I can state with absolute certainty that the “documentation” of the DICOM and NIFTI standards (and ancillary software) is definitely hostile. Our initial hopes for a quick adventure were thus immediately blown to bits.\nStill, I pushed through and, after many bloopers, I have found the solution. I hope it can help somebody else in the future to tackle these f…antastic file formats."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#the-solution",
    "href": "posts/2024/04/aligning_dicom_pixels.html#the-solution",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "2 The solution",
    "text": "2 The solution\n\n2.1 A bit of geometry\nTo understand the solution, it is important to understand projective geometry. Do not sweat, we don’t need to understand everything3. We just need to know that, if we want to transform coordinates between two reference frames, then that can be reframed as a matrix multiplication.\n\n\n\n\n\n\nNote\n\n\n\n\nA change of reference frame from \\(R_1\\) to \\(R_2\\) can be represented by a \\(4,4\\) (or sometimes \\(3,3\\)) matrix \\(M_{R_2 \\leftarrow R_1}\\).\nComputing the inverse of the matrix gives the matrix for the opposite change of reference frame:\n\\[\n\\left(M_{R_2 \\leftarrow R_1}\\right)^{-1} = M_{R_1 \\leftarrow R_2}\n\\]\nComputing the matrix product \\(M_{R_3 \\leftarrow R_2} M_{R_2 \\leftarrow R_1}\\) gives the matrix for change of reference frame from \\(R_1\\) to \\(R_3\\).\n\n\n\nThat’s all we need to know, so feel free to skip to the next section: Section 2.2, or read my detailed explanations below.\nFor example:\n\nlet \\(x,y,z\\) denote the coordinates in the MRI room, with respect to the earth: \\(x\\) is the south-north axis, \\(y\\) is the west-east axis, \\(z\\) is the down-up axis. Let the origin be the middle of the door into the room.\nlet \\(a,b,c\\) denote the coordinates in the patient space. \\(a\\) is the left-right axis of the patient, \\(b\\) is the back-to-front axis, \\(c\\) is the feet-to-head axis. The origin point is the middle of the head of the patient.\n\nThen, there exists a matrix \\(M\\) of shape \\(4, 4\\) which can be used to translate from \\(x,y,z\\) coordinates to \\(a,b,c\\):\n\\[\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\nM\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\]\nWait, why do we have constant coordinates \\(1\\) here? Why are the vectors 4D instead of 3? It’s needed so that we can also represent the change of origin using \\(M\\). If we are representing transformations between reference frames with the same origin, we can work with a \\(3, 3\\) matrix instead. However, I wanted to present the 4D case, because it is what is described in the DICOM and NIFTI docs.\nInverting the matrix reverses the direction of the change of variables:\n\\[\\begin{align}\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\nM\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\\\\nM^{-1}\n\\begin{pmatrix}\na \\\\ b \\\\ c \\\\ 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx \\\\ y \\\\ z \\\\ 1\n\\end{pmatrix}\n\\end{align}\\]\nSimilary, if we had three frames of reference, then applying sequentially a change of reference from \\(R_1\\) to \\(R_2\\) then a second from \\(R_2\\) to \\(R_3\\) would give an overall change from \\(R_1\\) to \\(R_3\\). The same property holds for the associated matrices:\n\\[\nM_{R_3 \\leftarrow R_2} M_{R_2 \\leftarrow R_1} = M_{R_3 \\leftarrow R_1}\n\\]\n\n\n2.2 Finding the matrices\nSo now we know that we need to go looking for matrices.\nFor the NIFTI file format, this is immediate: the matrix is encoded as a header key4. If using nibabel, it is immediately accessible.\nCarefully reading the docs specifies that this affine matrix specifies the transformation between the voxel indices \\(i,j,k\\) and the RAS patient-space (the acronym gives the order and direction of the axes: Right then Anterior (i.e. back-to-front) then Superior (foot-to-head)).\nSurely, the dicom format must be similarly simple. Nope. However, since I know have digested the docs, here are the steps:\n\nFirst, there is a transposition between what the DICOM format calls voxels and how the array is organized on disk5. This is explained here.\nThen, the ImageOrientationPatient header specifies the first two columns of the matrix.\na = np.array(dicom.ImageOrientationPatient[:3])\nb = np.array(dicom.ImageOrientationPatient[3:])\n\n1matrix = np.zeros((4, 4))\nmatrix[3, 3] = 1\n\n2matrix[:3, 0] = b\nmatrix[:3, 0] = a\n\n1\n\nInitializing the matrix and specifying the fourth column.\n\n2\n\nNote the change of order with respect to ImageOrientationPatient. This is due to the transposition.\n\n\nFinally, by considering the change of position between two different dicom slices, we can find the third column.\nslice_diff = (np.array(dicom2.ImagePositionPatient) - np.array(dicom.ImagePositionPatient)) / (\n    dicom2.InstanceNumber - dicom.InstanceNumber\n)\n1c = slice_diff / np.sum(slice_diff**2) ** 0.5\n\n1\n\nNormalizing the vector so that it has norm 1.\n\n\n\nThis gives us the matrix to transform from the DICOM array coordinates \\(i,j,k\\) to the LPS patient-space. This is almost the same as the RAS space used by the NIFTI format: the first two axes are just pointing in the opposite direction.\nOverall, we are now able to combine:\n\nThe matrix from DICOM coordinates to LPS,\nThe matrix from LPS to RAS,\nThe matrix from NIFTI coordinates to RAS,\n\nin order to find the matrix corresponding to the change of variable we want:\n\\[\nM_{\\text{DICOM} \\leftarrow \\text{NIFTI}}\n=\n\\left[ M_{\\text{RAS} \\leftarrow \\text{LPS}} M_{\\text{LPS} \\leftarrow \\text{DICOM}} \\right]^{-1}\nM_{\\text{RAS} \\leftarrow \\text{NIFTI}}\n\\]\nAnd that’s it. We can now analyze the matrix to find how it swaps axes around and reorders them, and apply that transformation to the NIFTI array:\nmatrix_dicom_from_nifti = ...\nornt = nib.orientations.io_orientation(matrix_dicom_from_nifti)\nreoriented_nifti = nib.orientations.apply_orientation(nifti, ornt)\nHonestly, the only way you’ve made it this far is if you are yourself trying to deal with this exact problem. If so, then best of luck and bon courage. You will need both."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#bloopers",
    "href": "posts/2024/04/aligning_dicom_pixels.html#bloopers",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "3 Bloopers",
    "text": "3 Bloopers\nI can’t resist but tell you about all of the hilarious moments along the way were the DICOM spec blew up in my face.\n\nThe DICOM format does not define an ordering of DICOM slices. This means that different tools could choose different orderings. If you struggle with a mismatch of direction along the axis over which the slices are gathered, then the underlying issue might be that your tools do not align along this axis in the same way. NB: the difference in ordering in our case manifests in less than 5% of cases, so that was very tricky to debug.\nAfter a little bit of (unsuccesfully) poking around and trying to read the docs, I asked a colleague about the issue. We quickly went from: “Oh, but all dicoms have the same orientation.” to “No wait, there are two.” to “Well technically it’s maybe three.”. We then found a fourth one later in our tests.\nThe nibabel documentation, and the NIFTI docs I have found all repeat: “the NIFTI format uses a RAS reference frame” throughout. Imagine my surprise when I discovered the existence of the affine header.\nIn my first calculation of the shift between two DICOM slices, I initially naively thought that the slices would be in order and that the first file (with name slice_00000) would actually be the first slice. Ha. Ha. Ha. They are in a random order instead."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#references",
    "href": "posts/2024/04/aligning_dicom_pixels.html#references",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "4 References",
    "text": "4 References\n\nNibabel documentation on the orientation of dicom files.\nNibabel documentation on the transposition between voxels and the array coordinates.\nNibabel documentation: full formula for the DICOM affine matrix.\nNibabel documentation: the nifti affines.\nInnolitics DICOM documentation: Image Orientation Patient.\nInnolitics DICOM documentation: Image Position Patient."
  },
  {
    "objectID": "posts/2024/04/aligning_dicom_pixels.html#footnotes",
    "href": "posts/2024/04/aligning_dicom_pixels.html#footnotes",
    "title": "Aligning a DICOM to a NIFTI, pixel-by-pixel",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(3!=6\\) possibilities for the order of the axes; \\(2^3=8\\) possibilities for the order of each axis.↩︎\nHistory of this quote↩︎\nSeriously, don’t worry if you don’t quite get it: this projective geometry stuff is a bit crazy.↩︎\nTechnically three but nibabel automatically chooses the most appropriate one.↩︎\nPossibly due to the differences between Fortran and C array layouts on disk?↩︎"
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html",
    "href": "posts/2024/05/statistics_experiment_aar.html",
    "title": "After action report: running a simple statistical experiment",
    "section": "",
    "text": "1 Experiments in statistics\n  2 The situation\n  \n  2.1 Data-generating model\n  2.2 Performance comparison\n  2.3 Experiments\n  \n  3 Key points\n  4 Python project: physical organization\n  5 Generating a Student mixture"
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#experiments-in-statistics",
    "href": "posts/2024/05/statistics_experiment_aar.html#experiments-in-statistics",
    "title": "After action report: running a simple statistical experiment",
    "section": "1 Experiments in statistics",
    "text": "1 Experiments in statistics\nThe premise of this post might seem slightly weird to some. Can we actually run experiments in Statistics? But, at the heart of statistics lies the following trio:\n\nA probabilistic model, which generates the data.\nAn analysis algorithm, which produces some result from the data.\nA good property1 that the result has.\n\nFor example, the empirical mean of the dataset is close to the true mean of the data-generating model.\nCritically, the algorithm needs to be tuned to the model. Each analysis algorithm typically applies to an ensemble of models which share some features. If an algorithm is applied to a model that does not have the right features, then the good property probably does not apply.\nTypically, we find the constraints that the model must respect through a sophisticated probabilistic analysis and we prove a theorem clearly establishing when the good property holds. But such theorems are often limited: they only show that the property holds, up to a small error. That’s where experiments also have role to play:\n\nThey can give concrete proof of the performance of an algorithm.\nThey can serve as a basis to build intuition.\n\nThat’s why, even if it is probably a bit heretical for a statistician, I am a big believer in high-quality statistical experiments2."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#the-situation",
    "href": "posts/2024/05/statistics_experiment_aar.html#the-situation",
    "title": "After action report: running a simple statistical experiment",
    "section": "2 The situation",
    "text": "2 The situation\nThe precise details of my situation are not particularly important: my process typically does not vary that much. In this case, I have a classification problem where I want to predict the correct class, and also give a confidence level associated to that prediction. We have a current baseline method, and I have an improvement in mind.\nI thus have a simple plan in mind:\n\nI will create a python project (see Section 4 if you want details).\nWhere I can implement both the existing method and a variant.\nWhere I can tune the data-generating model.\nWhere I can compute the performance of both methods.\n\nThis will give a simple tunable benchmark to check that, under a wide range of conditions, the new method is indeed better.\n\n2.1 Data-generating model\nFor my model, I have the following constraints:\n\nI want to generate pairs consisting of a class \\(C\\) and 2D features \\(X\\). Each class will have a different distribution for \\(X\\). I want to be able to tune the number of classes, but I will use 3 classes.\nI want a model that has an explicit density function for \\(X\\) given \\(C\\).\nI don’t want to use a simple model, such as a Gaussian, Gamma, etc.\nI want to be able to tune the model easily from an external configuration file.\n\nUsing a mixture distribution is the simplest way to accomplish this: the density of the mixture is the weighted sum of the densities of each component. Using student distributions instead of Gaussians gives dense tails to the density, and I like using models which produce outliers. Please see Section 5 for details.\nAll of this data-generation mechanism is supported by a small amount of reusable python code. This means that all of the steps are straightforward to reproduce or modify in the future.\n\n\n2.2 Performance comparison\nIn this experiment, I want to compare:\n\na standard method.\nan improvement.\n\nIn order to make this comparison quantitative, I need to define:\n\nan experimental setup where both methods can be applied simultaneously.\none (or multiple) measures of the performance of each method.\n\nFor this specific example, my problem is a problem of classification. I should thus measure:\n\nwhether the predictions of each method are correct or not.\nwhether the uncertainty estimates associated to each method are correct or not.\n\nFor point 1. there are many standard measures of the quality of a prediction. Let’s pick the precision and recall as good baselines. For point 2. we enter trickier territory. I took a solid hour before deciding to use the Brier score (L2 loss over the probabilities) and a custom measure of the whether the predicted probabilities of each class match the actual probabilities. Explaining the details of this choice is out-of-topic for this post.\nThe critical points here are:\n\nI have several quantitative measures of the performance of both methods.\nI took the time to think in detail about these measures of performance.\nSince I’m working with artifical data, I can compare the performance to the best possible performance: the one that an oracle that knows the data-generating model would reach.\n\nAgain, all of this I translate into concise and reusable python code.\n\n\n2.3 Experiments\nFinally, I can combine my data-generating mecanism and my measures of performance. This yields several experiments to compare my methods to one-another, and to the performance of an oracle with knowledge of the data-generating process.\nI can then:\n\nobserve how tweaking the parameters of the experiments modifies the measures of performance for both algorithms, and the gap between the two,\nplay-around with the difficulty of the class. Here, this corresponds to separating the classes more,\nplay around with class-imbalance,\nplay around with drift between the training dataset and the validation dataset, for example by modifying the proportion of each class in both sets.\n\nOverall, this gave me great confidence in the gains of the tweaked method, and a bit of additional understanding of its strengths."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#key-points",
    "href": "posts/2024/05/statistics_experiment_aar.html#key-points",
    "title": "After action report: running a simple statistical experiment",
    "section": "3 Key points",
    "text": "3 Key points\n\nStatistics / data-science is an applied discipline. Controlled experiments with toy-data can thus be a great way to improve our understanding of our methods.\nThis requires using non-trivial data. I believe that a mixture of student distributions is a good starting point. It is critical to avoid cases that are too easy since they might lack some critical features of realistic data, such as:\n\noutliers / exceptional datapoints,\nambiguity between classes,\nnon-linearity.\n\nComparison between methods needs to be quantitative. We should use, if possible, simple and well-established measures of performance. We should understand the statistical relevance of these measures. If possible, we should compare the performance of all methods to the performance of an oracle knowning the data-generating process, since this provides an upper-bound on the performance of any method.\nThis should be supported by high-quality code that is easy to tweak and to reuse. This code should be tested to avoid bugs.\n\nI am a great believer in this approach to statistics / data science, and I hope you can succesfully integrate it in your own work."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#sec-project-management",
    "href": "posts/2024/05/statistics_experiment_aar.html#sec-project-management",
    "title": "After action report: running a simple statistical experiment",
    "section": "4 Python project: physical organization",
    "text": "4 Python project: physical organization\n\nIn case you need guidance on this point, here is my default file-structure in a project.\nproject_root/\n├─ data/\n│  ├─ some_data.csv\n├─ scripts/\n│  ├─ experiment_1.py\n│  ├─ experiment_2.py\n│  ├─ experiment_3.py\n├─ src/\n│  ├─ library_name/\n│  │  ├─ __ini__.py\n│  │  ├─ file.py\n├─ test/\n│  ├─ test1.py\n├─ .gitignore\n├─ pyproject.toml\n├─ README.md\nIt’s a very standard structure:\n\nI’m using the modern pyproject.toml specification for dependencies and setup, instead of having a requirements.txt file.\ndata is saved in a separate folder, in a human-readable format. It gets commited as part of the project.\nany generalist function is saved in a python module / library saved under src/library_name.\nany script manipulating these functions to achieve a result (here: run an experiment) is saved under scripts.\ntests (using pytest) are separately saved under the tests folder. Always write test code."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#sec-generating-student-mixture",
    "href": "posts/2024/05/statistics_experiment_aar.html#sec-generating-student-mixture",
    "title": "After action report: running a simple statistical experiment",
    "section": "5 Generating a Student mixture",
    "text": "5 Generating a Student mixture\n\nTo generate the parameterization of the mixture, for each class:\n\nI define a list of centers, which roughly draw a tree-shape over the space. Each tree starts at 0,0, which will have an ambiguous class attribution. Each class goes in a different direction.\nI sample several (random number between 3 and 6) IID Gaussians centered around each center.\nI give each center a random weight, using a Gamma distribution.\n\nI tweaked the centers and the parameters of the sampling until I was visually satisfied with the result, then saved the mixture parameters in a json file. This makes sure that the mixture is (somewhat) human-readable but, more importantly, that it is machine-readable, can be shared trivially, can be resampled, can be commited to git, etc."
  },
  {
    "objectID": "posts/2024/05/statistics_experiment_aar.html#footnotes",
    "href": "posts/2024/05/statistics_experiment_aar.html#footnotes",
    "title": "After action report: running a simple statistical experiment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI know that this is a very ambiguous statement. Data science has several frameworks which define the particular good properties that are interesting. For the present post, I do not wish to go further.↩︎\nNot convinced? Then consider deep neural networks. They are, by-far, the largest breakthrough in data science in the last 20 years, and they have been built purely on the back of empirical results.↩︎"
  },
  {
    "objectID": "posts/2024/06/gitlab_redmine_connection.html",
    "href": "posts/2024/06/gitlab_redmine_connection.html",
    "title": "Connecting gitlab to redmine",
    "section": "",
    "text": "1 Gitlab to redmine connection\n  2 Redmine to git connection\n  3 Gitlab connection\nIn this blog post, I tell you how to connect:\nOur objective was to create a system so that redmine tickets can be linked to the code commits addressing them.\nIn our current workflow, we follow these steps:"
  },
  {
    "objectID": "posts/2024/06/gitlab_redmine_connection.html#gitlab-to-redmine-connection",
    "href": "posts/2024/06/gitlab_redmine_connection.html#gitlab-to-redmine-connection",
    "title": "Connecting gitlab to redmine",
    "section": "1 Gitlab to redmine connection",
    "text": "1 Gitlab to redmine connection\nThe connection from gitlab to redmine is minimalist and straightforward to setup.\nNormally, gitlab provides automated functionality to parse patterns of the form #ID (where ID is an integer) in any text block on the site. When this pattern is identified, it is linked to the gitlab issue with the same number. We wanted to use redmine as an issue tracker instead of gitlab. Gitlab provides a simple way to do so.\n\nFor each repository, disable gitlab issues.\nFor each repository, enable the redmine connection."
  },
  {
    "objectID": "posts/2024/06/gitlab_redmine_connection.html#redmine-to-git-connection",
    "href": "posts/2024/06/gitlab_redmine_connection.html#redmine-to-git-connection",
    "title": "Connecting gitlab to redmine",
    "section": "2 Redmine to git connection",
    "text": "2 Redmine to git connection\nThings will now get trickier. We cannot directly connect redmine to gitlab. Instead, we will connect redmine to git. This will make git commit messages visible on redmine.\nOnce git commit messages are visible on redmine, you can choose patterns to automatically link a specific commit to a redmine work item. Commits can also be manually linked to a work item, via the web interface.\nAnnoyingly, redmine only refreshes which commits the web server is aware of when somebody visits the repository page on its web interface.\n\nOn the machine hosting your redmine instance:\n\nAuthentify as www-data user: sudo -su www-data (or whichever user is used by your redmine installation).\nCreate a folder to host repositories. For example: /var/www/repos.\nCreate a method to authenticate this machine on your gitlab server. For example, you can create a new user:\n\nCreate a new gitlab user for your redmine machine.\nCreate a ssh key: ssh-keygen -t ed25519; note save location.\nCopy ssh key: cat PATH/TO/id_ed25519.pub.\nSave ssh key in gitlab user preferences.\n\nClone project with mirror option: git clone --mirror SSH_CLONE_ADDRESS.\nCreate a cron task to fetch all repositories:\n\n⚠️ You only need to do this once. ⚠️\nCreate a script to fetch all repositories: nano git_fetch_all.sh\n#!/bin/sh\necho \"Starting fetch script\"\nfor dir in $(ls -d /var/www/repos/*/); do\n    echo \"fetching $dir\" && cd \"$dir\" && git fetch -q --all -p && cd ..\ndone\necho \"Fetch script done\"\nCreate a cron task to run the script:\n\nOpen cron edit: crontab -u www-data -e.\nAdd a new line: */1 * * * * (sh /var/www/repos/git_fetch_all.sh) &gt; /var/www/repos/log.\n\n\n\nOn the redmine web interface:\n\nIn the global settings:\n\nIn tab repositories, select git as enabled scm.\nChange referencing keywords to: *.\n\nIn the target project:\n\nIn settings, open repositories tab.\nAdd new repository with path /var/www/repos/PROJECT_NAME.git.\n\n\nDebug:\n\nIf redmine reports a 404 error, check paths.\nIf redmine reports a 403 error, check ownership of all folders.\nIf the repositories do not update on the machine:\n\nCheck that the script runs correctly.\nCheck that the cron task is correctly set for the www-data user.\n\nIf the commits do not update in work items:\n\nredmine only refreshes which commits the web server is aware of when somebody visits the repository page on its web interface. Check on the specific page corresponding to the repository where the commit has been added."
  },
  {
    "objectID": "posts/2024/06/gitlab_redmine_connection.html#gitlab-connection",
    "href": "posts/2024/06/gitlab_redmine_connection.html#gitlab-connection",
    "title": "Connecting gitlab to redmine",
    "section": "3 Gitlab connection",
    "text": "3 Gitlab connection\nWe can now connect gitlab to redmine by writing information about gitlab into git commit messages. Whenever we accept a merge request on gitlab, a merge request commit and, optionally, a squash commit are created. We can customize the commit messages for both to link these commits to redmine issues. By default, the commit messages reproduce the description of the merge request, but additional information can be added.\n\nMake a habit of mentionning in the merge request description which redmine items are concerned with the #ID syntax.\nFor each repository, write a custom commit message template to include the information you wish to be present on redmine."
  },
  {
    "objectID": "posts/2024/09/pruning_git_branches.html",
    "href": "posts/2024/09/pruning_git_branches.html",
    "title": "Pruning git local branches.",
    "section": "",
    "text": "1 Deleting branches quickly and accurately\n  2 How it works\n  \n  2.1 git fetch --prune\n  2.2 Listing and parsing local branches\n  2.3 Parsing each line\n  2.4 A note on deleting\ngit is a great piece of software but it can be a bit tricky to understand how to do advanced manipulations with it. Here is a trick which I find quite useful. It is very normal to accumulate unused local branches on a work computer, typically due to them being merged into the main code branch. You could manually delete them, but let me share my procedure to do so automatically."
  },
  {
    "objectID": "posts/2024/09/pruning_git_branches.html#deleting-branches-quickly-and-accurately",
    "href": "posts/2024/09/pruning_git_branches.html#deleting-branches-quickly-and-accurately",
    "title": "Pruning git local branches.",
    "section": "1 Deleting branches quickly and accurately",
    "text": "1 Deleting branches quickly and accurately\nLet’s get straight to the point. Open your terminal inside the project, then run the following commands.\n\nCreate the file branches_to_prune.txt with all branches which do not have associated upstream branches.\n\ngit fetch --prune && git for-each-ref --format '%(refname) %(upstream:track)' refs/heads | awk '$2 == \"[gone]\" {sub(\"refs/heads/\", \"\", $1); print $1}' &gt;&gt; branches_to_prune.txt\n\nThen review the file to make sure that everything is all right.\n\nnano branches_to_prune.txt\n\nErase all branches in the file and delete the file.\n\nwhile read p; do branch -d $p; done &lt; branches_to_prune.txt\nrm branches_to_prune.txt"
  },
  {
    "objectID": "posts/2024/09/pruning_git_branches.html#how-it-works",
    "href": "posts/2024/09/pruning_git_branches.html#how-it-works",
    "title": "Pruning git local branches.",
    "section": "2 How it works",
    "text": "2 How it works\nIf you want to understand the dark magic going on, here are all the important details.\nIf you feel lost, the key reference is the pro git book.\n\n2.1 git fetch --prune\nNormally, we run git fetch which updates all branches present on the remote. However, branches which are not present on the remote are not updated. With the --prune option, branches which are not on the remote get removed. This makes it so that branches which have no counter-part on the remote can now be identified since they don’t have an upstream branch which they track.\n\n\n2.2 Listing and parsing local branches\nGit offers a built-in tool for listing and parsing local branches: git-for-each-ref. In the final command, we do not take advantage of the fact that its outputs can be formatted. A human readable command would be:\ngit for-each-ref --format '%(align:width=60)%(refname)%(end)%(align:width=60)%(upstream)%(end)%(align:width=10)%(upstream:track)%(end)%(align:width=3)%(upstream:trackshort)%(end)' refs/heads\nThe pieces of information are a bit tricky to understand. upstream provides information about the reference upstream from the branch. upstream:track and upstream:trackshort provide information about whether the upstream is behind, ahead, equal compared to the local branch. Furthermore, upstream:track has the special value [gone] if the upstream is absent, which is what our final command parses.\nAnother important\nThe command we use in the workflow is a machine-formatted variant:\ngit for-each-ref --format '%(refname) %(upstream:track)' refs/heads\nPlay around with for-each-ref: it’s a really interesting command.\n\n\n2.3 Parsing each line\nEach line of the for-each-ref is parsed with awk:\nawk '$2 == \"[gone]\" {sub(\"refs/heads/\", \"\", $1); print $1}'\nThis parses as:\n\nif the second input $2 is exactly [gone],\nthen remove refs/heads/ from the first input and print the remainder.\n\nThis transforms the list from for-each-ref into the list of all local branches with no upstream.\n\n\n2.4 A note on deleting\nThere are two commands for deletion:\n\ngit branch -d $name: this is the normal --delete option.\ngit branch -D $name: this is a shorthand for --delete --force.\n\nThe --force option allows deleting the branch irrespective of its merged status, or whether it even points to a valid commit. Please be careful and only use it when necessary."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html",
    "href": "posts/2024/12/stack_overflow_1_k__1.html",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "",
    "text": "1 Problem statement\n  2 Simulation code\n  3 A simple counter-example\n  4 The simple cases: \\(m=1\\), \\(m=n\\), \\(m=n-1\\)\n  5 An exhaustive exploration\n  6 Mathew Spam improvement\n  7 Conclusions"
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#problem-statement",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#problem-statement",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "1 Problem statement",
    "text": "1 Problem statement\nInitial question on SO.\n\nThere are \\(n\\) cards which have numbers \\(1\\)~\\(n\\) on each. You pick \\(m\\) cards from it, and you don’t put it back once you pick from it. Is the probability that their sum is divisible by \\(k\\) always \\(\\dfrac 1 k\\), while \\(k|n\\)? If not, how do we generalize the probability?\n\nThe key features of this problem are:\n\npicking without replacement \\(X_i \\in [1, n]\\).\n\\(k\\) possible values for the output of \\(\\sum X_i \\bmod k\\).\n\nWe would like to show that a single value has probability \\(1/k\\). This hints at the distribution being uniform over the \\(k\\) possibilities, but maybe something more complex is going on.\nTo investigate such questions, unless I happen to glance exactly the right analysis straight away, my process is to:\n\nlook for easy cases\nperform simulations and try to identify patterns."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#simulation-code",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#simulation-code",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "2 Simulation code",
    "text": "2 Simulation code\nThe situation is straightforward to implement in python using numpy.\n\nimport numpy as np\nfrom scipy.stats import binomtest\n\nrepetitions = 10000\n\ndef single_simulation(n, m, k):\n    \"\"\"Compute the sum modulo k of m elements in [1, n].\"\"\"\n    # Check input\n    assert n % k == 0, f\"Expected n % k == 0 but got {n % k} with {n=} and {k=}\"\n    assert m &lt;= n, f\"Excpected m &lt;= n but got {m=} and {n=}\"\n\n1    picks = np.random.choice(1 + np.arange(n), size=(m,), replace=False)\n    result = np.sum(picks) % k\n\n    return result\n\ndef compute_success_probability(n, m, k, repetitions=10000, confidence_level=0.95):\n    \"\"\"Compute a Monte-Carlo approximation of the probability of success.\"\"\"\n    num_success = 0\n    for idx in range(repetitions):\n        num_success += single_simulation(n, m, k) == 0\n    \n    probability = num_success / repetitions\n\n2    result = binomtest(k=num_success, n=repetitions).proportion_ci(confidence_level=confidence_level)\n    low = result.low\n    high = result.high\n\n    return probability, low, high\n\n\n1\n\nSample without replacement: cf numpy docs.\n\n2\n\nExact confidence interval: cf scipy docs."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#a-simple-counter-example",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#a-simple-counter-example",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "3 A simple counter-example",
    "text": "3 A simple counter-example\nThere are \\(n \\choose m\\) possibilities for the selection of the cards. These possibilities are equally likely. Thus, the final probability must be some integer multiple of \\(\\left[ n \\choose m \\right]^{-1}\\). Thus, if \\(n \\choose m\\) is not divisible by \\(k\\), then the conjecture is false.\nFor example, for the triplet \\(n=6, m=3, k=3\\), we have \\({n \\choose m} = 20\\) and the conjecture must be false.\nThis is confirmed by a simulation:\n\nfrom scipy.special import comb\n\nn = 6\nm = 3\nk = 3\nprobability, low, high = compute_success_probability(n=n, m=m, k=k)\n\nprint(f\"Number of combinations: {comb(n, m) = }\")\nprint(f\"Number of combinations makes conjecture impossible: {comb(n, m) % k = }\")\nprint(f\"Estimated probability: {np.round(probability, 3)} (0.95 interval: {np.round(low,3), np.round(high,3)})\")\nprint(f\"Conjectured probability: {np.round(1/k, 3)}\")\n\nNumber of combinations: comb(n, m) = 20.0\nNumber of combinations makes conjecture impossible: comb(n, m) % k = 2.0\nEstimated probability: 0.396 (0.95 interval: (0.386, 0.405))\nConjectured probability: 0.333"
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#the-simple-cases-m1-mn-mn-1",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#the-simple-cases-m1-mn-mn-1",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "4 The simple cases: \\(m=1\\), \\(m=n\\), \\(m=n-1\\)",
    "text": "4 The simple cases: \\(m=1\\), \\(m=n\\), \\(m=n-1\\)\nSome cases are immediate.\nIf \\(m=1\\), then the result is a uniform variable with \\(k\\) possibilities. The conjecture is thus immediately true. [^If the sampling was with replacement instead of without, then the result would be immediately true due to this argument.]\nIf \\(m=n\\), there is no randomness: we just select all cards. The final result depends on the parity of \\(k\\).\n\nIf \\(k\\) is odd, then the terms cancel out in the sum by pairing them: \\(1\\) with \\(k-1\\), etc. The final sum is thus always 0.\nIf \\(k\\) is even, then we again pair the terms, but the term \\(k/2\\) is left standing alone. The final sum is thus:\n\n\\[\n((n / k) \\bmod 2) * k / 2\n\\]\nIf \\(m=n-1\\), we can represent this situation as selecting a single card to exclude from the total sum. Thus, by the same argument as with \\(m=1\\), we know that the probability is \\(1/k\\)."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#an-exhaustive-exploration",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#an-exhaustive-exploration",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "5 An exhaustive exploration",
    "text": "5 An exhaustive exploration\nWe can also run a simple exhaustive experiment:\n\ntest all values of \\(k\\) in \\([2,7]\\),\ntest \\(n\\) in \\(k * [1, 5]\\),\ntest \\(m\\) in \\([2, n-2]\\) (excluding the easy cases discussed above),\nif \\({n \\choose m} \\bmod k = 0\\) but the probability is different (which we measure using an exact 0.99 confidence interval) from \\(1/k\\), print out the details of the results.\n\nNote that we expect there to be roughly 1 percent of false positives.\n\nfrom IPython.display import Markdown\nfrom tabulate import tabulate\n\ntable = []\nfor k in range(2, 7 + 1):\n    for n in range(k, k * 5 + 1, k):\n        for m in range(0, n+1):\n            if m in [0, 1, n-1, n]:\n                continue\n            p, l, h = compute_success_probability(n=n,m=m,k=k,confidence_level=0.99)\n            c = int(comb(n, m))\n            conjecture = 1/k\n            conjecture_validated = l &lt;= conjecture &lt;= h\n            if c % k == 0 and not conjecture_validated:\n                line = [\n                    (n, m, k),\n                    np.round(conjecture,4),\n                    (np.round(l,4), np.round(h,4)),\n                ]\n                table.append(line)\n\nMarkdown(\n    tabulate(\n        table, \n        headers=[\n            \"(n, m, k)\",\n            \"Conjectured proba $1/k$\",\n            \"Probability 0.99 confidence interval\"\n        ],\n    )\n)\n\n\n\n\n(n, m, k)\nConjectured proba \\(1/k\\)\nProbability 0.99 confidence interval\n\n\n\n\n(4, 2, 2)\n0.5\n(0.3157, 0.34)\n\n\n(8, 2, 2)\n0.5\n(0.4168, 0.4424)\n\n\n(8, 4, 2)\n0.5\n(0.5302, 0.556)\n\n\n(8, 6, 2)\n0.5\n(0.4187, 0.4443)\n\n\n(10, 4, 2)\n0.5\n(0.5036, 0.5294)\n\n\n(10, 6, 2)\n0.5\n(0.4573, 0.4831)\n\n\n(9, 3, 3)\n0.3333\n(0.3438, 0.3686)\n\n\n(9, 6, 3)\n0.3333\n(0.3433, 0.368)\n\n\n(8, 2, 4)\n0.25\n(0.2073, 0.2286)\n\n\n(8, 6, 4)\n0.25\n(0.2068, 0.2281)\n\n\n(12, 6, 4)\n0.25\n(0.2272, 0.2492)\n\n\n(12, 9, 4)\n0.25\n(0.2522, 0.275)\n\n\n(16, 2, 4)\n0.25\n(0.222, 0.2439)\n\n\n(16, 14, 4)\n0.25\n(0.222, 0.2439)\n\n\n(12, 2, 6)\n0.1667\n(0.1457, 0.1644)\n\n\n(12, 7, 6)\n0.1667\n(0.1442, 0.1629)\n\n\n(12, 10, 6)\n0.1667\n(0.1421, 0.1606)\n\n\n\n\n\nWe find many examples where the \\({n \\choose m} \\bmod k = 0\\) conjecture is not true. For some of these, it is straightforward to prove that the probability is not \\(1/k\\).\nFor example, consider the triplet \\(n=4, m=2, k=2\\).\n\nThe winning pairs are (2, 4) and (1, 3).\nThere are 6 total pairs.\nThe true probability is thus \\(1/3\\).\n\nOther cases with \\(m=2\\) are similarly easy to analyze."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#mathew-spam-improvement",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#mathew-spam-improvement",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "6 Mathew Spam improvement",
    "text": "6 Mathew Spam improvement\nIn of the answers, Matthew Spam proposes the following observation:\n\nIf \\(\\operatorname{gcd}(m,n)=1\\) then the probability is equal to \\(\\frac{1}{k}\\).\n\nand offers a proof: there is a symmetry that enables us to show that the distribution of \\(\\sum X_i \\bmod k\\) is uniform over \\([1,k]\\).\nThis is straightforward to extend to the case \\(\\operatorname{gcd}(m, k) = 1\\).\nIn view of this, let’s rerun the analysis focusing only on cases where \\(\\operatorname{gcd}(m,k) \\neq 1\\).\n\ntable = []\nfor k in range(2, 7 + 1):\n    for n in range(k, k * 5 + 1, k):\n        for m in range(1, (n-1)+1):\n            divisor = np.gcd(m, k)\n            if divisor == 1:\n                continue\n\n            p, l, h = compute_success_probability(n=n,m=m,k=k,confidence_level=0.99)\n            c = int(comb(n, m))\n            conjecture = 1/k\n            conjecture_validated = l &lt;= conjecture &lt;= h\n            if not conjecture_validated:\n                line = [\n                    (n, m, k),\n                    divisor,\n                    np.round(conjecture,4),\n                    (np.round(l,4), np.round(h,4)),\n                ]\n                table.append(line)\n\nMarkdown(\n    tabulate(\n        table, \n        headers=[\n            \"(n, m, k)\",\n            \"gcd(m, k)\",\n            \"Conjectured proba $1/k$\",\n            \"Probability 0.99 confidence interval\"\n        ],\n    )\n)\n\n\n\n\n(n, m, k)\ngcd(m, k)\nConjectured proba \\(1/k\\)\nProbability 0.99 confidence interval\n\n\n\n\n(4, 2, 2)\n2\n0.5\n(0.3133, 0.3375)\n\n\n(6, 2, 2)\n2\n0.5\n(0.3941, 0.4195)\n\n\n(6, 4, 2)\n2\n0.5\n(0.5812, 0.6066)\n\n\n(8, 2, 2)\n2\n0.5\n(0.4154, 0.441)\n\n\n(8, 4, 2)\n2\n0.5\n(0.5287, 0.5545)\n\n\n(8, 6, 2)\n2\n0.5\n(0.4125, 0.438)\n\n\n(10, 2, 2)\n2\n0.5\n(0.438, 0.4637)\n\n\n(10, 4, 2)\n2\n0.5\n(0.5112, 0.537)\n\n\n(10, 6, 2)\n2\n0.5\n(0.4639, 0.4897)\n\n\n(10, 8, 2)\n2\n0.5\n(0.5448, 0.5705)\n\n\n(6, 3, 3)\n3\n0.3333\n(0.3866, 0.4119)\n\n\n(9, 3, 3)\n3\n0.3333\n(0.3414, 0.3661)\n\n\n(9, 6, 3)\n3\n0.3333\n(0.3335, 0.3581)\n\n\n(12, 9, 3)\n3\n0.3333\n(0.3385, 0.3632)\n\n\n(15, 12, 3)\n3\n0.3333\n(0.3346, 0.3592)\n\n\n(4, 2, 4)\n2\n0.25\n(0.1572, 0.1765)\n\n\n(8, 2, 4)\n2\n0.25\n(0.2069, 0.2282)\n\n\n(8, 4, 4)\n4\n0.25\n(0.2506, 0.2734)\n\n\n(8, 6, 4)\n2\n0.25\n(0.2032, 0.2244)\n\n\n(12, 2, 4)\n2\n0.25\n(0.2123, 0.2338)\n\n\n(12, 10, 4)\n2\n0.25\n(0.2057, 0.227)\n\n\n(16, 2, 4)\n2\n0.25\n(0.2225, 0.2444)\n\n\n(16, 8, 4)\n4\n0.25\n(0.2504, 0.2732)\n\n\n(16, 14, 4)\n2\n0.25\n(0.2268, 0.2488)\n\n\n(20, 2, 4)\n2\n0.25\n(0.226, 0.248)\n\n\n(20, 16, 4)\n4\n0.25\n(0.2509, 0.2737)\n\n\n(20, 18, 4)\n2\n0.25\n(0.2205, 0.2423)\n\n\n(6, 2, 6)\n2\n0.1667\n(0.1274, 0.1452)\n\n\n(6, 3, 6)\n3\n0.1667\n(0.1846, 0.2051)\n\n\n(6, 4, 6)\n2\n0.1667\n(0.1895, 0.2102)\n\n\n(12, 2, 6)\n2\n0.1667\n(0.1444, 0.1631)\n\n\n(12, 8, 6)\n2\n0.1667\n(0.1693, 0.1892)\n\n\n(12, 10, 6)\n2\n0.1667\n(0.1383, 0.1566)\n\n\n(24, 22, 6)\n2\n0.1667\n(0.1471, 0.1659)\n\n\n\n\n\nIn these results, we observe that, when \\(\\operatorname{gcd}(m, k) \\neq 1\\):\n\nWe often have that the probability does not match \\(1/k\\).\nThe pattern is less apparent when \\(n\\) grows, but it could be due to the missmatch being smaller."
  },
  {
    "objectID": "posts/2024/12/stack_overflow_1_k__1.html#conclusions",
    "href": "posts/2024/12/stack_overflow_1_k__1.html#conclusions",
    "title": "Investigating a conjecture from Stack Overflow 1/2",
    "section": "7 Conclusions",
    "text": "7 Conclusions\n\nThe conjecture is false, as is: we identified several counter-examples.\nIf \\({n \\choose m} \\bmod k != 0\\) then the conjecture is clearly wrong.\nThere are cases where \\({n \\choose m} \\bmod k = 0\\) but the conjecture is wrong. For example: \\(n=4, m=2, k=2\\) where the probability is provably \\(1/3\\) instead of \\(1/2\\).\nIf \\(\\operatorname{gcd}(m, k)=1\\) the conjecture is provably true.\nFrom a simulation study, it seems possible that when \\(\\operatorname{gcd}(m, k) \\neq 1\\), the probability is always different from \\(1/k\\).\n\nThese final observations has put me on the right track, and I am now able to move on to rigorous mathematical analysis. I will do so in a second post to keep things organized."
  }
]