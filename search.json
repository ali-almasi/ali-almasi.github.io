[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog. I write mostly about:\n\nstatistics, with a focus on Bayesian statistics, variational inference and theory.\ntypesetting documents with Quarto and alternatives.\nprogramming, mostly in Python.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantifying and Classifying Entanglement\n\n\n\n\n\n\nquantum-information-theory\n\n\ncourse-notes\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nAn Attempt at Understanding the Grand Unification\n\n\n\n\n\n\nquantum-algorithms\n\n\ncourse-notes\n\n\n\n\n\n\n\n\n\nDec 8, 2024\n\n\n\n\n\n\n\n\n\n\n\nClassical and Quantum Source Coding\n\n\n\n\n\n\nclassical-information-theory\n\n\nquantum-information-theory\n\n\ncourse-notes\n\n\n\n\n\n\n\n\n\nNov 15, 2024\n\n\n\n\n\n\n\n\n\n\n\nTwo CS-inspired Proofs for the Infinitude of Primes\n\n\n\n\n\n\nprimes\n\n\nclassical-information-theory\n\n\nautomata-theory\n\n\n\n\n\n\n\n\n\nJun 11, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "assets/teaching_materials/linear_algebra2020/test.html",
    "href": "assets/teaching_materials/linear_algebra2020/test.html",
    "title": "Ali ALMASI",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "Misc.",
    "section": "",
    "text": "Some art, some literature and a bunch of personal stuff.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome of My Digital Artworks\n\n\n\n\n\n\ndigital-art\n\n\npainting\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ali ALMASI",
    "section": "",
    "text": "I am interested in quantum information science, mostly the information-theoretic and complexity-theoretic aspects. I enjoy using tools from optimization theory, mathematical analysis, and discrete mathematics to tackle problems in these areas.\nEmail - LinkedIn"
  },
  {
    "objectID": "index.html#sec-header",
    "href": "index.html#sec-header",
    "title": "Ali ALMASI",
    "section": "",
    "text": "I am interested in quantum information science, mostly the information-theoretic and complexity-theoretic aspects. I enjoy using tools from optimization theory, mathematical analysis, and discrete mathematics to tackle problems in these areas.\nEmail - LinkedIn"
  },
  {
    "objectID": "index.html#sec-education",
    "href": "index.html#sec-education",
    "title": "Ali ALMASI",
    "section": " Education",
    "text": "Education\nM.Sc. in Computer Science (PhD-track Program)\nÉcole Polytechnique, Institut Polytechnique de Paris\n2023 - 2025 (expected)\nB.Sc. in Pure Mathematics, Minor in Computer Science\nSharif University of Technology\n2018 - 2023"
  },
  {
    "objectID": "index.html#sec-research",
    "href": "index.html#sec-research",
    "title": "Ali ALMASI",
    "section": " Research Projects",
    "text": "Research Projects\n\nQuantum Guessing Games (M1 Internship) | Summer 2024\nReport, Poster, Slides\n\nSupervised by Peter Brown and Cambyse Rouzé\nAn investigation of quantum guessing games, a generalization of quantum state discrimination and antidiscrimination, in which we found closed-form solutions for specific games, defined and analyzed the pretty good measurement for this generalization, and studied the convergence behaviour of an iterative algorithm for determining the optimal strategy.\n\n\n\nPositive but not Completely Positive Maps | Fall 2023\nReport, Slides\n\nSupervised by Peter Brown\nA numerical and analytical study of the properties of positive but not completely positive (PnCP) maps, focusing on their applications in entanglement detection and relaxing certain optimization problems over the set of separable quantum states. Using semidefinite programming, we explored the relationship between two fundamental entanglement criteria: positivity under partial application of a PnCP map and entanglement witnesses.\n\n\n\nQuantum Proofs | Fall 2022\nThesis\n\nSupervised by Shahram Khazaei\nA pedagogical survey (in Persian) intended to be an introduction to quantum complexity theory, focusing on QMA, a quantum analogue of NP."
  },
  {
    "objectID": "index.html#other-academic-activities",
    "href": "index.html#other-academic-activities",
    "title": "Ali ALMASI",
    "section": " Other Academic Activities",
    "text": "Other Academic Activities\n\nHead Organizer and Mentor of the Sharif Mathematics Summer School | Summer 2019\n\nA 3-day event aimed at introducing high school students to topics in mathematics and computer science beyond the high school curriculum.\n\n\n\nMember of Hamband Scientific Association | 2019 - 2020\n\n\nEditorial Board of Sharif Students Mathematics Journal | 2023"
  },
  {
    "objectID": "index.html#interests-outside-academia",
    "href": "index.html#interests-outside-academia",
    "title": "Ali ALMASI",
    "section": " Interests Outside Academia",
    "text": "Interests Outside Academia\n\nDigital Art\n\nHere are some works.\n\n\n\nClassical Music\n\n\nPersian Poetry"
  },
  {
    "objectID": "posts/2024/12/grand-unification.html",
    "href": "posts/2024/12/grand-unification.html",
    "title": "An Attempt at Understanding the Grand Unification",
    "section": "",
    "text": "1 Introduction\n  2 Block-Encoding\n  3 Quantum Signal Processing (QSP)\n  4 Proof of the QSP Theorem\nIt’s unfortunate that there aren’t many good open-source fonts designed specifically for dyslexic readers. However, there’s a helpful Chrome extension that can change the font of the text you read online, making it easier to follow."
  },
  {
    "objectID": "posts/2024/12/grand-unification.html#introduction",
    "href": "posts/2024/12/grand-unification.html#introduction",
    "title": "An Attempt at Understanding the Grand Unification",
    "section": "1 Introduction",
    "text": "1 Introduction\nThere are roughly two main lines of designing quantum algorithms, and many of the algorithms we know today can be classified under one of these two categories. The representative algorithm of the first category is the Grover’s search algorithm, and the second category is represented by the Shor’s factoring algorithm [One may even argue that Shor’s algorithm is not that different from Grover’s algorithm.]. It is a natural question then to ask what is the underlying technique that unifies all these algorithms. This is the main claim of the “The Grand Unification” paper: Finding the common underlying technique that unifies all quantum algorithms."
  },
  {
    "objectID": "posts/2024/12/grand-unification.html#block-encoding",
    "href": "posts/2024/12/grand-unification.html#block-encoding",
    "title": "An Attempt at Understanding the Grand Unification",
    "section": "2 Block-Encoding",
    "text": "2 Block-Encoding\nIn block-encoding, we want to encode an arbitrary matrix \\(A \\in \\mathbb{C}^{r \\times c}\\) in a unitary matrix \\(U \\in \\mathbb{C}^{d \\times d}\\), in a way that the top-left \\(r \\times c\\) block of \\(U\\) is \\(A\\). This condition can be formulated as\n\\[ I_{r,d} U I_{d,c} = A, \\]\nwhere \\(I_{r,d}\\) is first \\(r\\) rows of the identity matrix of size \\(d \\times d\\), and \\(I_{d,c}\\) is the first \\(c\\) columns of the identity matrix of size \\(d \\times d\\).\nLet us think of the matrix \\(A\\) as an operator acting on a \\(s\\)-qubit system (you may think of \\(A\\) as a Hamiltonian acting on \\(s\\) qubits.), and \\(U\\) as a unitary acting on the \\(k\\)-qubit system plus an ancilla \\(a\\)-qubit register. Then, \\(U\\) is a block-encoding of \\(A\\) if\n\\[ \\left( \\bra{0}^{\\otimes a} \\otimes I_{2^k} \\right) U \\left( \\ket{0}^{\\otimes a} \\otimes I_{2^k} \\right) = A. \\]\nAn example of a block-encoding is the following:\nSuppose that we have a control-qubit \\(C\\) and a target-system \\(T\\) with Hilbert space \\(\\mathcal{H}_T\\), and there is a Hermitian operator \\(H\\) acting on the target-system. The following is a block-encoding of \\(H\\):\n\\[ U = \\ket{0}\\bra{0}_C \\otimes H_T + \\ket{0}\\bra{1}_C \\otimes \\sqrt{I - H^2}_T + \\ket{1}\\bra{0}_C \\otimes \\sqrt{I - H^2}_T + \\ket{1}\\bra{1}_C \\otimes -H_T, \\]\nwhen \\(\\| H \\| \\leq 1\\).\nHaving the spectral decomposition of \\(H\\) as\n\\[ H = \\sum_{i} \\lambda_i \\ket{\\lambda_i}\\bra{\\lambda_i}, \\]\nwe can also write \\(U\\) as\n\\[ U = \\sum_{i} \\left( \\ket{0}\\bra{0}_C \\lambda_i + \\ket{0}\\bra{1}_C \\sqrt{1 - \\lambda_i^2} + \\ket{1}\\bra{0}_C \\sqrt{1 - \\lambda_i^2} - \\ket{1}\\bra{1}_C \\lambda_i \\right) \\otimes \\ket{\\lambda_i}\\bra{\\lambda_i}_T, \\]\nwhich can be written in the matrix form as\n\\[ U = \\sum_{i=1}^{\\dim(\\mathcal{H}_T)} \\begin{bmatrix} \\lambda_i & \\sqrt{1 - \\lambda_i^2} \\\\ \\sqrt{1 - \\lambda_i^2} & -\\lambda_i \\end{bmatrix} \\otimes \\ket{\\lambda_i}\\bra{\\lambda_i}. \\]\nBlock-encodings are in some sense a generalization of unitary operations, in the sense that if \\(U\\) is a block-encoding of \\(H\\), then by applying \\(U\\) on the state \\(\\ket{0}^{\\otimes a} \\otimes \\ket{\\psi}\\), and then measuring the ancilla register, we obtain the state \\(\\frac{H\\ket{\\psi}}{\\|H\\ket{\\psi}\\|}\\) with probability \\(\\|H\\ket{\\psi}\\|^2\\).\nNow suppose that we have access to a block-encoding of \\(H\\). Consider a matrix \\(A\\) that is a function of \\(H\\), say \\(A = f(H)\\). Can we find a block-encoding of \\(A\\)?"
  },
  {
    "objectID": "posts/2024/12/grand-unification.html#quantum-signal-processing-qsp",
    "href": "posts/2024/12/grand-unification.html#quantum-signal-processing-qsp",
    "title": "An Attempt at Understanding the Grand Unification",
    "section": "3 Quantum Signal Processing (QSP)",
    "text": "3 Quantum Signal Processing (QSP)\nLet us start with the simplest case: consider your matrix \\(H\\) to be a \\(1\\times 1\\) matrix, i.e., a scalar \\(a \\in [-1,1]\\). The following unitary is a block-encoding of \\(H\\) (this gate )\n\\[ R(a) = \\begin{bmatrix} a & \\sqrt{1 - a^2} \\\\ \\sqrt{1 - a^2} & -a \\end{bmatrix}. \\]\nWe prefer to continue with a slightly modified version of \\(R(a)\\), which we call \\(W(a)\\):\n\\[ W(a) = \\begin{bmatrix} a & i\\sqrt{1 - a^2} \\\\ i\\sqrt{1 - a^2} & a \\end{bmatrix}. \\]\nYou can easily verify that\n\\[ W(a) = i \\begin{bmatrix} e^{-i\\frac{\\pi}{4}} & 0 \\\\ 0 & e^{i\\frac{\\pi}{4}} \\end{bmatrix} R(a) \\begin{bmatrix} e^{-i\\frac{\\pi}{4}} & 0 \\\\ 0 & e^{i\\frac{\\pi}{4}} \\end{bmatrix}. \\]\nThis unitary can be seen as a rotation of the Bloch sphere around the \\(X\\)-axis by an angle \\(-2\\arccos(a)\\).\nNow, assume that we have also access to another unitary \\(S(\\phi)\\), that realizes a rotation with angle \\(-2\\phi\\) around the \\(Z\\)-axis, which means that\n\\[ S(\\phi) = \\begin{bmatrix} e^{i\\phi} & 0 \\\\ 0 & e^{-i\\phi} \\end{bmatrix}. \\]\nThis latter unitary is in fact a family of unitaries, giving a gate \\(S(\\phi)\\) for each \\(\\phi \\in [0,2\\pi]\\).\nNow, the question is, “having access to \\(W(a)\\) and \\(S(\\phi)\\), what sort of unitaries can we construct?”. In other words, if for \\(\\phi = (\\phi_0, \\phi_1, \\dots, \\phi_{d})\\), we define\n\\[ U_{\\phi} = S(\\phi_0) \\Pi_{i=1}^{d} W(a) S(\\phi_i), \\]\nthen, how can we characterize the set of unitaries \\(U_{\\phi}\\)?\nThe answer to this question is given by the main theorem of Quantum Signal Processing (QSP), which is as follows:\nTheorem (QSP): A QSP sequence as defined above can be expressed as\n\\[ \\begin{bmatrix} P(a) & iQ(a)\\sqrt{1 - a^2} \\\\ iQ^*(a)\\sqrt{1 - a^2} & P^*(a) \\end{bmatrix}, \\]\nfor \\(a \\in [-1,1]\\), where \\(P(a)\\) and \\(Q(a)\\) are polynomials satisfying 1. \\(\\deg(P) \\leq d\\), and \\(\\deg(Q) \\leq d-1\\), 2. \\(P\\) has parity \\(d \\mod 2\\) and \\(Q\\) has parity \\((d-1) \\mod 2\\), 3. For all \\(a \\in [-1,1]\\), \\(\\vert P(a)\\vert^2 + (1 - a^2)\\vert Q(a)\\vert^2 = 1\\).\nMoreover, for any \\(P\\) and \\(Q\\) satisfying the above conditions, there exists a QSP sequence realizing the above unitary.\nAs you can see, in this simple case, we have a rather neat characterization of achievable functions of \\(H\\), which was the matrix whose block-encoding we had access to.\nYou may wonder if we can get rid of the dependency to \\(Q\\) in the third condition. To this end, you can instead consider the possible transformations on the subspace spanned by \\(\\ket{+}\\), since\n\\[ \\bra{+} \\begin{bmatrix} P(a) & iQ(a)\\sqrt{1 - a^2} \\\\ iQ^*(a)\\sqrt{1 - a^2} & P^*(a) \\end{bmatrix} \\ket{+} = \\operatorname{Re}(P(a)) + \\operatorname{Re}(Q(a))i\\sqrt{1 - a^2}. \\]\nThis implies that for any real polynomial \\(P\\) of degree at most \\(d\\) that has parity \\(d \\mod 2\\), and for any \\(a \\in [-1,1]\\), \\(\\vert P(a)\\vert \\leq 1\\), there exists a QSP sequence realizing this polynomial on the subspace spanned by \\(\\ket{+}\\)."
  },
  {
    "objectID": "posts/2024/12/grand-unification.html#proof-of-the-qsp-theorem",
    "href": "posts/2024/12/grand-unification.html#proof-of-the-qsp-theorem",
    "title": "An Attempt at Understanding the Grand Unification",
    "section": "4 Proof of the QSP Theorem",
    "text": "4 Proof of the QSP Theorem\n\nProof of the Direct Part\nWe first prove by induction that (1) and (2) hold for any QSP sequence. Let \\(U_{\\phi} = S(\\phi_0) \\Pi_{i=1}^{d} W(a) S(\\phi_i)\\). For \\(d = 0\\), we have\n\\[U_{\\phi} = S(\\phi_0) = \\begin{bmatrix} e^{i\\phi_0} & 0 \\\\ 0 & e^{-i\\phi_0} \\end{bmatrix}, \\]\nand we see that \\(P(a) = e^{i\\phi_0}\\) and \\(Q(a) = 0\\), which are polynomials of degree \\(0\\) and \\(-\\infty\\), and parity \\(0\\) and \\(1\\), respectively.\nNow, assume that the conditions hold for \\(d=k-1\\). For \\(d=k\\), we have\n\\[ \\begin{align}\nU_{\\phi} &= \\left( S(\\phi_0) \\Pi_{i=1}^{k-1} W(a) S(\\phi_i) \\right) W(a) S(\\phi_k)\\\\\n&= \\begin{bmatrix} \\widetilde{P}(a) & i\\widetilde{Q}(a)\\sqrt{1 - a^2} \\\\ i\\widetilde{Q}^*(a)\\sqrt{1 - a^2} & \\widetilde{P}^*(a) \\end{bmatrix} \\begin{bmatrix} a e^{i\\phi_k} & i\\sqrt{1 - a^2} e^{-i\\phi_k} \\\\ i\\sqrt{1 - a^2} e^{i\\phi_k} & a e^{-i\\phi_k} \\end{bmatrix}\\\\\n&= \\begin{bmatrix} \\widetilde{P}(a) a e^{i\\phi_k} - \\widetilde{Q}(a)(1-a^2)e^{i\\phi_k} & i\\widetilde{P}(a)\\sqrt{1 - a^2} e^{-i\\phi_k} + i\\widetilde{Q}(a)a\\sqrt{1 - a^2} e^{-i\\phi_k} \\\\ i\\widetilde{Q}^*(a)a\\sqrt{1 - a^2} e^{i\\phi_k} + i\\widetilde{P}^*(a)\\sqrt{1 - a^2} e^{i\\phi_k} & -\\widetilde{Q}^*(a)(1 - a^2)e^{-i\\phi_k} + \\widetilde{P}^*(a)a e^{-i\\phi_k} \\end{bmatrix}.\n\\end{align} \\]\nDefining\n\\[ P(a) = \\widetilde{P}(a) a e^{i\\phi_k} - \\widetilde{Q}(a)(1-a^2)e^{i\\phi_k},\\]\nand\n\\[ Q(a) = \\widetilde{P}(a)e^{-i\\phi_k} + \\widetilde{Q}(a)a e^{-i\\phi_k}, \\]\nwe see that \\(P\\) and \\(Q\\) are polynomials of degree at most \\(k\\) and \\(k-1\\), and that the parity of \\(P\\) and \\(Q\\) are the opposite of the parity of \\(\\widetilde{P}\\) and \\(\\widetilde{Q}\\), respectively.\nFinally, we note that since \\(U_{\\phi}\\) is a unitary, we have \\(\\vert P(a)\\vert^2 + (1 - a^2)\\vert Q(a)\\vert^2 = 1\\).\n\n\nProof of the Converse Part\nFor the converse part, let us assume that\n\\[ T = \\begin{bmatrix} P(a) & iQ(a)\\sqrt{1 - a^2} \\\\ iQ^*(a)\\sqrt{1 - a^2} & P^*(a) \\end{bmatrix}, \\]\nwhere \\(P\\) and \\(Q\\) are polynomials satisfying (1), (2), and (3), for some \\(d\\). We want to show that a vector of angles \\(\\phi = (\\phi_0, \\phi_1, \\dots, \\phi_d)\\) exists such that \\(U_{\\phi} = T\\).\nWe prove this by induction on \\(\\deg(P)\\). For \\(\\deg(P) = 0\\), \\(P\\) is a constant, and for \\(a \\in [-1,1]\\), \\(P(a) = P(1)\\). From (3), we have \\(\\vert P(1)\\vert = 1\\), and since for infinitely many \\(a \\in [-1,1]\\), \\(\\vert Q(a)\\vert^2 (1 - a^2) = 0\\), by the fundamental theorem of algebra, we conclude that \\(Q(a) = 0\\). Since \\(\\vert P(a)\\vert = 1\\), \\(P(a) = e^{i\\phi_0}\\) for some \\(\\phi_0\\), and we have \\(T = S(\\phi_0)\\).\nTo choose the rest of the angles, it is easy to see that no matter what the value of \\(a\\) is, we always have\n\\[ W(a) S(\\frac{\\pi}{2}) W(a) S(-\\frac{\\pi}{2}) = I. \\]\nYou can either verify this by direct calculation, or try to convince yourself geometrically by noting that \\(W(a)\\) is a rotation around the \\(X\\)-axis by some angle \\(\\theta\\), and \\(S(\\frac{\\pi}{2})\\) and \\(S(-\\frac{\\pi}{2})\\) are rotations around the \\(Z\\)-axis by \\(-\\pi\\) and \\(\\pi\\), respectively.\nFinally, we note that \\(P\\) is a non-zero constant, thus its parity is even, and since (2) holds, \\(d\\) is even. Thus, we can set \\(\\phi_{2i+1} = \\pi/2\\) and \\(\\phi_{2i} = -\\pi/2\\) for \\(i = 0,1,\\dots,d/2\\), resulting in \\(U_{\\phi} = T\\).\nNow, assume that the statement holds for \\(\\deg(P) \\leq k-1\\). Consider \\(T\\) with polynomial \\(P\\) of degree \\(k &gt; 0\\).\nLemma: If \\(P\\) and \\(Q\\) satisfy (3), and \\(\\deg(P) = k &gt; 0\\), then \\(\\deg(Q) = k-1\\), and the leading coefficients of \\(P\\) and \\(Q\\) have the same modulus.\nProof of the Lemma: Let us write \\(P\\) and \\(Q\\) as\n\\[ P(a) = \\sum_{i=0}^{k} p_i a^i, \\quad Q(a) = \\sum_{i=0}^{k-1} q_i a^i, \\]\nwhere \\(p_k \\neq 0\\). From (3), we have\n\\[ \\vert P(a)\\vert^2 + (1 - a^2)\\vert Q(a)\\vert^2 = 1, \\]\nfor all \\(a \\in [-1,1]\\). The coefficients of the non-constant terms of the left-hand side have to be zero, thus \\(\\deg(Q) \\leq k-1\\). Moreover, the coefficient of \\(a^{2k}\\) in the left-hand side is \\(\\vert p_k\\vert^2 - \\vert q_{k-1}\\vert^2\\), which has to be zero. This implies that \\(\\deg(Q) = k-1\\), and \\(\\vert p_k\\vert = \\vert q_{k-1}\\vert\\).\nConsider the matrix\n\\[ T W(a)^\\dagger S(\\phi_{d})^\\dagger, \\]\nwhich can be written as\n\\[ \\begin{align}\nT W(a)^\\dagger S(\\phi_{d})^\\dagger &= \\begin{bmatrix} P(a) & iQ(a)\\sqrt{1 - a^2} \\\\ iQ^*(a)\\sqrt{1 - a^2} & P^*(a) \\end{bmatrix} \\begin{bmatrix} a e^{-i\\phi_d} & -i\\sqrt{1 - a^2} e^{-i\\phi_d} \\\\ -i\\sqrt{1 - a^2} e^{i\\phi_d} & a e^{i\\phi_d} \\end{bmatrix}\\\\\n&= \\begin{bmatrix} P(a) a e^{-i\\phi_d} + Q(a)(1 - a^2)e^{i \\phi_d} & i\\sqrt{1 - a^2} \\left(- P(a) e^{-i\\phi_d} + Q(a) a e^{i\\phi_d} \\right) \\\\ i\\sqrt{1 - a^2} \\left(- P^*(a) e^{i\\phi_d} + Q^*(a) a e^{-i\\phi_d} \\right) & P^*(a) a e^{i\\phi_d} + Q^*(a)(1 - a^2)e^{-i \\phi_d} \\end{bmatrix}.\n\\end{align} \\]\nWe can choose \\(\\phi_d\\) such that \\(e^{2i\\phi_d} = \\frac{p_k}{q_{k-1}}\\) (From the Lemma, we know that this is possible). Then, the \\(a^{k+1}\\) term in \\(P(a) a e^{-i\\phi_d} + Q(a)(1 - a^2)e^{i \\phi_d}\\) will have a coefficient equal to zero, and since the parity of \\(P\\) and \\(Q\\) are \\(d \\mod 2\\) and \\((d-1) \\mod 2\\), respectively, the coefficient of the \\(a^{k-1}\\) in \\(P(a)\\) and \\(a^{k-2}\\) in \\(Q(a)\\) is zero, implying that \\(P(a) a e^{-i\\phi_d} + Q(a)(1 - a^2)e^{i \\phi_d}\\) is a polynomial of degree at most \\(k-2\\). Similarly, by taking the same \\(\\phi_d\\), \\(P(a) e^{-i \\phi_d} + a e^{i\\phi_d} Q(a)\\) will be a polynomial of degree at most \\(k-2\\). Moreover, the parity of \\(P(a) a e^{-i\\phi_d} + Q(a)(1 - a^2)e^{i \\phi_d}\\) and \\(P(a) e^{-i \\phi_d} + a e^{i\\phi_d} Q(a)\\) are \\(d-1 \\mod 2\\) and \\(d - 2 \\mod 2\\), respectively. Finally, \\(T W(a)^\\dagger S(\\phi_{d})^\\dagger\\) is also a unitary, and the third condition is preserved for the new polynomials.\nBy the induction hypothesis, we can find \\(\\phi_0, \\phi_1, \\dots, \\phi_{d-1}\\) such that $T W(a)^S(_{d})^$ is a QSP sequence with the corresponding angles. By right-multiplying this unitary with \\(W(a) S(\\phi_d)\\), we obtain \\(T\\) as a QSP sequence with angles \\(\\phi_0, \\phi_1, \\dots, \\phi_{d}\\).\nThe above proof gives a constructive way of finding the angles \\(\\phi\\) for given polynomials \\(P\\) and \\(Q\\) satisfying the conditions of the QSP theorem, and this vector of angles can be computed in time \\(O(d^2)\\)."
  },
  {
    "objectID": "posts/2024/11/source-coding.html",
    "href": "posts/2024/11/source-coding.html",
    "title": "Classical and Quantum Source Coding",
    "section": "",
    "text": "1 Introduction\n  2 Fixing Some Notation\n  3 Compression: A Rigorous Definition, and a Fundamental Threshold\n  4 Proof of Shannon’s Source Coding Theorem For Fixed-Length Codes\n  5 Lifting the Result to the Quantum Realm\n  6 Acknowledgement\n  7 Bibliography\nIt’s unfortunate that there aren’t many good open-source fonts designed specifically for dyslexic readers. However, there’s a helpful Chrome extension that can change the font of the text you read online, making it easier to follow.\nDisclaimer: There are many excellent resources available on source coding from different viewpoints, varying from pure mathematical treatments (e.g., see this mini-course by Alain Chenciner) to those that have a more engineering flavor. There are also many textbooks, such as Cover and Thomas that include a chapter on source coding. This blog post is intended to only gather some of the ideas around source coding that I have found interesting, important or insightful."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#introduction",
    "href": "posts/2024/11/source-coding.html#introduction",
    "title": "Classical and Quantum Source Coding",
    "section": "1 Introduction",
    "text": "1 Introduction\nAs it is customary in information theory, we model the source of information as a stochastic process over a finite alphabet \\(\\mathcal{X}\\). The source emits symbols from \\(\\mathcal{X}\\) according to some probability distribution. Here, we limit ourselves to the case of memoryless sources with i.i.d. outputs, i.e. there is a random variable \\(X\\) such that for all \\(i\\), \\(X_i \\sim X\\), and for all \\(i \\neq j\\), \\(X_i\\) and \\(X_j\\) are independent. In this case, the source can be characterized by the probability mass function \\(p(x)\\) of \\(X\\). We may occasionally refer to “\\(n\\) uses of the source”, which means that we are considering a message of length \\(n\\) generated by the source.\nThe main problem we are considering in the source coding setting is to find a way to compress the source outputs (say, \\(n\\) uses of the source) in a reliable way, i.e. we want to encode the source output in a way that allows us to recover the original message.\nYou may say that compression is not possible if one wants to strictly shorten the message, while having the ability to recover the original message with certainty. However, as Umesh Vazirani used to point out to his students, “If you’ve never missed a flight, you’re spending too much time in airports.” We can sacrifice the perfect recovery of the original message in order to achieve much shorter encodings, while still being able to recover the original message with high probability. Shannon’s source coding theorem tells us there is some fundamental limit, however, on how much we can compress the source outputs while still being able to reliably recover the original message. This limit is governed by the entropy of the source."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#fixing-some-notation",
    "href": "posts/2024/11/source-coding.html#fixing-some-notation",
    "title": "Classical and Quantum Source Coding",
    "section": "2 Fixing Some Notation",
    "text": "2 Fixing Some Notation\nIn the rest of this blog post, I will use uppercase letters to denote random variables, and lowercase letters to denote their realizations. For example, \\(X\\) is a random variable and \\(x\\) is a possible realization of \\(X\\). By \\(x^n\\), I mean an output of \\(n\\) uses of the source, i.e. \\(x^n = x_1x_2\\ldots x_n\\). Similarly, by \\(X^n\\), I mean the random variable corresponding to \\(n\\) uses of the source, i.e. \\(X^n = (X_1, X_2, \\ldots, X_n)\\).\nI use \\(p(X)\\) to denote the probability mass function of \\(X\\), and also extend this notation to \\(p(X^n)\\) for the joint probability mass function of \\(n\\) i.i.d. random variables \\((X_1, X_2, \\ldots, X_n)\\)."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#compression-a-rigorous-definition-and-a-fundamental-threshold",
    "href": "posts/2024/11/source-coding.html#compression-a-rigorous-definition-and-a-fundamental-threshold",
    "title": "Classical and Quantum Source Coding",
    "section": "3 Compression: A Rigorous Definition, and a Fundamental Threshold",
    "text": "3 Compression: A Rigorous Definition, and a Fundamental Threshold\nA naive way of encoding \\(n\\) uses of the source into a binary string gives us a code of length \\(n \\log |\\mathcal{X}|\\). Can we do better? What do we exactly mean by “better”?\nDefinition (Fixed-length Compression Schemes): Let \\(n\\) be a positive integer, \\(\\alpha &gt; 0\\) and \\(\\delta \\in [0,1]\\). A fixed-length \\((n, \\alpha, \\delta)\\) compression scheme is a pair of functions \\((f,g)\\), where \\(f: \\mathcal{X}^n \\to \\\\{0,1\\\\}^{\\lfloor \\alpha n \\rfloor}\\) and \\(g: \\\\{0,1\\\\}^{\\lfloor \\alpha n \\rfloor} \\to \\mathcal{X}^n\\), are encoding and decoding functions, respectively, such that\n\\[ \\mathbb{P} [X^n \\in G] \\geq 1 - \\delta, \\]\nwhere\n\\[ G = \\{ x^n \\in \\mathcal{X}^n : g(f(x^n)) = x^n \\}. \\]\nIn the above definition, \\(\\alpha\\) is the number of bits per source use in the compressed message, and \\(\\delta\\) is the probability of decoding error. Ideally, we would like to find a compression scheme that has both \\(\\alpha\\) and \\(\\delta\\) as small as possible. Your first intuition might be that there is a trade-off between \\(\\alpha\\) and \\(\\delta\\). Well, it turns out that that this intuition is not entirely correct. The following theorem, which is the fixed-length version of what Shannon proved in his seminal paper, tells us that if the rate chosen is less than the entropy of the source, then even for the error probability tending to one, we cannot find a compression scheme for large enough \\(n\\).\nTheorem (Shannon’s Source Coding Theorem): Let \\(\\delta \\in (0,1)\\). - If \\(\\alpha &gt; H(X)\\), then for all but finitely many \\(n\\), there exists a fixed-length \\((n, \\alpha, \\delta)\\) compression scheme. - If \\(\\alpha &lt; H(X)\\), then for all but finitely many \\(n\\), there does not exist a fixed-length \\((n, \\alpha, \\delta)\\) compression scheme.\nWhere does this statement come from? There is a simple intuition behind this theorem. When the source is i.i.d. and \\(n\\) is large enough, we know that a word of length \\(n\\) generated by the source has approximately \\(n p(a)\\) occurrences of every symbol \\(a \\in \\mathcal{X}\\), with high probability. This is really what enables us to compress the source outputs. Information theorists call this “typicality”, or “Asymptotic Equipartition Property (AEP)”. For a mathematician, this is just a concentration of measure phenomenon.\nTo make this more formal, let us define typical sets.\nDefinition (Typical Sets): Let \\(X\\) be an i.i.d. source over the alphabet \\(\\mathcal{X}\\), \\(n \\in \\mathbb{N}\\) and \\(\\epsilon &gt; 0\\). An \\((n,\\epsilon)\\)-typical set with respect to \\(X\\) is the set of all strings \\(x^n = x_1x_2\\ldots x_n \\in \\mathcal{X}^n\\) that satisfy\n\\[ 2^{-n(H(X)+\\epsilon)} \\leq p(x_1)p(x_2)\\ldots p(x_n) \\leq 2^{-n(H(X)-\\epsilon )}. \\]\nWe denote the \\((n,\\epsilon)\\)-typical set by \\(T_{n,\\epsilon}^X\\). (we usually drop the superscript \\(X\\) when the source is clear from the context).\nYou might also see some authors define the typical set as\n\\[\\{x^n \\in \\mathcal{X}^n: |\\frac{1}{n}\\log \\frac{1}{p(x^n)} - H(X)| \\leq \\epsilon\\}.\\]\nIt is a simple exercise to show that these two definitions are equivalent.\nWait a minute! How did the entropy suddenly come into the play? Let’s go back to the intuition we had earlier. With high probability, a word of length \\(n\\) generated by the source has approximately \\(n p(a)\\) occurrences of every symbol \\(a \\in \\mathcal{X}\\). What is the probability for one such word to be generated? It is\n\\[ \\Pi_{a \\in \\mathcal{X}} p(a)^{n p(a)} = \\Pi_{a \\in \\mathcal{X}} 2^{\\log p(a) n p(a)} = 2^{n\\sum_{a \\in \\mathcal{X}} p(a) \\log p(a)} = 2^{-n H(X)}. \\]\nSo instead of characterizing the most likely words by their number of occurrences of each symbol, we may decide to characterize them by their probability of being generated. The latter might be better, as now we can use a powerful tool from probability theory, called the law of large numbers, to really show that these strings are indeed the most likely ones.\nFirst, let us see that for any \\(\\epsilon &gt; 0\\), the probability that a word generated by the source is in \\(T_{n,\\epsilon}\\) tends to one as \\(n\\) grows. It is in fact very simple: All we need to show is that the expectation of the i.i.d. random variables \\(-\\log p(X_i)\\) is \\(H(X)\\). This is a little exercise for one who knows the definition of entropy.\nNow, note that \\(\\frac{1}{n} \\log \\frac{1}{p(X^n)} = \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{1}{p(X_i)}\\). By the (weak) law of large numbers, we have\n\\[ \\frac{1}{n} \\log \\frac{1}{p(X^n)} = \\frac{1}{n} \\sum_{i=1}^n \\log \\frac{1}{p(X_i)} \\xrightarrow{p} H(X), \\]\nas \\(n \\to \\infty\\). This means that for any \\(\\epsilon &gt; 0\\) and for any \\(\\delta &gt; 0\\),\n\\[ \\mathbb{P} [X^n \\in T_{n,\\epsilon}] \\geq 1 - \\delta, \\]\nfor large enough \\(n\\), or equivalently,\n\\[ \\lim_{n \\to \\infty} \\mathbb{P} [X^n \\in T_{n,\\epsilon}] = 1. \\]\nWhat we obtained above also tells us something about the cardinality of the typical set. For any \\(\\epsilon, \\delta &gt; 0\\), we showed that\n\\[ \\mathbb{P} [X^n \\in T_{n,\\epsilon}] \\geq 1 - \\delta, \\]\nWe have\n\\[ \\begin{align}\n(1-\\delta) \\leq \\mathbb{P} [X^n \\in T_{n,\\epsilon}] & = \\sum_{x^n \\in T_{n,\\epsilon}} p(x^n) \\\\\n& \\leq |T_{n,\\epsilon}| 2^{-n(H(X)-\\epsilon)},\n\\end{align} \\]\nwhich implies that \\(\\|T_{n,\\epsilon}\\| \\geq 2^{n(H(X)-\\epsilon)}(1-\\delta)\\). Similarly, we can show that \\(\\|T_{n,\\epsilon}\\| \\leq 2^{n(H(X)+\\epsilon)}\\). This means that the typical set has a cardinality approximately equal to \\(2^{nH(X)}\\)."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#proof-of-shannons-source-coding-theorem-for-fixed-length-codes",
    "href": "posts/2024/11/source-coding.html#proof-of-shannons-source-coding-theorem-for-fixed-length-codes",
    "title": "Classical and Quantum Source Coding",
    "section": "4 Proof of Shannon’s Source Coding Theorem For Fixed-Length Codes",
    "text": "4 Proof of Shannon’s Source Coding Theorem For Fixed-Length Codes\nWe are now ready to prove Shannon’s source coding theorem for fixed-length codes. The proof consists of two parts: the achievability part, which concerns the first bullet point of the theorem, and the converse part, which concerns the second bullet point.\n\nProof of Achievability\nAssume that \\(\\alpha &gt; H(X)\\). We wish to show that there exists a \\(T_{n,\\epsilon}\\) such that \\(\\|T_{n,\\epsilon}\\| \\leq 2^{\\lfloor n\\alpha \\rfloor}\\). If we can show the existence of such a typical set, defining a compression scheme is easy. Let \\(F: \\mathcal{X}^n \\to T_{n,\\epsilon}\\) be a function whose restriction to \\(T_{n,\\epsilon}\\) is identity, and it is arbitrary elsewhere. Index all the elements of \\(T_{n,\\epsilon}\\) in some way, and define \\(f(x^n)\\) to be the index of \\(F(x^n)\\). Now, define \\(g\\) to be the inverse of \\(f\\). This way, the set \\(G\\) corresponding to the compression scheme \\((f,g)\\) is exactly \\(T_{n,\\epsilon}\\), and we know that for any \\(\\delta &gt; 0\\), for large enough \\(n\\), \\(T_{n,\\epsilon}\\) has probability at least \\(1-\\delta\\). Hence, the probability of correct decoding is at least \\(1-\\delta\\).\nNow, let us show the existence of such a typical set.\nSince \\(\\alpha &gt; H(X)\\), there exists an \\(\\epsilon &gt; 0\\) such that \\(\\alpha &gt; H(X) + 2\\epsilon\\). Take \\(n\\) large enough so that \\(n \\geq \\frac{1}{\\epsilon}\\), and it is also large enough so that \\(\\mathbb{P} [X^n \\in T_{n,\\epsilon}] \\geq 1 - \\delta\\). We have\n\\[ |T_{n,\\epsilon}| \\leq 2^{n(H(X)+\\epsilon)} \\leq 2^{n\\alpha - n \\epsilon} \\leq 2^{n\\alpha - 1} \\leq 2^{\\lfloor n\\alpha \\rfloor}. \\]\nThis completes the proof of the achievability part.\n\n\nProof of Converse\nAssume that \\(\\alpha &lt; H(X)\\), and let \\((f,g)\\) be a fixed-length \\((n,\\alpha,\\delta)\\) compression scheme. Consider the set \\(G = \\{ x^n \\in \\mathcal{X}^n : g(f(x^n)) = x^n \\}\\). Take \\(\\epsilon &gt; 0\\) such that \\(\\alpha &lt; H(X) - \\epsilon\\). We have\n\\[ \\begin{align}\n\\mathbb{P} [X^n \\in G] & = \\mathbb{P} [X^n \\in G \\cap T_{n,\\epsilon}] + \\mathbb{P} [X^n \\in G \\cap T_{n,\\epsilon}^c] \\\\\n& \\leq \\mathbb{P} [X^n \\in G \\cap T_{n,\\epsilon}] + \\mathbb{P} [X^n \\in T_{n,\\epsilon}^c] \\\\\n& \\leq |G| 2^{-n(H(X)-\\epsilon)} + \\mathbb{P} [X^n \\in T_{n,\\epsilon}^c] \\\\\n& = 2^{\\lfloor n\\alpha \\rfloor - n(H(X)-\\epsilon)} + \\mathbb{P} [X^n \\in T_{n,\\epsilon}^c]\\\\\n& \\leq 2^{n(\\alpha - H(X) + \\epsilon)} + \\mathbb{P} [X^n \\in T_{n,\\epsilon}^c].\n\\end{align} \\]\nFor large enough \\(n\\), we know that \\(\\mathbb{P} [X^n \\in T_{n,\\epsilon}^c] \\leq \\frac{\\eta}{2}\\) for any \\(\\eta &gt; 0\\). Moreover, we have \\(\\alpha - H(X) + \\epsilon &lt; 0\\). This means that for large enough \\(n\\), we have \\(2^{n(\\alpha - H(X) + \\epsilon)} \\leq \\frac{\\eta}{2}\\). This implies that for large enough \\(n\\), \\(\\mathbb{P} [X^n \\in G] \\leq \\eta\\), for any \\(\\eta &gt; 0\\), which means that the probability of correct decoding is not lower bounded by \\(1-\\delta\\) for any \\(\\delta \\in (0,1)\\).\nThus, a fixed-length \\((n,\\alpha,\\delta)\\) compression scheme does not exist for large enough \\(n\\), which completes the proof of the converse part."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#lifting-the-result-to-the-quantum-realm",
    "href": "posts/2024/11/source-coding.html#lifting-the-result-to-the-quantum-realm",
    "title": "Classical and Quantum Source Coding",
    "section": "5 Lifting the Result to the Quantum Realm",
    "text": "5 Lifting the Result to the Quantum Realm\nIn the quantum setting, an i.i.d. source is modelled by a quantum ensemble \\(\\\\{p(x), \\ket{\\psi_x}\\\\}\\), or equivalently, the density operator \\(\\rho = \\sum_{x} p(x) \\ket{\\psi_x}\\bra{\\psi_x}\\). The question is, how many qubits do we need to encode \\(n\\) uses of the source reliably? A naive answer is \\(n \\log \\operatorname{dim}(\\rho)\\). However, as in the classical case, we can do better.\n\nQuantum Compression Schemes\nThe density matrix corresponding to the \\(n\\) uses of the channel is \\(\\rho^{\\otimes n}\\). A quantum compression scheme is a pair of quantum channels \\((\\mathcal{E}, \\mathcal{D})\\), where \\(\\mathcal{E}: \\mathcal{L}(A^{\\otimes n}) \\to \\mathcal{L}(\\mathbb{C}^{2^{\\lfloor \\alpha n \\rfloor}})\\) is the encoding channel, and \\(\\mathcal{D}: \\mathcal{L}(\\mathbb{C}^{2^{\\lfloor \\alpha n \\rfloor}}) \\to \\mathcal{L}(A^{\\otimes n})\\) is the decoding channel, and \\(\\alpha\\) is the rate of the compression scheme.\nThe merit with which we define the reliablity is a bit more subtle in the quantum case. We require that the composition of the encoding and decoding channels is close to the identity channel. However, we have to be careful with the presence of entnaglement. It might be the case that the source is entangled with a reference system, in which case we want to recover with high probability this entanglement after decoding. In this case, we require that not only the channel \\(\\mathcal{D} \\circ \\mathcal{E} : \\mathcal{L}(A^{\\otimes n}) \\to \\mathcal{L}(A^{\\otimes n})\\) to be close to the identity channel, but for any reference system \\(R\\), the channel \\(\\mathcal{D} \\circ \\mathcal{E} \\otimes \\operatorname{id}_R\\) has also be close to the identity channel on \\(\\mathcal{L}(A^{\\otimes n} \\otimes R)\\).\nThis shows that why the following is not an adequate merit for relaibliity:\n\\[ \\Vert \\rho_A^{\\otimes n} - \\mathcal{D} \\circ \\mathcal{E}(\\rho_A^{\\otimes n}) \\Vert \\leq \\epsilon. \\]\nWith this merit, you can easily see that for any source \\(\\rho_A\\),\n\\[\\mathcal{E}(M) := \\operatorname{Tr}(M), \\]\nand\n\\[\\mathcal{D}(\\lambda) := \\lambda \\rho_A^{\\otimes n}, \\]\ndefine a reliable compression scheme that compresses \\(n\\) uses of the source into zero qubits! This is non-sense. You can verify that when we compress one qubit of a bipartite maximally entangled state using this scheme, we end up with the maximally mixed state as the state of the compound system, showing that the entanglement is totally destroyed.\nAny choice of a merit for reliability should take into account the entanglement of the source with a reference system. Here is a number of possible choices:\n\nThe channel fidelity with respect to \\(\\mathcal{D} \\circ \\mathcal{E}\\):\n\n\\[ F(\\rho_A^{\\otimes n}, \\mathcal{D} \\circ \\mathcal{E}(\\rho_A^{\\otimes n})) &gt; 1 - \\delta. \\]\n\nWe consider a purification of the source, \\(\\ket{\\psi}_{AR}\\), and require that the normalized trace distance between this state and the state after encoding and decoding is small:\n\n\\[ \\frac{1}{2}   \\Vert \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} - (\\mathcal{D} \\circ \\mathcal{E} \\otimes \\operatorname{id}_R)(\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) \\Vert_1 \\leq \\delta. \\]\n\nThe average ensemble trace distance: We can write\n\n\\[ \\rho_A^{\\otimes n} = \\sum_{x^n} p(x^n) \\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}}, \\]\nwhere \\(\\ket{\\psi_{x^n}} = \\ket{\\psi_{x_1}}\\ket{\\psi_{x_2}}\\dots \\ket{\\psi_{x_n}}\\), and \\(p(x^n) = p(x_1)p(x_2)\\cdots p(x_n)\\). Then, we require that\n\\[ \\frac{1}{2} \\sum_{x^n} p(x^n) \\Vert \\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}} - \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}}) \\Vert_1 \\leq \\delta. \\]\nFor the relationship between the first and the second, see Section 3.2.3 of Watrous’ book. We show here that the second implies the third.\nLet us first show that the second condition is independent of the choice of the purification of the source. Let \\(\\ket{\\phi}\\) be another purification of the source. Then, since \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\) are equal up to a local unitary on the reference system, we have\n\\[ \\begin{align}\n\\frac{1}{2} \\Vert (U_R)^{\\otimes n} \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} (U_R^{\\dagger})^{\\otimes n} &- \\mathcal{D} \\circ \\mathcal{E} [(U_R\\ket{\\psi}\\bra{\\psi}_{AR}U_R^\\dagger)^{\\otimes n}] \\Vert_1\\\\\n&= \\frac{1}{2} \\Vert U_R^{\\otimes n} \\left( \\ket{\\phi}\\bra{\\phi}_{AR}^{\\otimes n} - \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\phi}\\bra{\\phi}_{AR}^{\\otimes n}) \\right) (U_R^{\\dagger})^{\\otimes n} \\Vert_1 \\\\\n& = \\frac{1}{2} \\Vert \\ket{\\phi}\\bra{\\phi}_{AR}^{\\otimes n} - (\\mathcal{D} \\circ \\mathcal{E} \\otimes \\operatorname{id}_R)(\\ket{\\phi}\\bra{\\phi}_{AR}^{\\otimes n}) \\Vert_1.\n\\end{align} .\\]\nTo prove that the second condition implies the third one, we consider the purification\n\\[ \\ket{\\psi}_{AR} = \\sum_{x} \\sqrt{p(x)} \\ket{\\psi_x} \\ket{x}, \\]\nand the application of the channel\n\\[ \\Phi(M) = \\sum_{x^n} \\ket{x^n}\\bra{x^n} M \\ket{x^n}\\bra{x^n} \\]\nto the reference system. Using the data processing inequality for the trace distance, we have\n\\[ \\begin{align}\n\\Vert \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} - \\mathcal{D} \\circ \\mathcal{E} (\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) \\Vert_1 & \\geq \\Vert \\Phi(\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) - \\Phi(\\mathcal{D} \\circ \\mathcal{E} (\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n})) \\Vert_1 \\\\\n& = \\Vert \\sum_{x^n} p(x^n) \\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}} \\otimes \\ket{x^n}\\bra{x^n} - \\sum_{x^n} p(x^n) \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}}) \\otimes \\ket{x^n}\\bra{x^n} \\Vert_1 \\\\\n& = \\sum_{x^n} p(x^n) \\Vert \\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}} - \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\psi_{x^n}}\\bra{\\psi_{x^n}}) \\Vert_1.\n\\end{align} \\]\nIn the following, we limit ourselves to the second condition, as in Mark Wilde’s book. A proof based on the first condition can be found in Watrous’ book, or in Mike & Ike.\n\n\nSchumacher’s Theorem\nThe nice thing that we will see in a moment is that a similar regime as in the classical case holds in the quantum case: the rate of reliable compressions is governed by entropy of the source, which in the quantum case is the von Neumann entropy of the density operator \\(\\rho\\).\nTheorem (Schumacher’s Quantum Source Coding Theorem): Let \\(\\delta \\in (0,1)\\), and \\(\\rho\\) be a density operator on the system \\(A\\). - If \\(\\alpha &gt; H(A)_{\\rho}\\), then for all but finitely many \\(n\\), there exists an \\((n, \\alpha, \\delta)\\) quantum compression scheme. - If \\(\\alpha &lt; H(A)_{\\rho}\\), then for all but finitely many \\(n\\), there does not exist an \\((n, \\alpha, \\delta)\\) quantum compression scheme.\nIt is a good place to make a remark. Assume that instead of using a quantum compression scheme, we wanted to compress the source classically. Then, we could model our quantum source as a classical source \\(X\\) that generates a symbol \\(x \\in \\mathcal{X}\\) with probability \\(p(x)\\). For any $ &gt; H(X)$ and \\(\\delta \\in (0,1)\\), there exists an \\((n, \\alpha, \\delta)\\) classical compression scheme for large enough \\(n\\). Upon decoding then, we would generate the state \\(\\ket{\\psi_x}\\) according to the classically decoded string \\(x\\). This shows that the rates greater than \\(H(X)\\) are achievable. However, recall that \\(H(X) \\geq H(A)_{\\rho}\\), which together with the above theorem implies that in compressing quantum data, better rates may be achieved by using quantum compression schemes.\n\n\nTypical Subspaces\nAs in the classical case, the main ingredient of source coding is the notion of typicality. In the quantum case, however, instead of considering typical sets of the ensemble distribution, we look at the typical sets with respect to the distribution given by the eigenvalues of the density operator.\nAssume \\(\\rho\\) has a spectral decomposition \\(\\rho = \\sum_i \\lambda_i \\ket{x_i}\\bra{x_i}\\). We define the typical set \\(T_{n,\\epsilon}\\) as\n\\[ T_{n,\\epsilon} = \\{ x^n \\in \\mathcal{X}^n : 2^{-n(H(A)_{\\rho}+\\epsilon)} \\leq \\Pi_{i=1}^n \\lambda_{x_i} \\leq 2^{-n(H(A)_{\\rho}-\\epsilon)} \\}, \\]\nand the typical subspace \\(V_{n,\\epsilon}\\) as\n\\[ V_{n,\\epsilon} = \\operatorname{span} \\{ \\ket{x^n} : x^n \\in T_{n,\\epsilon} \\}. \\]\nWe also denote the orthogonal projection onto \\(V_{n,\\epsilon}\\) by \\(\\Pi_{n,\\epsilon}\\). As in the classical case, typical subspaces have nice properties:\nProposition (Properties of Typical Subspaces): - For any \\(\\epsilon &gt; 0\\), \\(\\lim_{n \\to \\infty} \\operatorname{Tr}(\\Pi_{n,\\epsilon}\\rho^{\\otimes n}) = 1\\). - For any \\(\\epsilon &gt; 0\\) and \\(\\delta &gt; 0\\),\n\\[ (1-\\delta) 2^{n(H(A)_{\\rho}-\\epsilon)} \\leq \\operatorname{dim}(V_{n,\\epsilon}) \\leq 2^{n(H(A)_{\\rho}+\\epsilon)}. \\]\nHaving these properties, we can now prove Schumacher’s theorem.\n\n\nProof of Schumacher’s Theorem: Achievability\nLet \\(\\alpha &gt; {H(A)}_{\\rho}\\). Similar to the proof of Shannon’s theorem, consider a typical set \\(T_{n,\\epsilon}\\) whose dimension suits the rate \\(\\alpha\\). We define the encoding and decoding channels as\n\\[ \\mathcal{E}(M) = K_n M K_n^{\\dagger} + \\operatorname{Tr}(M (I - K_n^\\dagger K_n))\\sigma, \\]\nand\n\\[ \\mathcal{D}(N) = K_n^{\\dagger} N K_n + \\operatorname{Tr}(N (I - K_n K_n^\\dagger))\\tau, \\]\nwhere\n\\[ K_n = \\sum_{x^n \\in T_{n,\\epsilon}} \\ket{x^n}\\bra{x^n}, \\]\nand \\(\\sigma\\) and \\(\\tau\\) are arbitrary density operators on the system \\(\\mathbb{C}^{2^{\\lfloor \\alpha n \\rfloor}}\\) and \\(A^{n}\\), respectively.\nNote that\n\\[ \\{K_n\\} \\cup \\{\\sqrt{\\lambda_j} \\ket{\\phi_j}\\bra{i} \\sqrt{I - K_n^\\dagger K_n}\\}_{i,j} \\]\nis a Kraus decomposition of \\(\\mathcal{E}\\), with \\(\\sigma = \\sum_{j} \\lambda_j \\ket{\\phi_j}\\bra{\\phi_j}\\). Thus, \\(\\mathcal{E}\\) (and similarly \\(\\mathcal{D}\\)) is indeed a quantum channel.\nIt is not difficult to see that for \\(\\ket{\\psi}_{AR}\\), the purification of \\(\\rho_A\\), we have\n\\[ \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) = \\Pi_{n,\\epsilon} \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\Pi_{n,\\epsilon} + \\operatorname{Tr}_{A^n}\\left( (I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\right) \\otimes \\mathcal{D}(\\sigma). \\]\nThen,\n\\[ \\Vert \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} - \\mathcal{D} \\circ \\mathcal{E}(\\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) \\Vert_1 \\leq \\Vert \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} - \\Pi_{n,\\epsilon} \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\Pi_{n,\\epsilon} \\Vert_1 + \\Vert \\operatorname{Tr}_{A^n}\\left( (I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\right) \\otimes \\mathcal{D}(\\sigma) \\Vert_1. \\]\nNow, we need to bound each term on the right-hand side. To do this, we recall two lemmas:\nLemma (Gentle Measurement Lemma): Let \\(\\rho\\) be a density operator on the system \\(A\\), and \\(P\\) be a positive operator on the system \\(A\\) such that \\(I - P\\) is also positive. If \\(\\operatorname{Tr}(\\rho P) \\geq 1 - \\epsilon\\), then\n\\[ \\Vert \\rho - P \\rho P \\Vert_1 \\leq 2\\sqrt{\\epsilon}. \\]\nLemma (A Partial Trace Identity): For \\(X_{AB} \\in \\mathcal{L}(AB)\\), and \\(N, M \\in \\mathcal{L}(A)\\), we have\n\\[ \\operatorname{Tr}_{B}((N \\otimes I) X_{AB} (M \\otimes I)) = N \\operatorname{Tr}_{B}(X_{AB}) M. \\]\nUsing the above lemma, for any \\(\\delta &gt; 0\\), we have\n\\[ \\begin{align}\n\\operatorname{Tr}(\\Pi_{n,\\epsilon} \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) & = \\operatorname{Tr}(\\operatorname{Tr}_{R^n}(\\Pi_{n,\\epsilon} \\ket{\\psi}\\bra{\\psi}_{AR}^{n})) \\\\\n& = \\operatorname{Tr}(\\Pi_{n,\\epsilon} \\rho_A^{\\otimes n}) \\\\\n& \\geq 1 - \\delta,\n\\end{align} \\]\nfor large enough \\(n\\). Using the Gentle Measurement Lemma, we conclude that\n\\[ \\Vert \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} - \\Pi_{n,\\epsilon} \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\Pi_{n,\\epsilon} \\Vert_1 \\leq 2\\sqrt{\\delta}. \\]\nSimilarly, for any \\(\\delta &gt; 0\\), we have\n\\[ \\begin{align}\n\\Vert \\operatorname{Tr}_{A^n}\\left( (I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n} \\right) \\otimes \\mathcal{D}(\\sigma) \\Vert_1 & = \\operatorname{Tr}( (I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n}) \\operatorname{Tr}(\\mathcal{D}(\\sigma)) \\\\\n& = \\operatorname{Tr}( (I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n})\\\\\n& = \\operatorname{Tr}(\\operatorname{Tr}_{R^n}((I - \\Pi_{n,\\epsilon}) \\ket{\\psi}\\bra{\\psi}_{AR}^{\\otimes n})) \\\\\n& = \\operatorname{Tr}((I - \\Pi_{n,\\epsilon}) \\rho_A^{\\otimes n}) \\\\\n& = 1 - \\operatorname{Tr}(\\Pi_{n,\\epsilon} \\rho_A^{\\otimes n}) \\\\\n& \\leq \\delta,\n\\end{align} \\]\nfor large enough \\(n\\). This completes the proof of the achievability part."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#acknowledgement",
    "href": "posts/2024/11/source-coding.html#acknowledgement",
    "title": "Classical and Quantum Source Coding",
    "section": "6 Acknowledgement",
    "text": "6 Acknowledgement\nIn writing this blog post, I greatly benefited from the lecture materials from Thomas Debris-Alazard’s course on Information Theory and Peter Brown’s course on Quantum Information Theory. I would like to thank them for their excellent teaching and the resources they provided."
  },
  {
    "objectID": "posts/2024/11/source-coding.html#bibliography",
    "href": "posts/2024/11/source-coding.html#bibliography",
    "title": "Classical and Quantum Source Coding",
    "section": "7 Bibliography",
    "text": "7 Bibliography\nThe proof of Shannon’s source coding theorem given here is based on the proof of Theorem 5.40 in Watrous’ wonderful book, The theory of quantum information. You can (legally) find parts of the book online (e.g. here).\nAnother excellent resource that I humbly suggest you to read is the famous easy-to-read paper by Shannon, A Mathematical Theory of Communication. It is one of the best examples of “old but gold”.\nFor the quantum part, the proof presented here is based on the proof of Schumacher’s theorem in Mark Wilde’s book."
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html",
    "href": "posts/2025/01/quantifying-entanglement.html",
    "title": "Quantifying and Classifying Entanglement",
    "section": "",
    "text": "1 Some “Black Magic Calculus”\n  2 Deciding Entanglement is Hard\n  3 Practical vs. Foundational\n  4 Schmidt Number\n  5 Conditional Entropy\n  6 Distillable Entanglement and Entanglement Cost\nThis post is not complete yet!\nIt’s unfortunate that there aren’t many good open-source fonts designed specifically for dyslexic readers. However, there’s a helpful Chrome extension that can change the font of the text you read online, making it easier to follow.\nDisclaimer: There are many excellent resources available on this topic (see for example this excellent survey by the Horodeckis). This blog post is intended to only gather some of the ideas around entanglement quantification that I have learned and found interesting."
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#some-black-magic-calculus",
    "href": "posts/2025/01/quantifying-entanglement.html#some-black-magic-calculus",
    "title": "Quantifying and Classifying Entanglement",
    "section": "1 Some “Black Magic Calculus”",
    "text": "1 Some “Black Magic Calculus”\n\n\nWhen two systems, of which we know the states by their respective representatives, enter into temporary physical interaction due to known forces between them, and when after a time of mutual influence the systems separate again, then they can no longer be described in the same way as before, viz. by endowing each of them with a representative of its own. I would not call that one but rather the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought.-Erwin Schrödinger, Discussion of Probability Relations between Separated Systems\n\n\n\n\nQuantum mechanics: Real Black Magic Calculus. -Attributed to Albert Einstein\n\n\nWe all have seen magical information-theoretic applications of shared entanglement, such as teleportation, superdense coding, and entanglement swapping. Below, you can revisit these three protocols and their verification using the language of ZX-calculus.\nUnderstanding entanglement and studying it from an information-theoretic perspective is in fact at the heart of quantum information theory, and it is a very active area of research, with many open and challenging problems.\n\n\n\nTeleportation, revisited in ZX-calculus\n\n\n\n\n\nEntanglement Swapping, revisited in ZX-calculus\n\n\n\n\n\nSuperdense Coding, revisited in ZX-calculus\n\n\nIn this post, I will discuss some of the ways to quantify entanglement and classify it. It is however important to ask why we even need to quantify or characterize it."
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#deciding-entanglement-is-hard",
    "href": "posts/2025/01/quantifying-entanglement.html#deciding-entanglement-is-hard",
    "title": "Quantifying and Classifying Entanglement",
    "section": "2 Deciding Entanglement is Hard",
    "text": "2 Deciding Entanglement is Hard"
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#practical-vs.-foundational",
    "href": "posts/2025/01/quantifying-entanglement.html#practical-vs.-foundational",
    "title": "Quantifying and Classifying Entanglement",
    "section": "3 Practical vs. Foundational",
    "text": "3 Practical vs. Foundational"
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#schmidt-number",
    "href": "posts/2025/01/quantifying-entanglement.html#schmidt-number",
    "title": "Quantifying and Classifying Entanglement",
    "section": "4 Schmidt Number",
    "text": "4 Schmidt Number"
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#conditional-entropy",
    "href": "posts/2025/01/quantifying-entanglement.html#conditional-entropy",
    "title": "Quantifying and Classifying Entanglement",
    "section": "5 Conditional Entropy",
    "text": "5 Conditional Entropy\nThis way of quantifying entanglement roots back to about a century ago:\n\n\nThus one disposes provisionally (until the entanglement is resolved by actual observation) of only a common description of the two in that space of higher dimension. This is the reason that knowledge of the individual systems can decline to the scantiest, even to zero, while that of the combined system remains continually maximal. Best possible knowledge of a whole does not include best possible knowledge of its parts — and this is what keeps coming back to haunt us-Erwin Schrödinger, Die gegenwärtige Situation in der Quantenmechanik \n\n\nIn the late 90s, when the operational meaning of von Neumann entropy was established by Schumacher, people realized that what Schrödinger was talking about can actually be quantified in terms of the von Neumann entropy.\nRecall that in the classical setting, the conditional entropy of a random variable \\(X\\) given another random variable \\(Y\\), \\(H(X\\vert Y)\\), is always positive. Writing \\(H(X\\vert Y) = H(X,Y) - H(Y)\\), this is equivalent to saying that \\(H(X,Y) \\geq H(Y)\\). In other words, the entropy of a compound system is as large as the entropy of its subsystems. In particular, we can say that being certain about the state of a compound system implies certainty about the states of its subsystems.\nThis is not the case in the quantum setting. Consider a bipartite quantum system whose state is\n\\[ \\ket{\\phi^+}_{AB} = \\frac{1}{\\sqrt{2}}(\\ket{00} + \\ket{11}).\\]\nThere is no uncertainty about the state of the compound system, since \\(H(AB) = 0\\). However, if you calculate the reduced density matrix of one of the subsystems, say \\(A\\), you will find that \\(H(A) = 1\\). Here as you can see, the conditional entropy can be negative unlike its classical counterpart.\nIt turns out that this only happens when the compound system is entangled.\nProposition: For a bipartite quantum state \\(\\rho_{AB}\\), if \\(H(A \\vert B) &lt; 0\\), then \\(\\rho_{AB}\\) is entangled. Moreover, if a pure bipartite quantum state \\(\\ket{\\psi}_{AB}\\) is entangled, then \\(H(A\\vert B) &lt; 0\\).\nProof: Let us first assume that \\(\\rho_{AB}\\) is a pure state, \\(\\ket{\\psi}\\bra{\\psi}\\). If \\(\\psi\\) is a product state, then the reduced density matrix of \\(B\\) is a pure state, thus \\(H(A\\vert B) = 0\\). If \\(\\psi\\) is entangled, then its Scmidt number is at least 2, thus the reduced density matrix of \\(B\\) has rank at least 2. This implies that \\(H(B) &gt; 0\\). Since \\(H(A,B) = 0\\), we have \\(H(A\\vert B) &lt; 0\\).\nFor a mixed state \\(\\rho_{AB}\\), let us first consider the case that \\(\\rho_{AB}\\) is a product state. Then \\(H(AB) = H(A) + H(B)\\), thus \\(H(A\\vert B) = H(A) \\geq 0\\). For a separable state \\(\\rho_{AB} = \\sum_{i} p_{i} \\sigma_{i} \\otimes \\tau_{i}\\), we use the concavity of the conditional von Neumann entropy, as\n\\[ H(A\\vert B)_{\\sum_{i} p_{i} \\sigma_{i} \\otimes \\tau_{i}} \\geq \\sum_{i} p_{i} H(A\\vert B)_{\\sigma_{i} \\otimes \\tau_{i}} \\geq 0.\\]\nThe above theorem implies that the mapping\n\\[ \\rho_{AB} \\mapsto \\max\\{0, -H(A\\vert B)_{\\rho}\\} \\]\nis a possible entanglemeent quantuifier. Note that when a state is mapped to \\(0\\), it is not necessarily separable. However, if it is mapped to a non-zero value, it is entangled.\nIt is a good place to make a remark: How can we determine if a given state has neqgative conditional entropy in the lab? Conditional entropy is not an observable, thus we cannot measure it directly. It turns out that this is not that much of a problem, as pointed out in this paper.\nUsing the fact that the conditional entropy is concave, it is easy to show that the set of bipartite states of a certain dimension that have non-negative conditional entropy is convex. Moreover, as the conditional entropy is a continuous function (see this paper for a proof), and the set of non-negative values that the conditional entropy can take is closed, the set of states with non-negative conditional entropy is closed, and thus compact. Now, by Hahn-Banach theorem, for any state \\(\\rho_{AB}\\) with negative conditional entropy, there exists a Hermitian operator \\(W\\) such that \\(\\operatorname{Tr}(W \\rho_{AB}) &lt; 0\\) and for any state \\(\\sigma_{AB}\\) with non-negative conditional entropy, \\(\\operatorname{Tr}(W \\sigma_{AB}) \\geq 0\\). This means that there exists an observable whose expectation tells us whether a given state has negative conditional entropy or not.\nWhat can this observable be? In fact, there is an obvious natural choice: recall that the conditional entropy can be formulated in terms of the relative entropy: \\(H(A\\vert B) = - D(\\rho_{AB} \\mid I_{A} \\otimes \\rho_{B})\\). Now you can convince yourself that if we take \\(W = \\log(I_{A} \\otimes \\rho_{B}) - \\log(\\rho_{AB})\\), then \\(\\operatorname{Tr}(W \\rho_{AB}) &lt; 0\\). The other part is less obvious, but it is still simple to prove. You can do it by using the fact that the relative entropy is monotone."
  },
  {
    "objectID": "posts/2025/01/quantifying-entanglement.html#distillable-entanglement-and-entanglement-cost",
    "href": "posts/2025/01/quantifying-entanglement.html#distillable-entanglement-and-entanglement-cost",
    "title": "Quantifying and Classifying Entanglement",
    "section": "6 Distillable Entanglement and Entanglement Cost",
    "text": "6 Distillable Entanglement and Entanglement Cost\nRemember from our discussion at the beginning of this post that if Alice and Bob share an EPR pair, they can use it to teleport a qubit, or do superdense coding. In practice, in the presence of noise, however, when Alice produces an EPR pair and sends one of the qubits to Bob, the state that Bob receives may change, and the final shared state between Alice and Bob may not be an EPR pair. Can we still use this state to perform information-theoretic tasks that require shared EPR pairs?\nThis motivates the definition of two important measures of entanglement: the distillable entanglement and the entanglement cost.\nDefinition: Given a bipartite quantum state \\(\\rho_{AB}\\), a rate of distillation \\(\\alpha\\) is said to be achievable if there exists a sequence of LOCC channels \\(\\mathcal{E}_{n}: \\mathcal{L}(A^n B^n) \\to \\mathcal{L}\\left((\\mathbb{C^{2}}\\otimes \\mathbb{C^{2}})^{\\otimes \\lfloor \\alpha n \\rfloor}\\right)\\) such that\n\\[ \\lim_{n \\to \\infty} \\Vert \\mathcal{E}_{n}(\\rho_{AB}^{\\otimes n}) - \\ket{\\phi^{+}}\\bra{\\phi^{+}}^{\\otimes \\lfloor \\alpha n \\rfloor} \\Vert_{1} = 0.\\]\nThen, the distillable entanglement of \\(\\rho_{AB}\\) is defined as\n\\[ E_{D}(\\rho_{AB}) = \\sup \\{\\alpha: \\alpha \\text{ is achievable}\\}.\\]\nDefinition: Given a bipartite quantum state \\(\\rho_{AB}\\), a rate of entanglement cost \\(\\alpha\\) is said to be achievable if there exists a sequence of LOCC channels \\(\\mathcal{E}_{n}: \\mathcal{L}\\left((\\mathbb{C^{2}}\\otimes \\mathbb{C^{2}})^{\\otimes \\lfloor \\alpha n \\rfloor}\\right) \\to \\mathcal{L}(A^n B^n)\\) such that\n\\[ \\lim_{n \\to \\infty} \\Vert \\mathcal{E}_{n}(\\ket{\\phi^{+}}\\bra{\\phi^{+}}^{\\otimes \\lfloor \\alpha n \\rfloor}) - \\rho_{AB}^{\\otimes n} \\Vert_{1} = 0.\\]\nThen, the entanglement cost of \\(\\rho_{AB}\\) is defined as\n\\[ E_{C}(\\rho_{AB}) = \\inf \\{\\alpha: \\alpha \\text{ is achievable}\\}.\\]\nWhen you think about it operationally, it should be clear that \\(E_{C}(\\rho_{AB}) \\geq E_{D}(\\rho_{AB})\\). This can be proved formally.\nAs we mentioned earlier, these two quantities can be used to quantify the amount of entanglement in a given state. To see this, first note that for any separable state \\(\\rho_{AB} = \\sum_{x} p_{x} \\sigma_{x} \\otimes \\tau_{x}\\), we have \\(E_{D}(\\rho_{AB}) = E_{C}(\\rho_{AB}) = 0\\). The reason is that for sharing such a state between Alice and Bob, we don’t even need to use any EPR pairs. Alice and Bob can locally prepare the states \\(\\sigma_{x}\\) and \\(\\tau_{x}\\), respectively, and then the state \\(\\rho_{AB}\\) can be prepared by the convex combination of the corresponding LOCC channels. Thus, the entanglement cost of a separable state is 0, as well as its distillable entanglement."
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html",
    "href": "posts/2024/06/PrimeInfinitude.html",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "",
    "text": "1 Introduction\n  2 Chaitin’s Proof\n  3 Thakkar’s Proof\n  4 Acknowledgement\n  5 Bibliography\nIt’s unfortunate that there aren’t many good open-source fonts designed specifically for dyslexic readers. However, there’s a helpful Chrome extension that can change the font of the text you read online, making it easier to follow."
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html#introduction",
    "href": "posts/2024/06/PrimeInfinitude.html#introduction",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "1 Introduction",
    "text": "1 Introduction\nPrime numbers are among the most fascinating objects in mathematics, and their study has led to advancements in several areas of math. One of the oldest known facts about primes is that the set of primes is not finite. The earliest existing proof for this statement is perhaps the one due to Euclid and can be found in Elements, Book IX, Proposition 20. Many different proofs, however, have emerged since then, with a comprehensive list compiled in [1].\nIn this blog post, I will present two computer science-inspired proofs for the infinitude of primes. The first proof is due to Gregory Chaitin [2] and it uses the notion of Shannon entropy, which is a fundamental concept in information theory. The second proof is due to Aalok Thakkar [3], which was recently featured in American Mathematical Monthly. The latter uses some tools from automata theory."
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html#chaitins-proof",
    "href": "posts/2024/06/PrimeInfinitude.html#chaitins-proof",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "2 Chaitin’s Proof",
    "text": "2 Chaitin’s Proof\nLet \\(n\\) be a natural number, and define \\(N\\) as a uniform random variable whose support is the set \\(\\{ 1,2, \\dots, n \\}\\). The number of primes less than or equal to \\(n\\) is denoted by \\(\\pi(n)\\), known as the prime-counting function. Let \\(p_1 &lt; p_2 &lt; \\cdots &lt; p_{\\pi(n)}\\) represent the sequence of primes less than or equal to \\(n\\). For \\(1 \\leq i \\leq \\pi(n)\\), define a random variable \\(X_i\\) as the exponent of \\(p_i\\) in the prime factorization of \\(N\\).\nBy the fundamental theorem of arithmetic, \\((X_1, \\dots , X_{\\pi(n)})\\) is a function of \\(N\\) and vice versa. Therefore, we have \\[ H(X_1, \\dots, X_{\\pi(n)}) = H(N), \\] where \\(H(\\cdot)\\) denotes Shannon entropy. Since \\(N\\) is a uniform random variable on a support of size \\(n\\), \\(H(N) = \\log n\\). On the other hand, we can bound \\(H(X_1, \\dots, X_{\\pi(n)})\\), using the inequality \\[ H(X_1, \\dots, X_{\\pi(n)}) \\leq \\sum_{i=1}^{\\pi(n)} H(X_i). \\] The cardinality of the support of \\(X_i\\) is \\(\\lbrack \\log_{p_i} n \\rbrack + 1 \\leq \\log n + 1\\), thus, \\[ H(X_i) \\leq \\log (\\log n + 1), \\] which leads to the inequality \\[ \\pi(n) \\geq \\frac{\\log n}{\\log (\\log n + 1)}. \\] As \\(n \\to \\infty\\), the right-hand side of this inequality tends to infinity, implying that \\(\\pi(n)\\) becomes arbitrarily large.\n\nIt is worth noting that this approach not only proves the infinitude of primes, but also provides a lower bound on the prime-counting function, which is of great interest to many mathematicians.\n\nAn Excercise\nIt is possible to improve the bound obtained above to\n\\[ \\pi(n) \\geq \\frac{ \\log n}{2 \\log 2}, \\]\nusing a different factorization of \\(n\\). Can you provide a proof?"
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html#thakkars-proof",
    "href": "posts/2024/06/PrimeInfinitude.html#thakkars-proof",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "3 Thakkar’s Proof",
    "text": "3 Thakkar’s Proof\nHere, I will only outline Thakkar’s proof and leave working out the details for the reader. The proof is based on the fact that the class of regular languages is closed under finite union. Consider the following family of languages over the alphabet \\(\\{ 0,1 \\}\\):\n\\[ \\mathcal{L}_n = \\{ w \\in \\{ 0,1 \\}^* : \\#_0(w) - \\#_1(w) \\equiv 0 \\pmod{n} \\}, \\]\nwhere \\(\\#_0(w)\\) and \\(\\#_1(w)\\) denote the number of occurrences of \\(0\\) and \\(1\\) in the string \\(w\\), respectively. You can easily verify that \\(\\mathcal{L}_n\\) is a regular language for any \\(n \\in \\mathbb{N}\\). In particular, \\(\\mathcal{L}_p\\) is regular for any prime \\(p\\). For the sake of contradiction, assume that the set of primes is finite. This implies that\n\\[ \\mathcal{L} = \\bigcup_{p \\in \\mathcal{P}} \\mathcal{L}_p, \\]\nwhere \\(\\mathcal{P}\\) is the set of all primes, is a finite union of regular languages, hence regular. To arrive at a contradiction, it is easier to use another fact about regular languages: the class of regular languages is closed under complementation. Consider the complement of \\(\\mathcal{L}\\), and observe that it is\n\\[ \\overline{\\mathcal{L}} = \\{ w \\in \\{ 0,1 \\}^* : \\#_0(w) - \\#_1(w) = \\pm 1 \\}. \\]\nUsing the pumping Lemma, or the Myhill-Nerode theorem, one can show that \\(\\overline{\\mathcal{L}}\\) is not regular, which is a contradiction. Therefore, the set of primes is infinite."
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html#acknowledgement",
    "href": "posts/2024/06/PrimeInfinitude.html#acknowledgement",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "4 Acknowledgement",
    "text": "4 Acknowledgement\nThe first time I came across Chaitin’s proof was in the final exam of a course I had on information theory taught by Thomas Debris-Alazard. Thanks to Thomas for his excellent teaching and evaluation methods!"
  },
  {
    "objectID": "posts/2024/06/PrimeInfinitude.html#bibliography",
    "href": "posts/2024/06/PrimeInfinitude.html#bibliography",
    "title": "Two CS-inspired Proofs for the Infinitude of Primes",
    "section": "5 Bibliography",
    "text": "5 Bibliography\n[1] Meštrović, R. (2012). Euclid’s theorem on the infinitude of primes: a historical survey of its proofs (300 BC–2022) and another new proof. arXiv preprint arXiv:1202.3670.\n[2] Chaitin, G. J. (1977). Toward a Mathematical Definition of Life, 2. IBM Thomas J. Watson Research Division.\n[3] Thakkar, A. (2018). Infinitude of Primes Using Formal Languages. The American Mathematical Monthly, 125(8), 745–749."
  },
  {
    "objectID": "misc_posts/digital-works.html",
    "href": "misc_posts/digital-works.html",
    "title": "Some of My Digital Artworks",
    "section": "",
    "text": "On this page\n   \n  \n  1 Photo, Effect, Paint\n  2 P-series\n  3 Touch\n  \n\n\n1 Photo, Effect, Paint\n\n\n\n\n\n\n\n\n\nMy Dear Friend, 2023\n\n\n\n\n\n\n\nThe secret chord is being played by the Lord herself!, 2023\n\n\n\n\n\n\n\nAh, lean upon it lightly!, 2023\n\n\n\n\n\n\n\n\n\nVoid, 2023\n\n\n\n\n\n\n\nArmita, 2023\n\n\n\n\n\n\n\nMirror, 2023\n\n\n\n\n\n\n\n\n\nUntitled, 2023\n\n\n\n\n\n\n\nTraces of yesterday, 2024\n\n\n\n\n\n\n\nBlossom, 2025\n\n\n\n\n\n\n\n\n\nMademoiselle and Her Lovers, 2023\n\n\n\n\n\n\n\n2 P-series\n\n\n\n\n\n\n\n\n\nNo. 1, 2023\n\n\n\n\n\n\n\nNo. 2, 2023\n\n\n\n\n\n\n\n\n\nNo. 3, 2023\n\n\n\n\n\n\n\nNo. 4, 2023\n\n\n\n\n\n\n\n\n\nNo. 5, 2023\n\n\n\n\n\n\n\nNo. 6, 2023\n\n\n\n\n\n\n\n\n\nNo. 7, 2023\n\n\n\n\n\n\n\nNo. 8, 2023\n\n\n\n\n\n\n\n\n\nNo. 9, 2023\n\n\n\n\n\n\n\nNo. 10, 2023\n\n\n\n\n\n\n\n\n\nNo. 11, 2023\n\n\n\n\n\n\n\nNo. 12, 2023\n\n\n\n\n\n\n\n\n\nNo. 13, 2023\n\n\n\n\n\n\n\nNo. 14, 2023\n\n\n\n\n\n\n\n\n\nNo. 15, 2023\n\n\n\n\n\n\n\nNo. 16, 2023\n\n\n\n\n\n\n\n\n\nNo. 17, 2023\n\n\n\n\n\n\n\nNo. 18, 2023\n\n\n\n\n\n\n\n\n\nNo. 19, 2023\n\n\n\n\n\n\n\nNo. 20, 2023\n\n\n\n\n\n\n\n\n\nNo. 21, 2023\n\n\n\n\n\n\n\nNo. 22, 2023\n\n\n\n\n\n\n\n\n\nNo. 23, 2023\n\n\n\n\n\n\n\nNo. 24, 2024\n\n\n\n\n\n\n\n\n\nNo. 25, 2024\n\n\n\n\n\n\n\nNo. 26, 2024\n\n\n\n\n\n\n\n\n\nNo. 27, 2025\n\n\n\n\n\n\n\nNo. 28, 2024\n\n\n\n\n\n\n\n3 Touch\n\n\n\n\n\n\n\n\n\nI couldn’t feel, so I tried to touch, no. 1, 2023\n\n\n\n\n\n\n\nI couldn’t feel, so I tried to touch, no. 2, 2023\n\n\n\n\n\n\n\nI couldn’t feel, so I tried to touch, no. 3, 2023\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "My research",
    "section": "",
    "text": "1 Publications\n  \n  1.1 A deterministic and computable Bernstein-von Mises theorem\n  1.2 Computing the quality of the Laplace approximation\n  1.3 Expectation Propagation in the large-data limit\n  1.4 Expectation Propagation performs a smoothed gradient descent\n  1.5 Bounding errors of expectation-propagation\n  \n  2 Research interests\n  \n  2.1 The problem of the uncomputable posteriors\n  2.2 Approximate inference schemes\n  2.3 Bayesian statistics for the frequentist\nI currently work full-time as an AI software engineer. This page recaps my work in statistics as a PhD. student at university of Geneva and as an instructor at Ecole Polytechnique Fédérale de Lausanne (EPFL) (from 2012 to 2020):"
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "My research",
    "section": "1 Publications",
    "text": "1 Publications\n\n1.1 A deterministic and computable Bernstein-von Mises theorem\nGuillaume Dehaene, 2019.\nArticle link.\nIn order to make Bayesian inference possible on large datasets, approximations are required. For example, computing the Laplace approximation is straightforward since it only requires finding the maximum of the posterior. However, while the Bernstein-von Mises theorem guarantees that the error of the Laplace approximation goes to in the limit of infinitely large datasets, it is hard to measure precisely the size of the error in a given example.\nThis article derives a tight and computable elegant approximation of the size of this error. I show that the Kullback-Leibler divergence between a given probability distribution and its Laplace approximation can be approximated using the “Kullback-Leibler variance”\n\n\n1.2 Computing the quality of the Laplace approximation\nGuillaume Dehaene, 2017, AABI NIPS 2017 Workshop.\nArticle link.\nBayesian inference requires approximations because the posterior distribution is generally uncomputable. The Laplace approximation is a fairly basic one which gives us a Gaussian approximation of the posterior distribution. This begs the question: how good is the approximation? The classical answer to this question is the Bernstein-von Mises theorem, which asserts that in the large-data limit, the Laplace approximation becomes exact. However, this theorem is mostly useless in practice, mostly because its assumptions are hard to check.\nThis article presents a computationally-relevant extension of the classical result: we give an explicit upper-bound for the distance between a given posterior and its Laplace approximation. The approach we follow can be extended to more advanced Gaussian approximation methods which we will do in further work.\n\n\n1.3 Expectation Propagation in the large-data limit\nGuillaume Dehaene and Simon Barthelmé, 2017, Journal of the Royal Statistical Society, series B.\nArticle link.\nExpectation Propagation is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it. Our two main contributions consist in showing that EP is closely related to Newton’s method for finding the maximum of the posterior, and showing that EP is asymptotically exact, meaning that when the number of datapoints goes to infinity the method recovers the posterior exactly.\nWe also introduce some new theoretical tools that help analysing EP formally, including a simpler variant, called Average-EP (or Stochastic-EP), that is asymptotically equivalent to EP.\n\n\n1.4 Expectation Propagation performs a smoothed gradient descent\nGuillaume Dehaene, 2016, AABI NIPS 2016 Workshop.\nArticle link.\nNeurIPS AABI Workshop 2016 Disney Research Paper Awards\nIf one wants to compute a Gaussian approximation of a probability distribution, there are three popular alternatives: the Laplace approximation, the Gaussian Variational Approximation, and Expectation Propagation.\nI show in this work that the approximations found by these three methods are actually very closely related, as they all correspond to variants from the same algorithm. This shines a bright light on the deep connections between these three algorithms.\n\n\n1.5 Bounding errors of expectation-propagation\nGuillaume Dehaene and Simon Barthelmé, 2015, NIPS 2015.\nArticle link.\nExpectation Propagation (EP) is a popular method for variational inference which can be remarkably effective despite the fact that there’s very little theory supporting it.\nOur contribution in this work consists in showing that, in the large-data limit, EP is asymptotically exact and, furthermore, more precise than the alternative Laplace approximation. However, our results only hold for strongly log-concave distributions, which very rarely exist."
  },
  {
    "objectID": "publications.html#research-interests",
    "href": "publications.html#research-interests",
    "title": "My research",
    "section": "2 Research interests",
    "text": "2 Research interests\nMy research interests are briefly summarized in this section.\n\n2.1 The problem of the uncomputable posteriors\nBayesian inference is a very interesting method for someone who is interested in an axiomatic approach to inference. Indeed, the statistician Cox set out a simple set of rules for a robot to represent, using real numbers, the strength of his beliefs in various propositions and proved that the only system which obeys these axioms is probability theory. Furthermore, the rule that the robot should use to update his beliefs when faced new information is Bayes’s rule. The only axiomatization of rational thinking about the world is thus Bayesian inference.\nHowever, there is a huge problem with Bayesian inference: in most cases, the computations required for exact implementation of the method are too expensive to be used in practice. Most of the practical research work on Bayesian methods actually revolves about how to deal with this thorny issue with various approximation schemes. These approximation schemes can be decomposed into two large families: sampling methods (dominated by Markov Chain Monte Carlo methods; acronym MCMC) which aim at producing samples from the posterior distribution and on which I don’t have much to say, and what I call approximate inference methods: methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution.\n\n\n2.2 Approximate inference schemes\nMy work centers on methods which aim to return a parametric approximation (very often Gaussian) of the true posterior distribution. These are often called “variational” methods but I’d rather call them approximate inference methods instead. This is because:\n\nthe word “variational” is already used for the Variational Bayes algorithm which, even though it is the most popular approximate inference method, is far from being the only one\n“variational” implies an optimisation, which means that the term variational excludes the Expectation Propagation algorithm\nthe only critic of the “approximate inference” vs “sampling” separation I have gotten is that sampling methods also aim at producing an approximation of the posterior. I still feel like this is fine, since sampling methods require quite a bit of further processing in order to answer questions about the posterior whereas approximate inference methods directy output an approximation.\n\nThere is currently a large number of open questions on such methods.\n\nThe most important one concerns the speed at which these algorithms perform their task. Most often, these algorithms perform an iteration until they reach a fixed-point. Estimating the number of loops needed for convergence is very important for being able to guarantee that our algorithms will run quickly.\nA second critical question concerns the quality of the approximation we obtain. We need to understand in which cases these algorithms are good enough, and in which cases we should use the more expensive but more accurate sampling methods. However, current theoretical results are inapplicable for a number of reasons: the hypotheses are untestable, they apply to approximations that are not in use, etc.\nThe final important question is of a more practical nature. It is simply whether the current versions of the algorithm we have are the best we can do, or whether there are better variants that are yet to be found. This can only be solved once we are able to compute the speed and the approximation-quality of current methods. We will then be able to see whether introducing slight changes to the current algorithms will improve them.\n\n\n\n2.3 Bayesian statistics for the frequentist\nAnother aspect of my work concerns trying to convince my frequentist colleagues that Bayesian inference is the best system of statistics.\nIn practice, the best way to do this is not to be a dogmatic Bayesian which goes on shouting about the Cox axioms and subjective probability, but instead to adopt the frequentist point of view on problems, and show that Bayesian methods are the best at solving these. Adopting Bayesian methods is then simply a matter of choosing the most efficient tool for solving problems.\nIn practice, following this idea means studying the posterior distribution as a function-valued random variable, and then studying the behavior of this random variable. My work expands on earlier work by, among other, Lecam on what is called the “Bernstein-von Mises theorem”. My objective is to expand current forms of this theorem so that they are:\nAs general as they can be. As efficient as they can be. Relevant in practical applications of Bayesian inference."
  },
  {
    "objectID": "assets/teaching_materials/functional_analysis2022/test.html",
    "href": "assets/teaching_materials/functional_analysis2022/test.html",
    "title": "Ali ALMASI",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "On this page\n   \n  \n  1 Quantum Algorithms\n  2 ZX-Calculus\n  3 Quantum Information Theory\n  4 Quantum Complexity Theory\n  \n\n\n1 Quantum Algorithms\n\n1.1 Survey Articles\n\nQuantum algorithms for algebraic problems by Andrew M. Childs and Wim van Dam\n\nThis is an excellent source for learning about a large class of quantum algorithms that are similar to the seminal algorithm by Peter Shor for factoring integers. My suggestion to beginners is to start with reading Appendix B, then move on to Section III.1\n1 Childs also has a set of lecture notes on quantum algorithms that may be helpful in reading this survey.\n\n\n2 ZX-Calculus\n\n2.1 Survey Articles\n\nZX-calculus for the working quantum computer scientist by John van de Wetering\n\nA wonderful easy-to-read and succinct introduction to ZX-calculus. If you are completely new to ZX, I believe that reading Section 3-6 and Section 10 will give you an idea of what ZX-calculus is and how it is useful.\n\n\n\n3 Quantum Information Theory\n\n\n4 Quantum Complexity Theory\n\nQuantum Computational Complexity by John Watrous\nQuantum Hamiltonian Complexity by Gharibian et al.\nQuantum Proofs by Thomas Vidick and John Watrous\n\n\n\n\n\n\n Back to top"
  }
]